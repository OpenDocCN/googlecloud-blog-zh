<html>
<head>
<title>Building a web server which receives a browser microphone stream and uses Dialogflow or the Speech to Text API for retrieving text results. (Part III)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建web服务器，该服务器接收浏览器麦克风流并使用Dialogflow或语音到文本API来检索文本结果。(第三部分)</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/building-a-web-server-which-receives-a-browser-microphone-stream-and-uses-dialogflow-or-the-speech-62b47499fc71?source=collection_archive---------0-----------------------#2020-04-22">https://medium.com/google-cloud/building-a-web-server-which-receives-a-browser-microphone-stream-and-uses-dialogflow-or-the-speech-62b47499fc71?source=collection_archive---------0-----------------------#2020-04-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e7e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是该系列的第三篇博客:</p><p id="87cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">将音频从浏览器麦克风流式传输到Dialogflow的最佳实践&amp; Google Cloud语音转文本。</strong></p><p id="e451" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你没有看过其他的博客，我推荐你浏览这些博客:</p><ul class=""><li id="544a" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><a class="ae jm" rel="noopener" href="/@ladysign/building-your-own-conversational-voice-ai-with-dialogflow-speech-to-text-in-web-apps-part-i-b92770bd8b47"> <strong class="ih hj">博客1:介绍GCP对话式AI组件，并在web app </strong> </a> <strong class="ih hj">中集成自己的语音AI。</strong></li><li id="0b67" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><a class="ae jm" rel="noopener" href="/@ladysign/building-a-client-side-web-app-which-streams-audio-from-a-browser-microphone-to-a-server-part-ii-df20ddb47d4e"> <strong class="ih hj">博客2:构建一个客户端web应用程序，将音频从浏览器麦克风传输到服务器。</strong>T11】</a></li></ul><p id="9013" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本系列的下一篇博客中，我将从服务器端的浏览器麦克风接收音频字节，因此我可以使用它来使Dialogflow检测意图或语音到文本转录呼叫！</p><p id="0e83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些博客<a class="ae jm" href="https://github.com/dialogflow/selfservicekiosk-audio-streaming/tree/master/examples" rel="noopener ugc nofollow" target="_blank">包含简单的代码片段</a>和一个演示应用程序；<a class="ae jm" href="https://github.com/dialogflow/selfservicekiosk-audio-streaming" rel="noopener ugc nofollow" target="_blank">机场自助服务亭</a>，将用作参考架构。</p><h1 id="1ee7" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">服务器端实现</h1><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kq"><img src="../Images/d6c9486831785f2732225e05619c582a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hq1fms0XEeDxAId9.png"/></div></div></figure><p id="99f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是创建Node.js Express应用程序的步骤，该应用程序集成了Google APIs，如Dialogflow、Speech to Text和Text to Speech。</p><p id="aead" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您将需要一个工作的前端应用程序，如前一篇博客所述，以便从HTML5麦克风获得音频缓冲区。在继续之前，请确保您已经阅读了博客2。</p><p id="396c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在编写任何Node.js服务器代码之前，让我们快速预览一下配置和我正在使用的NPM库:</p><h2 id="7b53" class="lc jt hi bd ju ld le lf jy lg lh li kc iq lj lk kg iu ll lm kk iy ln lo ko lp bi translated">我的应用程序配置:。包封/包围（动词envelop的简写）</h2><p id="7e8c" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">在我的代码库中；对于简单的例子，对于端到端的机场自助服务亭，我将所有的语音配置存储在项目之外。因此，不用浏览所有代码，就可以轻松地进行设置。这就是为什么我创建了一个<strong class="ih hj">。env </strong>系统环境文件。</p><p id="0fb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的应用程序代码的后面，我可以使用npm库<a class="ae jm" href="https://www.npmjs.com/package/dotenv" rel="noopener ugc nofollow" target="_blank"> dotenv </a>。它将环境变量从一个. env文件加载到<a class="ae jm" href="https://nodejs.org/docs/latest/api/process.html#process_process_env" rel="noopener ugc nofollow" target="_blank"> process.env </a>中。如果我稍后在容器中部署我的应用程序，或者使用App Engine灵活的环境，我可以在<strong class="ih hj"> GKE配置图</strong>或<strong class="ih hj"> app.yaml. </strong>中指定这些环境变量</p><p id="b41e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是我的<strong class="ih hj">。env </strong>文件看起来像:</p><pre class="kr ks kt ku fd lv lw lx ly aw lz bi"><span id="323e" class="lc jt hi lw b fi ma mb l mc md">PROJECT_ID=gcp-project-id</span><span id="a674" class="lc jt hi lw b fi me mb l mc md">LANGUAGE_CODE=en-US</span><span id="06bb" class="lc jt hi lw b fi me mb l mc md">ENCODING=AUDIO_ENCODING_LINEAR_16</span><span id="a235" class="lc jt hi lw b fi me mb l mc md">SAMPLE_RATE_HERZ=16000</span><span id="7129" class="lc jt hi lw b fi me mb l mc md">SINGLE_UTTERANCE=false</span><span id="9382" class="lc jt hi lw b fi me mb l mc md">SPEECH_ENCODING=LINEAR16</span><span id="cd32" class="lc jt hi lw b fi me mb l mc md">SSML_GENDER=FEMALE</span></pre><p id="fbda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Dialogflow，作为<a class="ae jm" href="https://cloud.google.com/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2beta1#detectintentrequest" rel="noopener ugc nofollow" target="_blank"> DetectIntentRequest </a>一部分的AudioConfig是必不可少的。它指示语音识别器如何处理语音音频。检查所有可能的<a class="ae jm" href="https://cloud.google.com/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#google.cloud.dialogflow.v2.InputAudioConfig" rel="noopener ugc nofollow" target="_blank">配置</a>的RPC参考。</p><p id="3461" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于语音转文本，将<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.RecognitionConfig" rel="noopener ugc nofollow" target="_blank">识别配置</a>和<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.RecognitionAudio" rel="noopener ugc nofollow" target="_blank">识别音频</a>传递给<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.RecognizeRequest" rel="noopener ugc nofollow" target="_blank">识别请求</a>很重要。</p><p id="9188" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">识别音频</strong>包含在<strong class="ih hj">识别配置</strong>中指定编码的音频数据。必须提供<strong class="ih hj">内容</strong>或<strong class="ih hj"> uri </strong>。</p><p id="ef22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> RecognitionConfig </strong>向识别器提供指定如何处理请求的信息。</p><p id="be34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于文本到语音，以下配置对于进行<a class="ae jm" href="https://cloud.google.com/text-to-speech/docs/reference/rpc/google.cloud.texttospeech.v1#synthesizespeechrequest" rel="noopener ugc nofollow" target="_blank"> SynthesizeSpeechRequest </a>调用非常重要。<a class="ae jm" href="https://cloud.google.com/text-to-speech/docs/reference/rpc/google.cloud.texttospeech.v1#google.cloud.texttospeech.v1.SynthesisInput" rel="noopener ugc nofollow" target="_blank">合成输入</a>(可以是文本或SSML)<a class="ae jm" href="https://cloud.google.com/text-to-speech/docs/reference/rpc/google.cloud.texttospeech.v1#google.cloud.texttospeech.v1.VoiceSelectionParams" rel="noopener ugc nofollow" target="_blank">语音选择参数</a>(描述使用哪种语音)和一个<a class="ae jm" href="https://cloud.google.com/text-to-speech/docs/reference/rpc/google.cloud.texttospeech.v1#google.cloud.texttospeech.v1.AudioConfig" rel="noopener ugc nofollow" target="_blank">音频配置</a>(描述要合成的音频数据)。</p><h2 id="4c40" class="lc jt hi bd ju ld le lf jy lg lh li kc iq lj lk kg iu ll lm kk iy ln lo ko lp bi translated">我正在使用的NPM库:package.json</h2><p id="620f" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">因为我的示例应用程序使用了Node.js和NPM，所以我需要下载外部节点库。在这里可以找到<a class="ae jm" href="https://github.com/dialogflow/selfservicekiosk-audio-streaming/blob/master/examples/package.json" rel="noopener ugc nofollow" target="_blank"> my package.json </a>。</p><p id="3fd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对构建语音集成很重要的npm包:</p><ul class=""><li id="ff3d" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><a class="ae jm" href="https://www.npmjs.com/package/dialogflow" rel="noopener ugc nofollow" target="_blank"> dialogflow </a>:与dialogflow交互，进行意图匹配(针对语音)</li><li id="2563" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><a class="ae jm" href="https://www.npmjs.com/package/@google-cloud/speech" rel="noopener ugc nofollow" target="_blank"> @google-cloud/speech </a>:与STT互动，转录语音</li><li id="7e75" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><a class="ae jm" href="https://www.npmjs.com/package/@google-cloud/text-to-speech" rel="noopener ugc nofollow" target="_blank">@ Google-cloud/text-to-speech</a>:与TTS交互，合成文本</li></ul><p id="b7cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下软件包也派上了用场:</p><ul class=""><li id="53ab" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><a class="ae jm" href="https://www.npmjs.com/package/pb-util" rel="noopener ugc nofollow" target="_blank"> pb-util </a>:用于处理常见protobuf类型的实用程序。它可以与Dialogflow意向响应一起使用。</li><li id="269a" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><a class="ae jm" href="https://www.npmjs.com/package/stream" rel="noopener ugc nofollow" target="_blank">流</a>、<a class="ae jm" href="https://www.npmjs.com/package/util" rel="noopener ugc nofollow" target="_blank">实用程序</a>、<a class="ae jm" href="https://www.npmjs.com/package/through2" rel="noopener ugc nofollow" target="_blank">至2 </a>:用于处理流。如果其中一条关闭了，就把它们全部摧毁。</li><li id="28e8" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><a class="ae jm" href="https://www.npmjs.com/package/recordrtc" rel="noopener ugc nofollow" target="_blank"> recordrtc </a>:音频+视频+屏幕+画布录制的WebRTC JavaScript库。我在服务器端不需要它，但是它在我的package.json文件中，所以我可以在本地托管这个库(而不是从CDN)。</li><li id="de92" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><a class="ae jm" href="https://www.npmjs.com/package/socket.io" rel="noopener ugc nofollow" target="_blank"> socket.io </a> : Socket。IO支持基于事件的实时双向通信。</li><li id="d85e" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><a class="ae jm" href="https://www.npmjs.com/package/socket.io-stream" rel="noopener ugc nofollow" target="_blank"> socket.io-stream </a>:这是通过socket.io与Stream API进行双向二进制数据传输的模块</li><li id="2023" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><a class="ae jm" href="https://www.npmjs.com/package/uuid" rel="noopener ugc nofollow" target="_blank"> uuid </a>:生成通用的唯一标识符</li></ul><h2 id="7e86" class="lc jt hi bd ju ld le lf jy lg lh li kc iq lj lk kg iu ll lm kk iy ln lo ko lp bi translated">设置对话流</h2><p id="71de" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">导航至:<a class="ae jm" href="http://console.dialogflow.com/" rel="noopener ugc nofollow" target="_blank">http://console.dialogflow.com</a>并创建一个新代理。确保您已经在设置中启用了<strong class="ih hj">测试版功能</strong>，因为我们将利用<strong class="ih hj">知识库连接器</strong>来导入基于网络的常见问题，并且该功能目前处于测试阶段。</p><p id="d1c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦启用，我们就可以创建一个新的<strong class="ih hj">知识库FAQ </strong>，用<strong class="ih hj"> text/html </strong>作为<strong class="ih hj"> mime-type </strong>。对于机场自助服务亭演示，我将从一个实时网站<a class="ae jm" href="https://www.flysfo.com/faqs" rel="noopener ugc nofollow" target="_blank">https://www.flysfo.com/faqs</a>将旧金山机场的问题和答案加载到我的代理中</p><p id="06a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">FAQ导入后，您将看到Dialogflow中列出的所有问题和答案。我们现在需要指定文本的答案和SSML的回应:<strong class="ih hj"> $Knowledge。答案【1】</strong></p><p id="1a3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Dialogflow将使用这个响应(知识库Q和A的第一个回答)合成为一个AudioBuffer。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es mf"><img src="../Images/aab7d6fbd88bb555f4f09ee913ab8f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JR8v_TZKwtc3O9gkmZgQdQ.png"/></div></div></figure><p id="aa59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注:</strong></p><p id="d482" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的机场自助服务亭演示中，我还在Angular web应用程序中将问题和答案显示为可读文本。我是怎么做到的？通过在文本&amp;旁边创建一个<strong class="ih hj">自定义有效载荷</strong>SSML响应:</p><pre class="kr ks kt ku fd lv lw lx ly aw lz bi"><span id="5a7e" class="lc jt hi lw b fi ma mb l mc md">{</span><span id="94fb" class="lc jt hi lw b fi me mb l mc md">“knowledgebase”: true,</span><span id="c3f0" class="lc jt hi lw b fi me mb l mc md">“QUESTION”: “$Knowledge.Question[1]”,</span><span id="aa4f" class="lc jt hi lw b fi me mb l mc md">“ANSWER”: “$Knowledge.Answer[1]”</span><span id="2bb5" class="lc jt hi lw b fi me mb l mc md">}</span></pre><h2 id="ad17" class="lc jt hi bd ju ld le lf jy lg lh li kc iq lj lk kg iu ll lm kk iy ln lo ko lp bi translated">编写服务器代码</h2><p id="0577" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">通常，服务器端代码将由以下部分组成:</p><ul class=""><li id="8a1a" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">导入所有必需的库</li><li id="1806" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated">加载环境变量</li><li id="377a" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated">用套接字设置Express服务器。IO侦听器</li><li id="2e10" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated">Google Cloud API调用:Dialogflow音频检测意图&amp;检测流调用、语音到文本识别&amp;流识别调用、文本到语音合成调用</li></ul><p id="e7ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">出于演示目的，我不会讨论如何用express服务器设置Node.js应用程序。但是作为参考，你可以看看我的<a class="ae jm" href="https://github.com/dialogflow/selfservicekiosk-audio-streaming/blob/master/examples/simpleserver.js" rel="noopener ugc nofollow" target="_blank">简单服务器代码</a>，它已经被用于简单的<a class="ae jm" href="https://github.com/dialogflow/selfservicekiosk-audio-streaming/tree/master/examples" rel="noopener ugc nofollow" target="_blank">客户端示例</a>。你也可以看看<a class="ae jm" href="https://github.com/dialogflow/selfservicekiosk-audio-streaming/tree/master/server" rel="noopener ugc nofollow" target="_blank">机场自助服务亭</a>的代码，一个端到端的例子。这个例子使用了云语音转文本<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.Speech.StreamingRecognize" rel="noopener ugc nofollow" target="_blank">流识别</a>，对话流<a class="ae jm" href="https://cloud.google.com/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#google.cloud.dialogflow.v2.Sessions.DetectIntent" rel="noopener ugc nofollow" target="_blank">检测意图</a>，以及文本转语音<a class="ae jm" href="https://cloud.google.com/text-to-speech/docs/reference/rpc/google.cloud.texttospeech.v1?hl=fi#google.cloud.texttospeech.v1.TextToSpeech.SynthesizeSpeech" rel="noopener ugc nofollow" target="_blank">合成语音</a>。</p><p id="66bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当您浏览这些代码片段时，您将能够看到Express服务器。它们都通过Socket.io进行通信，如下所示:</p><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="mg mh l"/></div></figure><ol class=""><li id="2b17" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc mi jj jk jl bi translated">通过实例化Socket.io，我可以监听连接发出。一旦Socket.io客户端连接到服务器，这段代码就会执行。</li><li id="2684" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">当连接到套接字，并且客户端触发了“消息”事件时，执行以下代码。它将检索停止WebRTC记录器时设置的数据。回想一下我以前的博客，我创建了一个带有子对象的对象，它包含mime-type ( <strong class="ih hj"> audio/webm </strong>)和<strong class="ih hj"> audioDataURL </strong>，后者是包含音频记录的Base64字符串。让我们将Base64字符串转换成Node.js文件缓冲区。</li><li id="2662" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">有了那个<strong class="ih hj"> fileBuffer </strong>，我就可以调用我的自定义dialog flow<strong class="ih hj">detect intent</strong>实现，本文稍后会解释:</li></ol><pre class="kr ks kt ku fd lv lw lx ly aw lz bi"><span id="3339" class="lc jt hi lw b fi ma mb l mc md">const results = await detectIntent(fileBuffer);</span><span id="4cce" class="lc jt hi lw b fi me mb l mc md">client.emit(‘results’, results);</span></pre><p id="cfc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">或者，我可以将我的自定义语音到文本识别实现称为语音到文本识别实现，这将在本文后面解释:</p><pre class="kr ks kt ku fd lv lw lx ly aw lz bi"><span id="a840" class="lc jt hi lw b fi ma mb l mc md">const results = await transcribeAudio(fileBuffer);</span><span id="9993" class="lc jt hi lw b fi me mb l mc md">client.emit(‘results’, results);</span></pre><p id="a5ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这两个调用都是异步的，并返回一个带有结果的承诺。这些结果将被发送到客户端应用程序。</p><p id="0eb2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">客户端可以监听套接字发出的消息，如下所示:</p><pre class="kr ks kt ku fd lv lw lx ly aw lz bi"><span id="338d" class="lc jt hi lw b fi ma mb l mc md">socketio.on(‘results’, function (data) {</span><span id="6b32" class="lc jt hi lw b fi me mb l mc md">    console.log(data);</span><span id="ecb5" class="lc jt hi lw b fi me mb l mc md">});</span></pre><p id="9c42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.这是客户端触发的第二个事件的例子。在这种情况下是流事件。现在，我将在WebRTC记录器在<strong class="ih hj"> ondataavailable </strong>监听器中传输大块音频数据时检索数据。注意，客户端套接字用<strong class="ih hj"> socket.io-stream </strong>包装，用于流式二进制数据传输。</p><p id="8fd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我正在检索音频块，以及额外的数据，比如流名(一个字符串)。这可以用来在服务器上存储一个临时音频文件，我可以通过管道将音频流传输到这个文件中。它被用作一个支架，激活我的自定义对话流或语音到文本的实现。</p><p id="14a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.就像本文后面解释的DetectIntentStreaming实现一样:</p><pre class="kr ks kt ku fd lv lw lx ly aw lz bi"><span id="55fa" class="lc jt hi lw b fi ma mb l mc md">detectIntentStream(stream, function(results){</span><span id="4aec" class="lc jt hi lw b fi me mb l mc md">    client.emit(‘results’, results);</span><span id="11aa" class="lc jt hi lw b fi me mb l mc md">});</span></pre><p id="3b30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">或语音到文本流识别实现，本文稍后将对此进行解释:</p><pre class="kr ks kt ku fd lv lw lx ly aw lz bi"><span id="afe6" class="lc jt hi lw b fi ma mb l mc md">transcribeAudioStream(stream, function(results){</span><span id="4c4e" class="lc jt hi lw b fi me mb l mc md">    client.emit(‘results’, results);</span><span id="86e1" class="lc jt hi lw b fi me mb l mc md">});</span></pre><p id="7c46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这两个调用都在流中传递，还有一个回调函数在结果出来后执行。这些结果将被发送到客户端应用程序。</p><p id="d3cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">客户端可以监听套接字发出的消息，如下所示:</p><pre class="kr ks kt ku fd lv lw lx ly aw lz bi"><span id="a4c1" class="lc jt hi lw b fi ma mb l mc md">socketio.on(‘results’, function (data) {</span><span id="87ad" class="lc jt hi lw b fi me mb l mc md">    console.log(data);</span><span id="1d77" class="lc jt hi lw b fi me mb l mc md">});</span></pre><h2 id="bad6" class="lc jt hi bd ju ld le lf jy lg lh li kc iq lj lk kg iu ll lm kk iy ln lo ko lp bi translated">对Dialogflow的API调用</h2><p id="68d2" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">Dialogflow是一款人工智能工具，用于构建基于文本和语音的对话界面，如聊天机器人和语音应用。它使用自然语言理解等机器学习模型来检测对话的意图。</p><p id="179d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对话流意图检测的工作方式是，它首先尝试理解用户话语。然后，它将根据训练短语检查包含意图(聊天流)的Dialogflow代理。具有最佳匹配(最高置信度得分)的意图将返回答案，该答案可以是文本响应、音频响应或通过履行来自系统的响应。</p><p id="5347" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将使用Dialogflow Node.js客户端SDK，根据完成的音频缓冲区和传入的音频流，手动检测意图。</p><pre class="kr ks kt ku fd lv lw lx ly aw lz bi"><span id="845a" class="lc jt hi lw b fi ma mb l mc md">const df = require(‘dialogflow’);</span></pre><p id="7f25" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们首先准备客户端和请求。稍后，我可以通过添加音频输入来修改请求:</p><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="mg mh l"/></div></figure><ol class=""><li id="5c7c" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc mi jj jk jl bi translated">Dialogflow将需要一个会话ID。让我们使用UUID来生成一个随机的<a class="ae jm" href="https://www.ietf.org/rfc/rfc4122.txt" rel="noopener ugc nofollow" target="_blank">https://www.ietf.org/rfc/rfc4122.txt</a>RFC 4122 id，格式如下:' 1 B9 D6 BCD-bbfd-4b2d-9b5d-ab 8d FBD 4 bed '。</li><li id="1d23" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">之后，让我们创建一个Dialogflow会话路径。会话路径可以从Dialogflow会话客户端对象创建。它需要一个会话ID来使每个Dialogflow会话都是唯一的。它需要GCP项目id，该id指向一个GCP项目，该项目有一个工作的Dialogflow代理。<strong class="ih hj">注意:</strong>每个Google云平台项目只能有一个Dialogflow代理。以防您的Dialogflow代理需要测试和开发版本。您也可以利用Dialogflow中的<a class="ae jm" href="https://cloud.google.com/dialogflow/docs/agents-versions" rel="noopener ugc nofollow" target="_blank">版本</a>特性。或者您可以创建更多的GCP项目，一个用于测试代理，一个用于开发代理。</li><li id="2a60" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">让我们已经设置了一个请求对象，它将用于每个Dialogflow API调用。如果在流式传输音频时将使用该请求，则该请求将被用作初始请求。这意味着它首先在没有音频流的情况下连接到SDK，但是用它可以使用的音频配置来准备API。之后，音频块将流入。它需要有一个sessionPath(现在将指向一个客户端会话和一个特定的Dialogflow代理)。即使没有音频输入，我也已经可以设置<strong class="ih hj">查询输入</strong>。</li><li id="6bfa" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">因为我的应用程序支持语音，所以我需要设置<a class="ae jm" href="https://cloud.google.com/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#google.cloud.dialogflow.v2.InputAudioConfig" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">音频配置</strong> </a>对象。<strong class="ih hj"> audioConfig </strong>对象需要一个采样率赫兹(这个数字必须与客户端代码中的<strong class="ih hj"> desiredSampleRateHerz </strong>相同)。它需要包含口语文本语言的languageCode，并且它应该是在Dialogflow中设置的语言。它需要有一个编码，也需要与客户端使用的编码相同。在我的代码演示中，我使用了来自<strong class="ih hj">的配置。env </strong>文件。</li></ol><p id="1688" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们来看看这两个调用，<a class="ae jm" href="https://cloud.google.com/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#google.cloud.dialogflow.v2.Sessions.DetectIntent" rel="noopener ugc nofollow" target="_blank"> DetectIntent </a>和<a class="ae jm" href="https://cloud.google.com/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#google.cloud.dialogflow.v2.Sessions.StreamingDetectIntent" rel="noopener ugc nofollow" target="_blank"> StreamingDetectIntent </a>。</p><h2 id="87af" class="lc jt hi bd ju ld le lf jy lg lh li kc iq lj lk kg iu ll lm kk iy ln lo ko lp bi translated">检测意图</h2><p id="11f8" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">DetectIntent它在所有音频发送和处理后接收意图匹配结果。我正在创建一个异步函数，它获取音频缓冲区并将其添加到请求中。接下来，我通过传入请求来调用detectIntent。它返回一个可链接的承诺:</p><pre class="kr ks kt ku fd lv lw lx ly aw lz bi"><span id="00c8" class="lc jt hi lw b fi ma mb l mc md">async function detectIntent(audio){</span><span id="b189" class="lc jt hi lw b fi me mb l mc md">    request.inputAudio = audio;</span><span id="6654" class="lc jt hi lw b fi me mb l mc md">    const responses = await sessionClient.detectIntent(request);</span><span id="a9e1" class="lc jt hi lw b fi me mb l mc md">    return responses;</span><span id="ad43" class="lc jt hi lw b fi me mb l mc md">}</span></pre><p id="f442" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是响应的样子:<a class="ae jm" href="https://cloud.google.com/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#google.cloud.dialogflow.v2.DetectIntentResponse" rel="noopener ugc nofollow" target="_blank"> DetectIntentResponse </a>。您可能会对<strong class="ih hj">查询结果</strong>感兴趣。如果您在<strong class="ih hj"> DetectIntentRequest </strong>中传递了一个输出音频配置，您将能够检索基于在<strong class="ih hj">query result . fulfillmentmessages</strong>字段中找到的默认平台文本响应的值生成的音频数据字节。如果存在多个默认文本响应，它们将在生成音频时连接在一起。如果不存在默认的平台文本响应，则生成的音频内容将为空。</p><h2 id="deea" class="lc jt hi bd ju ld le lf jy lg lh li kc iq lj lk kg iu ll lm kk iy ln lo ko lp bi translated">流检测内容</h2><p id="1c42" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">StreamingDetectIntent执行双向流意图检测:在发送音频的同时接收结果。这个方法只能通过gRPC API使用(不能通过REST)。</p><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="mg mh l"/></div></figure><ol class=""><li id="1789" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc mi jj jk jl bi translated">我正在创建一个异步函数，它获取音频缓冲区并将其添加到请求中，以及回调函数的名称，一旦API获取结果，该函数将执行结果。</li><li id="27b6" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">执行<strong class="ih hj"> streamingDetectIntent() </strong>调用。</li><li id="d649" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">有一个<strong class="ih hj">on(‘data’)</strong>事件监听器，一旦音频块流入就执行。您可以在这里创建一些条件逻辑，以防在响应中有一个<strong class="ih hj"> data.recognitionResult </strong>，然后中间的抄本被识别。否则，可能已经检测到意图(或者在不匹配的情况下触发了后退意图)。我通过执行回调函数来返回结果。</li><li id="d46d" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">当请求出错时，您还可以监听<strong class="ih hj">错误</strong>事件。或者您可以在停止向Dialogflow发送流时，侦听<strong class="ih hj"> end </strong>事件。</li><li id="2b7e" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">其工作方式是，首先我们将让Dialogflow API知道将有一个<strong class="ih hj"> streamingDetectIntent </strong>调用，其中包含所有可以从请求中检索到的<strong class="ih hj"> queryInput </strong>和<strong class="ih hj">audio config</strong>。之后，所有其他将进入的消息将包含通过<strong class="ih hj">输入音频</strong>的音频流。</li><li id="c8f0" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">让我们使用一个名为<strong class="ih hj"> pump </strong>的小节点模块，它通过管道将流连接在一起，如果其中一个关闭，它就会销毁所有流。</li><li id="7b10" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">这里，我将转换流，因此请求现在也将包含带有音频缓冲区流的<strong class="ih hj"> inputAudio </strong>。</li></ol><p id="7f0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是响应的样子。<a class="ae jm" href="https://cloud.google.com/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#google.cloud.dialogflow.v2.StreamingDetectIntentResponse" rel="noopener ugc nofollow" target="_blank">streamingdetectintententresponse</a>。您可能会对查询结果感兴趣。如果您传入了<strong class="ih hj">streamingdetectintententrequest</strong>并输出音频配置，您将能够检索基于在<strong class="ih hj">query result . fulfillmentmessages</strong>字段中找到的默认平台文本响应的值生成的音频数据字节。如果存在多个默认文本响应，它们将在生成音频时连接在一起。如果不存在默认的平台文本响应，则生成的音频内容将为空。</p><p id="b5ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是它在实际生产应用程序中的样子，使用TypeScript:<a class="ae jm" href="https://github.com/dialogflow/selfservicekiosk-audio-streaming/blob/master/server/dialogflow.ts" rel="noopener ugc nofollow" target="_blank">https://github . com/dialog flow/self service kiosk-audio-streaming/blob/master/server/dialog flow . ts</a></p><h1 id="6904" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">语音转文本的API调用</h1><p id="b58e" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">语音到文本API将口语单词转录为书面文本。当你想在视频中生成字幕，从会议中生成文字记录等时，这是非常有用的。你也可以将它与聊天机器人(从文本中检测意图)结合起来，合成聊天机器人的答案。</p><p id="90fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">语音转文本功能非常强大，因为API调用响应将返回具有最高置信度得分的书面文本，还会返回一个包含可选文本选项的数组。还可以通过向API发送短语提示来偏向识别器。</p><p id="40da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我将使用Speech Node.js客户端SDK将语音转录为书面文本。</p><pre class="kr ks kt ku fd lv lw lx ly aw lz bi"><span id="91df" class="lc jt hi lw b fi ma mb l mc md">const speech = require('@google-cloud/speech');</span></pre><p id="4246" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们首先准备客户端和请求。稍后，我可以通过添加音频输入来修改请求:</p><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="mg mh l"/></div></figure><ol class=""><li id="61a2" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc mi jj jk jl bi translated">首先，让我们实例化语音客户端。</li><li id="c7a7" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">这里我将描述请求对象。由于我们的应用程序处理语音，我们将需要设置识别<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.RecognitionConfig" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">配置</strong> </a>对象。<strong class="ih hj">配置</strong>对象需要一个采样率赫兹(这个数字必须与客户端代码中的desiredSampleRateHerz相同)。它需要包含口语文本语言的languageCode。它需要一种编码，这种编码也需要与客户端中使用的编码相同。在我的代码演示中，我使用了来自<strong class="ih hj">的配置。env </strong>文件。</li></ol><p id="2822" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们来看看这两个调用，<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.Speech.Recognize" rel="noopener ugc nofollow" target="_blank">识别</a>和<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.Speech.StreamingRecognize" rel="noopener ugc nofollow" target="_blank">流识别</a>。</p><h2 id="e9e3" class="lc jt hi bd ju ld le lf jy lg lh li kc iq lj lk kg iu ll lm kk iy ln lo ko lp bi translated">承认</h2><p id="c606" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">识别呼叫执行同步语音识别。它在所有音频发送和处理后接收结果。</p><p id="793e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我正在创建一个异步函数，它获取音频缓冲区并将其添加到<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.RecognitionAudio" rel="noopener ugc nofollow" target="_blank">请求</a>中。接下来；我通过传入请求，从语音客户端调用<strong class="ih hj">识别</strong>方法。它返回一个可链接的承诺:</p><pre class="kr ks kt ku fd lv lw lx ly aw lz bi"><span id="8b2d" class="lc jt hi lw b fi ma mb l mc md">async function transcribeAudio(audio){</span><span id="bc7a" class="lc jt hi lw b fi me mb l mc md">    request.audio = {</span><span id="a142" class="lc jt hi lw b fi me mb l mc md">        content: audio</span><span id="0831" class="lc jt hi lw b fi me mb l mc md">    };</span><span id="4360" class="lc jt hi lw b fi me mb l mc md">    const responses = await speechClient.recognize(request);</span><span id="69a9" class="lc jt hi lw b fi me mb l mc md">    return responses;</span><span id="401d" class="lc jt hi lw b fi me mb l mc md">}</span></pre><p id="e78b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是响应的样子:<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.RecognizeResponse" rel="noopener ugc nofollow" target="_blank">识别器响应</a>。它将返回<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.SpeechRecognitionResult" rel="noopener ugc nofollow" target="_blank">SpeechRecognitionResults</a>，其中将包含一个包含<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.SpeechRecognitionAlternative" rel="noopener ugc nofollow" target="_blank">个备选项</a>的数组。每个备选项包含转录本、置信度得分(置信度估计值在0.0和1.0之间。数字越大，表示识别出的单词正确的可能性越大。)和一个包含所有单词的数组。备选方案按可信度排序，可信度最高的排在第一位。</p><h2 id="7a66" class="lc jt hi bd ju ld le lf jy lg lh li kc iq lj lk kg iu ll lm kk iy ln lo ko lp bi translated">流识别</h2><p id="87be" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">StreamingRecognize执行双向流式语音识别:在发送音频的同时接收结果。这个方法只能通过gRPC API使用(不能通过REST)。</p><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="mg mh l"/></div></figure><ol class=""><li id="fb96" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc mi jj jk jl bi translated">我正在创建一个异步函数，它获取音频缓冲区并将其添加到请求中，以及需要返回结果的回调函数的名称。</li><li id="1639" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">通过传入语音请求来执行streamingRecognize()调用。</li><li id="d957" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">有一个on(“数据”)事件监听器，它在音频块流入时执行。我通过执行回调函数来返回结果。</li><li id="7e12" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">当请求出错时，您还可以监听<strong class="ih hj">错误</strong>事件。或者您可以在停止向Dialogflow发送流时，侦听<strong class="ih hj"> end </strong>事件。</li><li id="4bea" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc mi jj jk jl bi translated">最后，我们将识别流与传入的音频一起通过管道传输。</li></ol><p id="2565" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是响应的样子:<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.StreamingRecognizeResponse" rel="noopener ugc nofollow" target="_blank">StreamingRecognizeResponse</a>。它将返回<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.StreamingRecognitionResult" rel="noopener ugc nofollow" target="_blank">StreamingRecognitionResult</a>，其中将包含一个带有<a class="ae jm" href="https://cloud.google.com/speech-to-text/docs/reference/rpc/google.cloud.speech.v1#google.cloud.speech.v1.SpeechRecognitionAlternative" rel="noopener ugc nofollow" target="_blank">选项</a>的数组。每个备选项包含转录本、置信度得分(置信度估计值在0.0和1.0之间。数字越大，表示识别出的单词正确的可能性越大。)和一个包含所有单词的数组。备选方案按可信度排序，可信度最高的排在第一位。</p><p id="6a4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，您已经看到了如何构建一个web应用程序，该应用程序通过浏览器将音频从本地设备上的麦克风流式传输到后端应用程序，并将Google Cloud Speech的结果提取到文本或Dialogflow中，并在用户界面中显示出来。</p><p id="2faa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果浏览器可以播放音频流，那就更好了。这就是下一篇博客的内容！</p></div></div>    
</body>
</html>