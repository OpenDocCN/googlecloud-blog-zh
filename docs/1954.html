<html>
<head>
<title>Sparkling Vertex AI Pipelines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">闪亮顶点人工智能管道</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/sparkling-vertex-ai-pipeline-cfe6e19334f7?source=collection_archive---------0-----------------------#2021-09-02">https://medium.com/google-cloud/sparkling-vertex-ai-pipeline-cfe6e19334f7?source=collection_archive---------0-----------------------#2021-09-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="eb50" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">如何使用顶点人工智能管道部署一个简单的Spark ML管道</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/ee11b757f565c2281ade9ac53fc60bae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JDaOsYivCo5zP0vjXHsfpA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图1。闪亮顶点人工智能管道</figcaption></figure><h2 id="99ca" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak"> <em class="kl">更新</em> </strong></h2><ul class=""><li id="11b1" class="km kn hi ko b kp kq kr ks jy kt kc ku kg kv kw kx ky kz la bi translated"><em class="lb">2022年4月11日:顶点AI管道现在支持</em> <a class="ae lc" href="https://cloud.google.com/blog/topics/developers-practitioners/announcing-serverless-spark-components-vertex-ai-pipelines" rel="noopener ugc nofollow" target="_blank"> <em class="lb">顶点AI管道的Dataproc无服务器组件</em> </a> <em class="lb">。有了这些组件，您就可以使用本地的KFP操作符轻松地将基于Spark的ML管道与Vertex AI管道和Dataproc无服务器进行协调。</em></li></ul></div><div class="ab cl ld le gp lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="hb hc hd he hf"><h2 id="e51d" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">好吧，你说吧，我会听的！</h2><p id="18e0" class="pw-post-body-paragraph lk ll hi ko b kp kq ij lm kr ks im ln jy lo lp lq kc lr ls lt kg lu lv lw kw hb bi translated">关于<a class="ae lc" href="https://cloud.google.com/data-science" rel="noopener ugc nofollow" target="_blank"> <strong class="ko hj">谷歌云数据科学</strong> </a>你有什么想法想看吗？请在此表格 上填写<a class="ae lc" href="https://forms.gle/H89eNLTVtCdpP1ro6" rel="noopener ugc nofollow" target="_blank"> <strong class="ko hj">告知。这将有助于我在未来的博客文章中=)</strong></a></p></div><div class="ab cl ld le gp lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="hb hc hd he hf"><h2 id="a795" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">免责声明</strong></h2><p id="bfd9" class="pw-post-body-paragraph lk ll hi ko b kp kq ij lm kr ks im ln jy lo lp lq kc lr ls lt kg lu lv lw kw hb bi translated">本文假设您已经对MLOps、机器学习管道和顶点人工智能有了一些了解。如果您没有，我建议您观看以下来自<a class="lx ly ge" href="https://medium.com/u/9b9e67983b04?source=post_page-----cfe6e19334f7--------------------------------" rel="noopener" target="_blank"> Priyanka Vergadia </a>的视频，以获得快速概述:<a class="ae lc" href="https://www.youtube.com/watch?v=Jrh-QLrVCvM" rel="noopener ugc nofollow" target="_blank">mlop和Vertex Pipelines简介</a>和<a class="ae lc" href="https://www.youtube.com/watch?v=1ykDWsnL2LE" rel="noopener ugc nofollow" target="_blank">带有Vertex AI的端到端mlop</a>。</p><h2 id="147e" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">前提</h2><p id="b05b" class="pw-post-body-paragraph lk ll hi ko b kp kq ij lm kr ks im ln jy lo lp lq kc lr ls lt kg lu lv lw kw hb bi translated"><a class="ae lc" href="https://cloud.google.com/vertex-ai/docs/pipelines/introduction" rel="noopener ugc nofollow" target="_blank">顶点人工智能管道</a>是今年在谷歌云上推出的顶点人工智能MLOps功能中最强大的服务之一。它们使得使用<a class="ae lc" href="https://www.kubeflow.org/" rel="noopener ugc nofollow" target="_blank"> Kubeflow </a>编排机器学习工作负载变得非常容易。实际上，您可以在管道组件中执行几乎任何操作。你并不局限于某些顶点函数。</p><p id="9d63" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">那么，如果您有一个PySpark ML管道呢？</p><p id="e918" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">在本文中，我们将通过在一些顶点管道组件中使用Spark来说明这种通用性。特别是，我将展示如何结合<a class="ae lc" href="https://cloud.google.com/dataproc" rel="noopener ugc nofollow" target="_blank">data roc</a>使用Vertex AI Pipelines来训练和部署一个ML模型，用于接近实时的预测性维护应用。</p><h2 id="36f2" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">我们的场景</h2><p id="25b8" class="pw-post-body-paragraph lk ll hi ko b kp kq ij lm kr ks im ln jy lo lp lq kc lr ls lt kg lu lv lw kw hb bi translated">假设你是一名ML工程师，现在你的公司正在向云迁移，你被告知领导一个概念验证，并证明如何训练一个预测性维护管道。基本上，目标是连续处理从本地加载到云的机器日志文件，用于训练和部署已经在本地构建的ML管道。</p><p id="ce94" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">关于这个过程，为了加速实验阶段，你需要变得更加独立于运行(ML)工作负载。此外，我们需要采用MLOps实践，以使整个团队的流程可靠、可扩展和可重复。</p><p id="9aa6" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">最后，你会发现顶点人工智能管道完成了所有这些。</p><p id="0165" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">事实上，您将能够使用基于Kubeflow的技术提交ML作业，与其他集群编排工具如<a class="ae lc" href="https://cloud.google.com/workflows" rel="noopener ugc nofollow" target="_blank"> Workflow </a>相比，我们假设您已经熟悉这种技术。此外，它们使您能够通过关联的元数据存储和其他顶点人工智能服务，如<a class="ae lc" href="https://cloud.google.com/vertex-ai/docs/datasets/datasets" rel="noopener ugc nofollow" target="_blank">数据集</a>、<a class="ae lc" href="https://cloud.google.com/vertex-ai/docs/featurestore" rel="noopener ugc nofollow" target="_blank">特征存储</a>和<a class="ae lc" href="https://cloud.google.com/vertex-ai/docs/experiments" rel="noopener ugc nofollow" target="_blank">实验</a>，跟踪您的ML工作流中的数据、特征、模型和实验指标的工件和血统。</p><h2 id="448b" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">数据集</h2><p id="61ed" class="pw-post-body-paragraph lk ll hi ko b kp kq ij lm kr ks im ln jy lo lp lq kc lr ls lt kg lu lv lw kw hb bi translated">为了实现这一场景，我们使用<a class="ae lc" href="https://archive.ics.uci.edu/ml/datasets/AI4I+2020+Predictive+Maintenance+Dataset" rel="noopener ugc nofollow" target="_blank"> UCI机器学习— AI4I 2020预测性维护</a>数据集，这是一个反映行业中遇到的真实预测性维护数据的合成数据集。它由14个与加工过程相关的特性组成，如加工温度、转速和扭矩。并且目标机器故障标签指示该机器是否对于至少一个定义的故障模式发生了故障。</p><h2 id="4648" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">我们管道的剖析:架构、组件和表现</h2><p id="35c5" class="pw-post-body-paragraph lk ll hi ko b kp kq ij lm kr ks im ln jy lo lp lq kc lr ls lt kg lu lv lw kw hb bi translated">关于顶点人工智能管道，我们假设</p><p id="3294" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated"><em class="lb">管道的每个组件都必须创建一个Dataproc集群，处理一个PySpark作业并销毁该集群。</em></p><p id="b695" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">有人可能会认为这种模式增加了额外的运行时间。确实如此，但是管道的每一步可能需要不同数量的资源，然后需要不同的集群配置。一个例子是通常需要更多处理能力的超参数调谐步骤。此外，使用Dataproc的一个优点与临时集群的概念有关，它同时提供了更多的控制和灵活性。</p><p id="0f24" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">也就是说，下面你可以找到管道架构</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es me"><img src="../Images/866d2c25ac6939128ada56a441de4be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BUr3p7-JtrB9KijO3KtFFw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图二。我们管道的剖析</figcaption></figure><p id="776a" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">查看其组件，您会发现:</p><ul class=""><li id="6fb9" class="km kn hi ko b kp lz kr ma jy mf kc mg kg mh kw kx ky kz la bi translated"><strong class="ko hj"> <em class="lb"> prepare_data </em> </strong>组件摄取数据，做一些简单的数据准备并创建<a class="ae lc" href="https://cloud.google.com/vertex-ai/docs/datasets/datasets" rel="noopener ugc nofollow" target="_blank">顶点数据集</a>来训练模型和评估模型</li><li id="ffd8" class="km kn hi ko b kp mi kr mj jy mk kc ml kg mm kw kx ky kz la bi translated"><strong class="ko hj"> <em class="lb"> train_model </em> </strong>组件使用<a class="ae lc" href="https://spark.apache.org/docs/latest/ml-guide.html" rel="noopener ugc nofollow" target="_blank"> PySpark MLlib库</a>训练一个简单的<em class="lb">gbt分类器</em>，计算几个分类度量，如<em class="lb">areasuprc、混淆矩阵、准确度</em> <strong class="ko hj"> </strong>并使用<a class="ae lc" href="https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction" rel="noopener ugc nofollow" target="_blank">顶点元数据</a>记录它们</li></ul><p id="e421" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">然后，如果精确召回曲线下的<strong class="ko hj"> <em class="lb">面积大于预定义的度量阈值</em> </strong>:</p><ul class=""><li id="310d" class="km kn hi ko b kp lz kr ma jy mf kc mg kg mh kw kx ky kz la bi translated"><strong class="ko hj"> <em class="lb"> hypertune_gb </em> </strong>组件使用<em class="lb"> ParamGridBuilder </em>和<em class="lb">cross validator</em><strong class="ko hj"/>方法寻找最佳模型和参数。所有指标和超调元数据再次使用元数据服务存储在管道沿袭中。</li><li id="21fb" class="km kn hi ko b kp mi kr mj jy mk kc ml kg mm kw kx ky kz la bi translated"><strong class="ko hj"> <em class="lb"> deploy_gb </em> </strong>组件使用<a class="ae lc" href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#:~:text=Spark%20Streaming%20is%20an%20extension,processing%20of%20live%20data%20streams.&amp;text=Spark%20Streaming%20provides%20a%20high,a%20continuous%20stream%20of%20data." rel="noopener ugc nofollow" target="_blank"> Spark Streaming </a>在Google Cloud Dataproc集群上部署最终模型</li></ul><p id="d6e2" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">一旦你弄清楚了每个组件，你就可以使用<a class="ae lc" href="https://kubeflow-pipelines.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> Kubeflow Pipelines SDK </a>来实现它们。</p><p id="eb4f" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">事实上，在我写这篇文章的时候，在<a class="ae lc" href="https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.1.5/" rel="noopener ugc nofollow" target="_blank">Google Cloud Pipeline Components SDK</a>中没有与Dataproc交互的预构建组件。但是你总是可以建立自定义的！</p><p id="7df9" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">在我们的例子中，由于我们所做的假设，组件需要使用<a class="ae lc" href="https://googleapis.dev/python/dataproc/latest/index.html" rel="noopener ugc nofollow" target="_blank"> Dataproc SDK </a>来:</p><ol class=""><li id="3972" class="km kn hi ko b kp lz kr ma jy mf kc mg kg mh kw mn ky kz la bi translated">创建Dataproc集群</li><li id="3c36" class="km kn hi ko b kp mi kr mj jy mk kc ml kg mm kw mn ky kz la bi translated">检查集群状态</li><li id="0e9f" class="km kn hi ko b kp mi kr mj jy mk kc ml kg mm kw mn ky kz la bi translated">提交Pyspark作业</li><li id="87c9" class="km kn hi ko b kp mi kr mj jy mk kc ml kg mm kw mn ky kz la bi translated">检查作业状态</li><li id="d14d" class="km kn hi ko b kp mi kr mj jy mk kc ml kg mm kw mn ky kz la bi translated">删除集群</li></ol><p id="e763" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">首先，构建管道组件的一种方式是通过装饰器来<a class="ae lc" href="https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.dsl.html#kfp.dsl.component" rel="noopener ugc nofollow" target="_blank">。例如，对于<strong class="ko hj"> <em class="lb">列车_模型组件</em> </strong>我们有</a></p><pre class="iy iz ja jb fd mo mp mq mr aw ms bi"><span id="a2c3" class="jn jo hi mp b fi mt mu l mv mw">@component(base_image="gcr.io/google.com/cloudsdktool/cloud-sdk:latest", packages_to_install=["google-cloud-dataproc==2.5.0", "google-cloud-storage==1.41.1", "scikit-learn==0.24.2"], output_component_file="2_sparkling_vertex_train_gb.yaml")</span><span id="1a47" class="jn jo hi mp b fi mx mu l mv mw">def train_gb(project_id:str, region:str, bucket_name:str,<br/>            cluster_spec:str,<br/>            train_dataset:Input[Dataset],<br/>            test_dataset:Input[Dataset],<br/>            val_dataset:Input[Dataset],<br/>            metrics: Output[Metrics],<br/>            graph_metrics: Output[ClassificationMetrics],<br/>            model: Output[Model])-&gt; NamedTuple("Outputs",<br/>                                    [("metrics_dict", str),<br/>                                     ("thold_metric", float),<br/>                                     ("model_uri", str),<br/>                                     ("train_dataset_uri", str),<br/>                                     ("val_dataset_uri", str),],):</span></pre><p id="a75a" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated"><em class="lb">一旦导入库并定义变量</em>，就创建了<a class="ae lc" href="https://cloud.google.com/dataproc/docs/guides/submit-job" rel="noopener ugc nofollow" target="_blank"> Dataproc API </a>所需的PySpark作业配置。</p><pre class="iy iz ja jb fd mo mp mq mr aw ms bi"><span id="3fa4" class="jn jo hi mp b fi mt mu l mv mw">train_gb_pyspark_job_spec = {<br/>     'reference': {<br/>         'project_id': project_id,<br/>         'job_id': train_gb_job_id<br/>     },<br/>     'placement': {<br/>         'cluster_name': cluster_name<br/>     },<br/>     'pyspark_job': {<br/>         'main_python_file_uri': f'gs://{bucket_name}/train_gb.py',<br/>         'args': ['--project-id', project_id,<br/>                  '--bucket', f'gs://{bucket_name}',<br/>                  '--train-uri', train_dataset.uri,<br/>                  '--test-uri', test_dataset.uri,<br/>                  '--metrics-file', metrics_file_name,<br/>                  '--model', model_name]<br/>                  }<br/> }</span></pre><p id="84a3" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">然后，您可以为我们在component⁴.中提到的每个任务包装一个方法下面您可以找到一个使用Python Client for Google Cloud data proc API提交Pyspark作业<a class="ae lc" href="https://googleapis.dev/python/dataproc/latest/index.html" rel="noopener ugc nofollow" target="_blank">的方法示例。</a></p><pre class="iy iz ja jb fd mo mp mq mr aw ms bi"><span id="9e9e" class="jn jo hi mp b fi mt mu l mv mw">def _submit_pyspark_job<em class="lb">(</em>project_id, region, job_spec<em class="lb">)</em>:</span><span id="5cfd" class="jn jo hi mp b fi mx mu l mv mw">    # create the job client.<br/>    job_client = dataproc.JobControllerClient<em class="lb">(<br/>        </em>client_options=<em class="lb">{<br/>            </em>'api_endpoint':<br/>            f'<em class="lb">{</em>region<em class="lb">}</em>-dataproc.googleapis.com:443'<br/>        <em class="lb">})<br/><br/>    </em># create the job operation.<br/>    job_op = job_client.submit_job_as_operation<em class="lb">(<br/>        </em>request=<em class="lb">{</em>"project_id": project_id,<br/>                 "region": region,<br/>                 "job": job_spec<em class="lb">}<br/>    )</em></span><span id="4252" class="jn jo hi mp b fi mx mu l mv mw"><em class="lb">    </em>result = job_op.result<em class="lb">()<br/>    </em>return result</span></pre><p id="c9e1" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">最后，您提交作业，并可以使用<a class="ae lc" href="https://www.kubeflow.org/docs/components/pipelines/sdk/output-viewer/" rel="noopener ugc nofollow" target="_blank">内置的Kubeflow SDK功能</a>来利用顶点管道功能，如度量和元数据。例如，如果您想记录培训指标，我们可以</p><pre class="iy iz ja jb fd mo mp mq mr aw ms bi"><span id="cd6a" class="jn jo hi mp b fi mt mu l mv mw"> # submit the job<br/> <br/> print<em class="lb">(</em>f"Submitting job <em class="lb">{</em>train_gb_job_id<em class="lb">}</em>."<em class="lb">)<br/> </em>job_result = _submit_pyspark_job<em class="lb">(</em>project_id, region, <br/>                                  train_gb_pyspark_job_spec<em class="lb">)</em></span><span id="1177" class="jn jo hi mp b fi mx mu l mv mw"><em class="lb"> </em>if _check_job_state<em class="lb">(</em>project_id, region, <br/>                     train_gb_job_id<em class="lb">) </em>== 'state.done':<br/>   print<em class="lb">(</em>f"Job <em class="lb">{</em>train_gb_job_id<em class="lb">}</em> successfully completed."<em class="lb">)</em></span><span id="f6b4" class="jn jo hi mp b fi mx mu l mv mw"><em class="lb"> </em>else:<br/>   raise RuntimeError<em class="lb">(</em>f'Job <em class="lb">{</em>train_gb_job_id<em class="lb">}</em> failed.'<br/>                      f'Please check logs.'<em class="lb">)</em></span><span id="64f3" class="jn jo hi mp b fi mx mu l mv mw"> # --------------------------------------------</span><span id="0d84" class="jn jo hi mp b fi mx mu l mv mw"> # log metrics<br/> metrics.log_metric<em class="lb">(</em>"areaUnderROC", area_roc<em class="lb">)<br/> </em>metrics.log_metric<em class="lb">(</em>"areaUnderPRC", area_prc<em class="lb">)<br/> </em>metrics.log_metric<em class="lb">(</em>"Accuracy", acc<em class="lb">)<br/> </em>metrics.log_metric<em class="lb">(</em>"f1-score", f1<em class="lb">)<br/> </em>metrics.log_metric<em class="lb">(</em>"Precision", w_prec<em class="lb">)<br/> </em>metrics.log_metric<em class="lb">(</em>"Recall", w_rec<em class="lb">)</em></span></pre><p id="c67f" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">使用with <em class="lb"> _check_job_state </em>作为辅助方法来获取作业状态，并且所有指标都是预先计算的。</p><p id="21f6" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">当然，这里的目的是提供一种可能的方法。你还可以探索其他的可能性。最后，如果您对每个步骤使用相同的设计，我们将拥有所有四个组件，并将获得以下管道表示</p><pre class="iy iz ja jb fd mo mp mq mr aw ms bi"><span id="4c55" class="jn jo hi mp b fi mt mu l mv mw">@kfp.dsl.pipeline<em class="lb">(</em>name=f"pyspark-anomaly-detection-pipeline-<em class="lb">{</em>ID<em class="lb">}</em>", pipeline_root=PIPELINE_ROOT<em class="lb">)<br/></em>def pipeline<em class="lb">(</em>project_id:str = PROJECT_ID,<br/>            region:str = REGION,<br/>            bucket_name:str = BUCKET,<br/>            cluster_spec: BASE_CLUSTER_SPEC,<br/>            raw_data: str = RAW_DATA_PATH,<br/>            thold: float = AUPR_THRESHOLD,<br/>            stream_folder: str = STREAM_FOLDER<br/>            <em class="lb">)</em>:<br/>   <em class="lb">"""<br/>   Combine prepare_data, train_model, hypertune_gb and deploy_gb components in order to train and serve the predictive maintenance model.<br/><br/>    Args:<br/>        project_id: The name of pipeline GPC project<br/>        region: The region where the pipeline will run<br/>        bucket_name: The bucket name where training data are stored<br/>        cluster_spec: The basic cluster spec to run pyspark jobs<br/>        raw_data: The name of training data source<br/>        thold: The minimum performance threshold to deploy the model<br/>        stream_folder: The name of folder to store the predictions<br/><br/>    """<br/><br/>  </em># Create the prepare_data operation to get training data<br/>  prepare_data_op = prepare_data<em class="lb">(<br/>      </em>project_id=project_id,<br/>      region=region,<br/>      bucket_name=bucket_name,<br/>      cluster_spec=cluster_spec,<br/>      raw_file=raw_data<em class="lb">)<br/><br/>  </em># Create the train_gb operation to train and evaluate model<br/>  train_gb_op = <em class="lb">(</em>train_gb<em class="lb">(<br/>      </em>project_id=project_id,<br/>      region=region,<br/>      bucket_name=bucket_name,<br/>      cluster_spec=cluster_spec,<br/>      train_dataset=prepare_data_op.outputs<em class="lb">[</em>'train_dataset'<em class="lb">]</em>,<br/>      test_dataset=prepare_data_op.outputs<em class="lb">[</em>'test_dataset'<em class="lb">]</em>,<br/>      val_dataset=prepare_data_op.outputs<em class="lb">[</em>'val_dataset'<em class="lb">])<br/>      </em>.after<em class="lb">(</em>prepare_data_op<em class="lb">))<br/><br/><br/>  </em># Set Condition to validate the model compared au_prc threshold<br/>  with Condition<em class="lb">(</em>train_gb_op.outputs<em class="lb">[</em>'thold_metric'<em class="lb">] </em>&gt; thold,<br/>                 name=AUPR_HYPERTUNE_CONDITION<em class="lb">)</em>:</span><span id="8399" class="jn jo hi mp b fi mx mu l mv mw"># Create the hypertune_gb operation to hypertune and evaluate model<br/>   hypertune_gb_op = <em class="lb">(</em>hypertune_gb<em class="lb">(<br/>       </em>project_id=project_id,<br/>       region=region,<br/>       bucket_name=bucket_name,<br/>       cluster_spec=cluster_spec,<br/>       train_dataset_uri=train_gb_op.outputs<em class="lb">[</em>'train_dataset_uri'<em class="lb">]</em>,<br/>       val_dataset_uri=train_gb_op.outputs<em class="lb">[</em>'val_dataset_uri'<em class="lb">])<br/>       </em>.after<em class="lb">(</em>train_gb_op<em class="lb">))<br/><br/>    </em># Create the deploy_op to deploy the model on Dataproc cluster<br/>    deploy_gb_op = deploy_gb<em class="lb">(<br/>        </em>project_id=project_id,<br/>        region=region,<br/>        bucket_name=bucket_name,<br/>        cluster_spec=cluster_spec,<br/>        stream_folder=stream_folder,<br/>        model=hypertune_gb_op.outputs<em class="lb">[</em>'tune_model'<em class="lb">])</em></span></pre><p id="d9a6" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">下面你可以看到当你把管道提交给顶点AI时，执行图是什么样子的</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es my"><img src="../Images/171b8ed9adcee6978a1c1aa4db25dda1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cmkzmja1NsJSjHeJaWwHow.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图3。Spark流水线执行图</figcaption></figure><h2 id="d2a2" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">好处:模拟接近实时的预测</h2><p id="bb6a" class="pw-post-body-paragraph lk ll hi ko b kp kq ij lm kr ks im ln jy lo lp lq kc lr ls lt kg lu lv lw kw hb bi translated">出于演示目的，您还可以模拟接近实时的预测，其中数据被加载到staging bucket，模型使用<a class="ae lc" href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#:~:text=Spark%20Streaming%20is%20an%20extension,processing%20of%20live%20data%20streams.&amp;text=Spark%20Streaming%20provides%20a%20high,a%20continuous%20stream%20of%20data." rel="noopener ugc nofollow" target="_blank"> Spark Streaming </a>生成部署在Google Cloud Dataproc集群上的预测。下面的<strong class="ko hj"><em class="lb">run _ get _ prediction _ transform</em></strong>函数可以用来生成预测。</p><pre class="iy iz ja jb fd mo mp mq mr aw ms bi"><span id="2ae9" class="jn jo hi mp b fi mt mu l mv mw">def run_get_prediction_transform<em class="lb">(</em>args<em class="lb">)</em>:<br/>    bucket = args.bucket<br/>    stream_uri = f'<em class="lb">{</em>args.bucket<em class="lb">}</em>/<em class="lb">{</em>args.stream_folder<em class="lb">}</em>'<br/>    tune_model_uri = args.tune_model_uri<br/><br/>    # create a spark session<br/>    logging.info<em class="lb">(</em>f"Instantiating the <em class="lb">{</em>APP_NAME<em class="lb">}</em> Spark session."<em class="lb">)<br/>    </em>spark = <em class="lb">(</em>SparkSession.builder \<br/>             .master<em class="lb">(</em>"local"<em class="lb">) </em>\<br/>             .appName<em class="lb">(</em>APP_NAME<em class="lb">) </em>\<br/>             .getOrCreate<em class="lb">())<br/><br/>    </em>try:<br/><br/>        # start prediction process<br/>        logging.info<em class="lb">(</em>f"Ingest streaming data under <em class="lb">{</em>stream_uri<em class="lb">}</em>."<em class="lb">)<br/>        </em>stream_raw_df = <em class="lb">(</em>spark.readStream \<br/>                         .option<em class="lb">(</em>"header", True<em class="lb">) </em>\<br/>                         .option<em class="lb">(</em>"delimiter", ','<em class="lb">) </em>\<br/>                         .option<em class="lb">(</em>"maxFilesPerTrigger", 1<em class="lb">) </em>\<br/>                         .option<em class="lb">(</em>"rowsPerSecond", 10<em class="lb">) </em>\<br/>                         .schema<em class="lb">(</em>DATA_SCHEMA<em class="lb">) </em>\<br/>                         .csv<em class="lb">(</em>stream_uri<em class="lb">))</em></span><span id="f525" class="jn jo hi mp b fi mx mu l mv mw"><em class="lb">        </em>logging.info<em class="lb">(</em>f"Load model into memory."<em class="lb">)<br/>        </em>tune_model = CrossValidatorModel.load<em class="lb">(</em>tune_model_uri<em class="lb">)</em></span><span id="11e8" class="jn jo hi mp b fi mx mu l mv mw"><em class="lb">        </em>logging.info<em class="lb">(</em>f" Start streaming prediction process."<em class="lb">)<br/>        </em>gb_prediction_transform = get_prediction_transform<em class="lb">(<br/>            </em>stream_raw_df,<br/>            tune_model<em class="lb">)</em></span><span id="5101" class="jn jo hi mp b fi mx mu l mv mw"><em class="lb">        </em>logging.info<em class="lb">(</em>f" Run prediction query."<em class="lb">)<br/>        </em>predictions_query = <em class="lb">(</em>gb_prediction_transform \<br/>                             .writeStream \<br/>                             .format<em class="lb">(</em>"console"<em class="lb">) </em>\<br/>                             .outputMode<em class="lb">(</em>"append"<em class="lb">) </em>\<br/>                             .queryName<em class="lb">(</em>"predictions"<em class="lb">) </em>\<br/>                             .start<em class="lb">())<br/>        </em>predictions_query.awaitTermination<em class="lb">()</em></span><span id="f65a" class="jn jo hi mp b fi mx mu l mv mw"><em class="lb">    </em>except RuntimeError as error:<br/>        logging.info<em class="lb">(</em>error<em class="lb">)</em></span></pre><p id="e6e9" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">最后，您通过在Dataproc作业UI中查看<em class="lb"> deploy_gb_2021xxxx </em>作业的日志来检查预测。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mz"><img src="../Images/37f167ff6036f02f8010e8d65a20b980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yjiNBpO0JdGfUm3jZRIa9g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图4。Dataproc作业UI中的预测日志</figcaption></figure><h2 id="6bb3" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">下一步是什么</h2><p id="9696" class="pw-post-body-paragraph lk ll hi ko b kp kq ij lm kr ks im ln jy lo lp lq kc lr ls lt kg lu lv lw kw hb bi translated">在本文中，我们探讨了如何使用顶点AI管道结合云Dataproc来训练和部署使用Spark MLlib的ML管道。我们对该场景做了一些假设，并设想了一种可能的管道架构来实现这一点。最后，我们展示了如何使用Kubeflow Pipelines SDK实现管道，以及如何使用Spark streaming模拟接近实时的预测。</p><p id="04c6" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">我们可以做得更多。例如，我们没有在这里讨论模型监控。实际上，我们可以构建一个定制的服务容器来部署SparkML模型，然后利用顶点监控及其与顶点预测的集成来监控它们。此外，谷歌云最近宣布了与Databricks的新合作伙伴关系，我们也可以探索MLFlow和Vertex AI平台的几个集成用例。</p><p id="aa59" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">现在，我希望你喜欢这篇文章。如果有，就鼓掌或者留言评论。此外，如果你想了解更多关于这个场景的信息，或者你有关于你想看的ML内容的想法，请随时通过LinkedIn或T2 Twitter联系我，让我们一起讨论。</p><p id="147a" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">直到下一个帖子…</p><p id="875a" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated"><em class="lb">感谢马克·科恩和布拉德·米罗的反馈。感谢Gianluca Ruffa，他教会了我“构建、记录、打包和发布”的艺术。</em></p><h2 id="7372" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">参考</h2><ul class=""><li id="d467" class="km kn hi ko b kp kq kr ks jy kt kc ku kg kv kw kx ky kz la bi translated"><a class="ae lc" href="https://cloud.google.com/vertex-ai" rel="noopener ugc nofollow" target="_blank">https://cloud.google.com/vertex-ai</a></li><li id="7b9a" class="km kn hi ko b kp mi kr mj jy mk kc ml kg mm kw kx ky kz la bi translated"><a class="ae lc" href="https://cloud.google.com/architecture/using-apache-spark-dstreams-with-dataproc-and-pubsub#creating-a-service-account-for-dataproc" rel="noopener ugc nofollow" target="_blank">https://cloud . Google . com/architecture/using-Apache-spark-dstreams-with-data proc-and-pubsub # creating-a-service-account-for-data proc</a></li><li id="d2a1" class="km kn hi ko b kp mi kr mj jy mk kc ml kg mm kw kx ky kz la bi translated"><a class="ae lc" href="https://github.com/GoogleCloudPlatform/vertex-ai-samples" rel="noopener ugc nofollow" target="_blank">https://github.com/GoogleCloudPlatform/vertex-ai-samples</a></li><li id="638f" class="km kn hi ko b kp mi kr mj jy mk kc ml kg mm kw kx ky kz la bi translated"><a class="ae lc" href="https://googleapis.dev/python/aiplatform/latest/index.html" rel="noopener ugc nofollow" target="_blank">https://googleapis.dev/python/aiplatform/latest/index.html</a></li><li id="3436" class="km kn hi ko b kp mi kr mj jy mk kc ml kg mm kw kx ky kz la bi translated"><a class="ae lc" href="https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.1.5/index.html" rel="noopener ugc nofollow" target="_blank">https://Google-cloud-pipeline-components . readthedocs . io/en/Google-cloud-pipeline-components-0 . 1 . 5/index . html</a></li><li id="0d9a" class="km kn hi ko b kp mi kr mj jy mk kc ml kg mm kw kx ky kz la bi translated"><a class="ae lc" href="https://spark.apache.org/docs/latest/index.html" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/docs/latest/index.html</a></li><li id="8e84" class="km kn hi ko b kp mi kr mj jy mk kc ml kg mm kw kx ky kz la bi translated"><a class="ae lc" href="https://spark.apache.org/docs/latest/ml-guide.html" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/docs/latest/ml-guide.html</a></li></ul></div><div class="ab cl ld le gp lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="hb hc hd he hf"><p id="c1c2" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">这个场景与我的经历密切相关。事实上，我看到当编排取决于谁管理集群的环境及其工具时，ML工程师需要很长时间才能提交模型培训。</p><p id="6066" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated"><em class="lb">为了简单起见，我只训练一个模型。但是你可以多次复制同一个组件。</em></p><p id="32f3" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated"><em class="lb">我选择这个指标是因为不平衡的使用。当然，你可以选择符合你问题的。</em></p><p id="74d9" class="pw-post-body-paragraph lk ll hi ko b kp lz ij lm kr ma im ln jy mb lp lq kc mc ls lt kg md lv lw kw hb bi translated">⁴ <em class="lb">在组件中建议包装方式。另一种方法是将它作为一个软件包来安装。</em></p></div></div>    
</body>
</html>