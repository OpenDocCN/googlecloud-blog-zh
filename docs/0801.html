<html>
<head>
<title>Optimizing TensorFlow Models for Serving</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为服务优化张量流模型</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf?source=collection_archive---------0-----------------------#2018-11-01">https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf?source=collection_archive---------0-----------------------#2018-11-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6815" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在机器学习的世界里，人们非常关注优化训练。关于优化预测的信息少得多。然而，为预测提供模型是我们在ML中赚钱的地方！服务性能会对您的用例中ML的价值产生重大影响。事实上，服务于预测的成本可能是ML应用的总投资回报的主要因素。在本帖中，我们将向您展示一些优化TensorFlow模型以服务预测的方法，帮助您降低成本并提高ML解决方案的性能。这项工作是与我在谷歌云解决方案架构师团队的同事<a class="jd je ge" href="https://medium.com/u/aa480048c61f?source=post_page-----959080e9ddbf--------------------------------" rel="noopener" target="_blank"> Khalid Salama </a>合作完成的。这篇文章的代码可以在github的<a class="ae jf" href="https://github.com/GoogleCloudPlatform/tf-estimator-tutorials/tree/master/00_Miscellaneous/model_optimisation" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="74a7" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">延迟(和大小)很重要</h1><p id="8d37" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">当谈到优化服务模型时，我们主要关心三件事:</p><ul class=""><li id="2b99" class="kj kk hi ih b ii ij im in iq kl iu km iy kn jc ko kp kq kr bi translated">模型尺寸</li><li id="c9a3" class="kj kk hi ih b ii ks im kt iq ku iu kv iy kw jc ko kp kq kr bi translated">预测速度</li><li id="73e3" class="kj kk hi ih b ii ks im kt iq ku iu kv iy kw jc ko kp kq kr bi translated">预测吞吐量</li></ul><p id="2fdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在服务ML时，<strong class="ih hj">模型大小</strong>很重要。当然，更小的型号使用更少的内存，更少的存储和网络带宽，它们加载更快。在某些情况下，硬件内存约束或服务限制可能会限制模型大小。例如，谷歌云上的机器学习引擎服务为模型设置了250MB的默认大小限制。当我们使用硬件加速进行预测时，我们需要确保我们的模型适合加速设备的内存。当我们在功能有限的边缘设备或移动设备上为模型提供服务时，模型大小有着特殊的影响。我们希望模型下载速度尽可能快，使用最少的网络带宽，占用尽可能少的内存和存储空间。</p><p id="282f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">预测速度</strong>是我们关心的另一个服务指标。当我们在线执行预测时，我们通常希望结果尽快返回。在许多在线应用程序中，服务延迟对于用户体验和应用程序需求至关重要。但是我们关心预测的速度，即使我们批量处理我们的预测。预测速度与服务成本直接相关，因为它与进行预测所需的计算资源量直接相关。在任何衡量预测吞吐量的公式中，进行预测所需的时间始终是一个关键变量。更快的预测意味着在相同硬件上更高的预测吞吐量，从而降低成本。</p><p id="7694" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">预测吞吐量</strong>是对我们的系统在给定时间段内可以执行多少预测的度量。除了刚才提到的预测速度之外，其他系统属性也会影响吞吐量，包括预测的批处理、硬件加速、负载平衡和服务实例的水平扩展。在本文中，除了针对单个输入示例的预测时间优化之外，我们不会讨论优化预测吞吐量的技术。</p><h1 id="1af6" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">TensorFlow中的模型格式</h1><p id="fa8b" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">TensorFlow在其相对较短的生命周期中，已经设法积累了几种模型序列化格式。对所有这些格式的讨论超出了我们目前的范围；<a class="ae jf" href="https://www.tensorflow.org/mobile/prepare_models" rel="noopener ugc nofollow" target="_blank">tensor flow文档中的这一页</a>很好地总结了它们。需要了解的最重要的是GraphDef和SavedModel格式。GraphDef格式是ProtoBuf序列化协议的一个版本，以文本或二进制的形式，对张量流图的定义进行编码。GraphDef还可以包括一个训练模型的权重，我们将在后面看到，但这不是必须的——权重可以存储为单独的检查点文件。SavedModel格式将GraphDef(实际上是元GraphDef，我们将在后面讨论)与存储权重的检查点文件结合在一起，所有这些都收集在一个文件夹中。有关已保存模型的更多信息，请参见TensorFlow文档中的<a class="ae jf" href="https://www.tensorflow.org/guide/saved_model" rel="noopener ugc nofollow" target="_blank">本页</a>。在本帖中，我们将同时使用GraphDef和SavedModel格式。</p><h1 id="9569" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">工具和技术</h1><p id="bf7a" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">TensorFlow中有多种技术可让您缩小模型的大小并改善预测延迟。以下是其中的一些:</p><ul class=""><li id="3cee" class="kj kk hi ih b ii ij im in iq kl iu km iy kn jc ko kp kq kr bi translated"><strong class="ih hj">冻结</strong>:将保存在SavedModel的检查点文件中的变量转换成直接存储在模型图中的常量。这减小了模型的整体尺寸。</li><li id="411a" class="kj kk hi ih b ii ks im kt iq ku iu kv iy kw jc ko kp kq kr bi translated"><strong class="ih hj">修剪</strong>:剥离预测路径和图形输出中未使用的节点，合并重复节点，以及清理其他节点操作，如摘要、标识等。</li><li id="c7f1" class="kj kk hi ih b ii ks im kt iq ku iu kv iy kw jc ko kp kq kr bi translated"><strong class="ih hj">常量合并</strong>:在模型中寻找任何子图，这些子图总是评估为常量表达式，并用这些常量替换它们。</li><li id="75ef" class="kj kk hi ih b ii ks im kt iq ku iu kv iy kw jc ko kp kq kr bi translated"><strong class="ih hj">折叠批量规范:</strong>将批量规范中引入的乘法折叠到上一层的权重乘法中。</li><li id="4e0d" class="kj kk hi ih b ii ks im kt iq ku iu kv iy kw jc ko kp kq kr bi translated"><strong class="ih hj">量化</strong>:将权重从浮点转换为较低精度，如16位或8位。</li></ul><p id="a222" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我们将展示如何执行上面列出的每一项技术。我们在这里讨论的优化过程将足以为各种模型执行基本的服务优化。</p><p id="b5b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在这里没有足够的空间来深入讨论这些技术。特别是量化，是一个很大的话题，本身就值得发几个帖子。当然，对于移动部署，还有<a class="ae jf" href="https://www.tensorflow.org/lite/" rel="noopener ugc nofollow" target="_blank"> TFLite </a>，它在移动模型上执行8位量化。我们不会讨论的其他优化技术包括融合卷积和AOT编译与<code class="du kx ky kz la b">tfcompile</code>。</p><p id="3e4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用TensorFlow <a class="ae jf" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#fold_constants" rel="noopener ugc nofollow" target="_blank">图形转换工具</a>来执行许多优化，这是一个C++命令行工具。我们将展示如何使用Python APIs来使用该工具。Graph Transform工具被设计用于处理以<em class="lb"> protobuf </em>格式保存为GraphDef文件的模型。然而，SavedModel格式是最现代的，也是最受其他工具和服务支持的格式。例如，在训练评估者之后导出的模型是SavedModel格式的。是云机器学习引擎唯一支持预测的格式。因此，在我们优化我们的模型后，我们将把它转换回SavedModel格式。我们还将展示如何使用<code class="du kx ky kz la b">saved_model_cli</code>工具输出SavedModel中的MetaGraphDef定义，以及TensorFlow saved_model包的Python API来检查GraphDef。</p><p id="ab59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将遵循的优化步骤以及模型格式转换如下:</p><ol class=""><li id="2203" class="kj kk hi ih b ii ij im in iq kl iu km iy kn jc lc kp kq kr bi translated">冻结保存的模型:</li></ol><p id="1634" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">saved model graph def</strong></p><p id="9762" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.优化冻结模型:</p><p id="7f48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">图表定义图表定义</strong></p><p id="44da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.将优化的冻结模型转换回SavedModel:</p><p id="3645" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> GraphDef已保存的模型</strong></p><h1 id="17e2" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">生成模型</h1><p id="4757" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">让我们开始吧。首先我们需要一个模型。我们将使用在“Hello World”深度学习数据集上训练过的一个，MNIST。这是我们模型的代码，一个简单的MNIST CNN分类器。该代码可在<a class="ae jf" href="https://github.com/GoogleCloudPlatform/tf-estimator-tutorials/blob/master/00_Miscellaneous/model_optimisation/Tutorial%20-%20TensorFlow%20Model%20Optimisation%20for%20Serving%20-%20MNIST%20with%20Keras.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>以及<a class="ae jf" href="https://github.com/GoogleCloudPlatform/tf-estimator-tutorials/blob/master/00_Miscellaneous/model_optimisation/optimize_graph_keras.py" rel="noopener ugc nofollow" target="_blank">github repo</a>中的脚本中获得。</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="li lj l"/></div></figure><p id="eaf6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是训练模型并导出保存的模型的代码:</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="li lj l"/></div></figure><p id="59f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型应该需要大约一分钟来训练，因为我们将<strong class="ih hj"> TrainSpec </strong>中的<code class="du kx ky kz la b">max_steps</code>值设置为50。我们在这里不追求准确性；我们只是想要一个导出模型的例子。运行此代码以执行培训并导出保存的模型:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="2dc9" class="lo jh hi la b fi lp lq l lr ls">train_data, train_labels, eval_data, eval_labels = load_mnist_keras()<br/>export_dir = train_and_export_model(train_data, train_labels)</span></pre><h1 id="71c1" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">检查保存的模型</h1><p id="f621" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">现在我们有了一个SavedModel，让我们检查一下它的内容。这是TensorBoard模型的图像，我们在这里设置了<code class="du kx ky kz la b">num_conv_layers= 3</code>和<code class="du kx ky kz la b">hidden_units=[512,512]</code>:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es lt"><img src="../Images/fbfab3487d1b47445b91628f4fc3473d.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*1rQ35AGFQHFz17fFcmptCw.png"/></div></figure><h2 id="39ff" class="lo jh hi bd ji lw lx ly jm lz ma mb jq iq mc md ju iu me mf jy iy mg mh kc mi bi translated">输出元图形定义</h2><p id="3f88" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">我们可以使用TensorFlow代码库中的<code class="du kx ky kz la b">saved_model_cli</code>工具从SavedModel中输出MetaGraphDef。什么是元图形定义？它是一个GraphDef，带有关于模型“签名”的附加信息，即输入和输出。让我们在刚刚导出的SavedModel上运行<code class="du kx ky kz la b">saved_model_cli</code>工具:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="9cc2" class="lo jh hi la b fi lp lq l lr ls">$ saved_models_base=models/mnist/cnn_classifier/export<br/>$ saved_model_dir=${saved_models_base}/$(ls ${saved_models_base} | tail -n 1)<br/>$ saved_model_cli show — dir=${saved_model_dir} — all</span></pre><p id="062b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是输出，在我们的图表中显示了输入和输出的特征。MetaGraphDef格式可以包含多个签名定义，但是<code class="du kx ky kz la b">estimator.export_savedmodel</code>方法只导出一个标记为<code class="du kx ky kz la b">‘serving_default’</code>的签名:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="2f41" class="lo jh hi la b fi lp lq l lr ls">MetaGraphDef <strong class="la hj">with</strong> tag-set: ‘serve’ contains the following SignatureDefs:</span><span id="29ef" class="lo jh hi la b fi mj lq l lr ls">signature_def[‘<!-- -->serving_default<!-- -->’]:<br/>The given SavedModel SignatureDef contains the following input(s):<br/> inputs[‘input_image’] tensor_info:<br/>     dtype: DT_FLOAT<br/>     shape: (-1, 28, 28)<br/>     name: serving_input_image:0<br/> The given SavedModel SignatureDef contains the following output(s):<br/> outputs[‘softmax’] tensor_info:<br/>     dtype: DT_FLOAT<br/>     shape: (-1, 10)<br/>     name: softmax/Softmax:0<br/>Method name <strong class="la hj">is</strong>: tensorflow/serving/predict</span></pre><p id="26ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“input_image”输入是我们在上面的<code class="du kx ky kz la b"><strong class="ih hj">make_serving_input_receiver_fn</strong></code>中定义的输入。“softmax”输出在keras模型函数中定义。</p><h2 id="efab" class="lo jh hi bd ji lw lx ly jm lz ma mb jq iq mc md ju iu me mf jy iy mg mh kc mi bi translated">加载并输出图形定义</h2><p id="b26d" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">让我们检查SavedModel中MetaGraphDef结构的GraphDef部分。我们使用前面提到的用于TensorFlow <code class="du kx ky kz la b">saved_model</code>模块的Python APIs来加载SavedModel并从MetaGraphDef获取GraphDef:</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="li lj l"/></div></figure><p id="f120" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下代码显示了如何显示GraphDef中的信息。我们可以输出图表中各种节点的名称列表，以及各种节点类型的计数。</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="li lj l"/></div></figure><p id="8462" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我们的SavedModel的输出:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="1a80" class="lo jh hi la b fi lp lq l lr ls">models/mnist/keras_classifier/export/1540846525<br/><br/>Input Feature Nodes: [u'serving_input_image', u'input_image']<br/><br/>Unused Nodes: []<br/><br/>Output Nodes: [u'softmax/Softmax']<br/><br/>Quanitization Nodes: []<br/><br/>Constant Count: 61<br/><br/>Variable Count: 97<br/><br/>Identity Count: 30<br/><br/>Total nodes: 308</span></pre><h2 id="066a" class="lo jh hi bd ji lw lx ly jm lz ma mb jq iq mc md ju iu me mf jy iy mg mh kc mi bi translated">输出模型尺寸</h2><p id="1d8b" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">让我们用下面的代码来看看模型的大小:</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="li lj l"/></div></figure><p id="7ecc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SavedModel大小的大小大致可以分为GraphDef的大小，和变量的大小(即模型的权重)。在任何优化之前，运行上面的代码来获得我们的模型的大小:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="0a3f" class="lo jh hi la b fi lp lq l lr ls">models/mnist/keras_classifier/export/1540846525</span><span id="0f61" class="lo jh hi la b fi mj lq l lr ls">Model size: 57.453 KB<br/>Variables size: 10691.978 KB<br/>Total Size: 10749.431 KB</span></pre><h1 id="0f42" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">基线预测基准</h1><p id="c997" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">现在，让我们对在未优化的图上执行预测所需的时间进行基准测试。在此之前，我们应该说几句关于基准测试方法的话。</p><h2 id="c1b7" class="lo jh hi bd ji lw lx ly jm lz ma mb jq iq mc md ju iu me mf jy iy mg mh kc mi bi translated">标杆管理模型</h2><p id="ce6b" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">对ML模型性能进行基准测试本身就是一个很深的话题。我们必须小心行事。有很多方法不正确地做到这一点，导致对我们的模型优化的影响的不准确的评估。有几种方法可以用来测试张量流模型的预测性能。以下是一些可能的候选人:</p><ol class=""><li id="71bf" class="kj kk hi ih b ii ij im in iq kl iu km iy kn jc lc kp kq kr bi translated">使用Python代码和contrib.predictor模块在本地开发系统上对模型执行预测。</li><li id="70a1" class="kj kk hi ih b ii ks im kt iq ku iu kv iy kw jc lc kp kq kr bi translated">使用云机器学习引擎的预测服务来测试使用该API进行预测的速度。</li><li id="ac0a" class="kj kk hi ih b ii ks im kt iq ku iu kv iy kw jc lc kp kq kr bi translated">使用TensorFlow的REST API来测试预测的速度。</li></ol><p id="5169" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">无论我们是在测试ML模型(还是其他任何东西)，任何基准测试方法都有几个关键属性:</p><ol class=""><li id="aecf" class="kj kk hi ih b ii ij im in iq kl iu km iy kn jc lc kp kq kr bi translated"><strong class="ih hj">可重复性</strong>:也就是说，我们的基准测试结果在不同的运行中表现出较低的方差。</li><li id="b75e" class="kj kk hi ih b ii ks im kt iq ku iu kv iy kw jc lc kp kq kr bi translated"><strong class="ih hj">受控环境</strong>:我们希望对基准环境有很大程度的了解和控制，这样我们就可以对我们正在测试的东西做出明确而准确的陈述。如果我们缺乏对环境的控制，那么我们可能会得出一个不同于我们声称要进行基准测试的系统的结论。</li><li id="1b6d" class="kj kk hi ih b ii ks im kt iq ku iu kv iy kw jc lc kp kq kr bi translated"><strong class="ih hj">有限焦点</strong>:我们要尽可能的限制基准的焦点。在任何像最大似然预测流水线这样复杂的系统中，都会有许多决定性能的系统因素。在我们的例子中，我们只想尽可能地测试由于模型图本身而产生的性能差异。理想情况下，网络性能、进行API调用和处理结果的速度以及可用的计算能力等其他因素不应该影响我们的基准测试结果。</li></ol><p id="0852" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">B)和C)的错误选择通常会导致A)的糟糕结果。换句话说，如果我们不在有限关注的受控环境中运行我们的测试，我们将在基准测试结果中看到很多可变性。</p><p id="b6d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jf" href="https://github.com/GoogleCloudPlatform/tf-estimator-tutorials/blob/master/00_Miscellaneous/model_optimisation/inference_test.py" rel="noopener ugc nofollow" target="_blank">示例代码</a>展示了如何使用上述三种技术选项运行推理基准。然而，根据上述标准A-C，TensorFlow服务选项是最佳选择。我们可以针对运行在docker容器中的TF Serving的本地实例轻松运行基准测试。这给了我们对环境的高度控制，并将测试的焦点尽可能地限制在图形本身的性能上。由于请求是在本地网络接口上发出的，因此对网络性能的影响很小。预测图的处理仅由TensorFlow运行时和TF服务引擎来执行，所有这些都是用C++代码实现的，c++代码在其执行中或多或少具有确定性。我们将使用TF Serving的REST API进行API调用，设置和接收结果只需要很少的处理。</p><p id="d5fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Python代码方法不是正确的选择，因为缺乏对环境的控制。这里的环境是Python解释器本身。因为Python在运行您的代码时做了很多事情，包括内存分配、数据管理和垃圾收集，所以有很多事情我们无法控制。事实上，我们观察到，当使用<code class="du kx ky kz la b">inference_test.py</code>中基于“纯Python”的推理测试在模型上运行本地测试时，每次运行的基准测试结果都有很大的可变性。这种可变性可能会导致我们错误地评估图形优化的性能影响。当然，我们可以运行许多测试，并对结果进行平均，以说明这种可变性。但是这使得基准测试方法更加复杂，增加了一层我们不愿意做的统计验证。</p><p id="dc1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还可以使用Google Cloud ML引擎的预测服务来执行基准测试；示例代码也展示了如何做到这一点。该服务是轻松部署您的生产模型以供服务的绝佳选择。当然，我们在这里讨论的一切都是为了优化生产服务的性能！但是由于缺乏对环境的控制，以及缺乏焦点，它并没有成为一个很好的系统来精确地、可重复地量化图形优化的影响。云ML引擎预测的环境有太多方面是我们作为基准环境无法控制的。像CMLE预测这样的自动扩展服务将根据您的使用情况自动部署可扩展的资源，这些资源可能会在基准测试运行过程中发生变化。资源可用性可能会随着时间的推移而变化，并且根据您在其中执行的区域而有所不同。由于该服务是一个云API，本地和云网络性能都将发挥作用，因此将测试的重点扩展到网络。</p><p id="98d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管如此，我们可以确信，如果我们成功地优化我们的张量流图，使服务速度显著提高，当我们在CMLE预测上部署我们的模型时，我们将看到延迟性能的提高和成本的降低。请注意，批量大小和模型复杂性等因素会对生产优化的相对改进产生影响。我们将在下面对此多说一点。</p><h2 id="15d6" class="lo jh hi bd ji lw lx ly jm lz ma mb jq iq mc md ju iu me mf jy iy mg mh kc mi bi translated">运行基准测试</h2><p id="0dad" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">为了运行基准测试，我们首先启动一个运行TF的本地docker容器，并加载我们的模型。示例代码中的<code class="du kx ky kz la b">tfserving.sh</code> <a class="ae jf" href="https://github.com/GoogleCloudPlatform/tf-estimator-tutorials/blob/master/00_Miscellaneous/model_optimisation/tfserving.sh" rel="noopener ugc nofollow" target="_blank">脚本</a>展示了如何做到这一点。为了使用这个脚本，你必须首先在你的系统上安装docker和TF服务容器；更多信息见<a class="ae jf" href="https://www.tensorflow.org/serving/docker" rel="noopener ugc nofollow" target="_blank">本页</a>。您应该在一个单独的shell窗口中运行<code class="du kx ky kz la b">tfserving.sh</code>脚本作为基准测试，因为TF Serving在运行时会产生控制台输出。</p><p id="2ed7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们调用<code class="du kx ky kz la b">tfserving.sh</code>脚本来启动一个指向我们模型的本地服务实例，然后在一个单独的shell中使用<code class="du kx ky kz la b">inference_test.py</code> <a class="ae jf" href="https://github.com/GoogleCloudPlatform/tf-estimator-tutorials/blob/master/00_Miscellaneous/model_optimisation/inference_test.py" rel="noopener ugc nofollow" target="_blank">脚本</a>调用<code class="du kx ky kz la b">inference_tfserving</code>方法来执行基准测试。</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="li lj l"/></div></figure><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="8a9d" class="lo jh hi la b fi lp lq l lr ls">$ ./tfserving.sh</span><span id="90aa" class="lo jh hi la b fi mj lq l lr ls">$ python inference_test.py tfserving serving_default</span></pre><p id="5569" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到以下结果:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="cda0" class="lo jh hi la b fi lp lq l lr ls">Total elapsed time: 189.821555 seconds<br/>Batch size 100 repeated 1000 times<br/>Average latency per batch: 0.189821555 seconds</span></pre><p id="0899" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">绝对结果对您来说可能是不同的，这取决于您运行它的系统。尽管如此，无论您在什么系统上运行它们，每次运行时它们都应该表现出很小程度的可变性。我们在为GCP使用<a class="ae jf" href="https://cloud.google.com/deep-learning-vm-image/" rel="noopener ugc nofollow" target="_blank">深度学习图像</a>创建的实例上运行了我们的基准，提供了对环境的额外控制。示例代码包括<a class="ae jf" href="https://github.com/GoogleCloudPlatform/tf-estimator-tutorials/blob/master/INSTALL.md" rel="noopener ugc nofollow" target="_blank">指令</a>，展示了如何在GCE上启动这样一个实例。</p><h1 id="cb8f" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">优化模型</h1><p id="cd1f" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">是时候对我们的模型进行一些优化了。我们必须执行的第一步是“冻结”模型的权重，将存储在单独变量文件中的权重合并到GraphDef中。为什么先做这个？好的，我们将在图上执行许多操作，合并节点，修剪节点，并且通常改变图的结构。我们需要对与图相关的变量，即权重，执行相同的操作。如果当我们修改图时权重保持分离，那么权重的结构将不再映射到模型的结构，并且我们将不能恢复模型的权重，使其对服务无用。</p><p id="cf6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">冻结图形也是一种优化形式。在SavedModel中，权重在图形定义中表示为“Const”ops，在检查点文件中也表示为变量。将这两者结合起来消除了作为变量的权重的冗余存储，并且还消除了单独加载和合并权重的需求，这将导致更快的加载过程。该操作将适度减少模型的总大小。正如我们将在下面看到的,<code class="du kx ky kz la b">freeze_graph</code>工具也执行一些图形修剪。</p><p id="e660" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当在移动设备上部署模型时，冻结是非常有用的，在移动设备上减少模型的大小、在单个文件中下载模型的能力以及减少加载时间都是非常重要的。TensorFlow Lite框架自动处理移动模型的图表冻结。</p><h2 id="aea7" class="lo jh hi bd ji lw lx ly jm lz ma mb jq iq mc md ju iu me mf jy iy mg mh kc mi bi translated">指定输出节点</h2><p id="0ba4" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">冻结图形时，您还需要指定最终冻结图形的输出节点。我们的模型只包含一个输出节点，“softmax”，但有时当您使用TensorFlow的高级API创建模型时，可能会创建多个输出节点。当您冻结图形时，除指定节点之外的输出节点将被移除。对于服务预测，我们通常只需要一组输出。移除额外的输出节点是一种优化，可以减少图表执行的计算，从而减小图表的大小并提高预测速度。</p><h2 id="734b" class="lo jh hi bd ji lw lx ly jm lz ma mb jq iq mc md ju iu me mf jy iy mg mh kc mi bi translated">冻结图表</h2><p id="bfa9" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">为了冻结图形，我们使用TensorFlow中的<code class="du kx ky kz la b">freeze_graph</code>工具，这是一个二进制命令行工具。除了冻结权重之外，冻结图表工具还会修剪图表，使其仅包含用于评估我们指定的输出节点的节点。</p><p id="99b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述，我们实际上将使用Python APIs调用该工具:</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="li lj l"/></div></figure><p id="09b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们通过调用上面的方法来冻结我们的图形:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="bf1c" class="lo jh hi la b fi lp lq l lr ls">freeze_graph(saved_model_dir, “head/predictions/class_ids”)</span></pre><h2 id="1a6b" class="lo jh hi bd ji lw lx ly jm lz ma mb jq iq mc md ju iu me mf jy iy mg mh kc mi bi translated">描述冻结后的图形</h2><p id="93e1" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">我们来看看冷冻后的曲线图。在图上运行我们上面定义的describe_graph方法:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="0a64" class="lo jh hi la b fi lp lq l lr ls">frozen_filepath = os.path.join(frozen_model_dir, ‘frozen_model.pb’)<br/>describe_graph(get_graph_def_from_file(frozen_filepath))</span></pre><p id="e27f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到以下输出:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="494b" class="lo jh hi la b fi lp lq l lr ls">models/mnist/cnn_classifier/export/1536079934/freezed_model.pb</span><span id="b172" class="lo jh hi la b fi mj lq l lr ls">Input Feature Nodes: [u'serving_input_image']</span><span id="d46e" class="lo jh hi la b fi mj lq l lr ls">Unused Nodes: []</span><span id="51c5" class="lo jh hi la b fi mj lq l lr ls">Output Nodes: [u'softmax/Softmax']</span><span id="faf3" class="lo jh hi la b fi mj lq l lr ls">Quantization Nodes: []</span><span id="a7f7" class="lo jh hi la b fi mj lq l lr ls">Constant Count: 34</span><span id="e25b" class="lo jh hi la b fi mj lq l lr ls">Variable Count: 0</span><span id="7db5" class="lo jh hi la b fi mj lq l lr ls">Identity Count: 27</span><span id="6cb8" class="lo jh hi la b fi mj lq l lr ls">Total nodes: 94</span></pre><p id="5d0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，与冻结操作之前的97个节点相比，变量节点的数量已经下降到0个。上图中的所有变量都已替换为存储权重值的常数。</p><p id="0fdc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型的总尺寸略有减小:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="2ebc" class="lo jh hi la b fi lp lq l lr ls">get_size(saved_model_dir, ‘frozen_model.pb’)</span><span id="bb73" class="lo jh hi la b fi mj lq l lr ls">models/mnist/cnn_classifier/export/1536079934/frozen_model.pb</span><span id="41af" class="lo jh hi la b fi mj lq l lr ls">Model Size: 10702.063 KB</span></pre><h1 id="5af1" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">优化图表</h1><p id="cd80" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">现在我们有了一个冻结的图，我们可以执行上面列出的其他优化。使用<a class="ae jf" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md" rel="noopener ugc nofollow" target="_blank">图形变换工具</a>，大部分都可以在一个步骤中完成。这是一个命令行工具，包含在TensorFlow的预构建二进制文件中，也可以从源代码中构建。我们将使用Python APIs调用该工具。下面的代码显示了如何:</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="li lj l"/></div></figure><p id="f6e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们通过传递所需优化的列表来调用模型上的代码，如下所示:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="2d70" class="lo jh hi la b fi lp lq l lr ls">transforms = [<br/> ‘remove_nodes(op=Identity)’, <br/> ‘merge_duplicate_nodes’,<br/> ‘strip_unused_nodes’,<br/> ‘fold_constants(ignore_errors=true)’,<br/> ‘fold_batch_norms’<br/>]</span><span id="5911" class="lo jh hi la b fi mj lq l lr ls">optimize_graph(saved_model_dir, “frozen_model.pb” , transforms, ‘head/predictions/class_ids’)</span></pre><p id="7adb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">列表中的前三个优化属于上面的“修剪”类别。它们从图中清除未使用的或重复的节点。“strip_unused_nodes”变换基本上与“冻结图形”工具执行的变换相同，即移除所有未连接到输出的节点。所以这个操作在目前的工作流程中技术上是多余的。我们在这里包括它只是为了指出图形转换工具也有执行这种优化的能力。第四个和第五个优化是上面提到的“常数折叠”和“折叠批量规范”。</p><p id="9d5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图形变换工具包括其他几个有趣的变换，包括量化，我们将在后面讨论。您甚至可以编写自己的转换，并将它们插入工具中。更多详细信息，请参见该工具的<a class="ae jf" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md" rel="noopener ugc nofollow" target="_blank">在线文档</a>。</p><h2 id="c93d" class="lo jh hi bd ji lw lx ly jm lz ma mb jq iq mc md ju iu me mf jy iy mg mh kc mi bi translated">关于批处理规范化的一个注记</h2><p id="5762" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">如果您在模型中使用批处理规范化，您总是可以通过运行“fold_batch_norms”转换来提高性能。如果您没有在深度神经网络或卷积神经网络模型中使用批处理规范化，您可能应该这样做。它将几乎总是提高训练速度和准确性。从TensorFlow 1.10开始，它被内置到dnn的“固定估值器”中。</p><h2 id="b3c7" class="lo jh hi bd ji lw lx ly jm lz ma mb jq iq mc md ju iu me mf jy iy mg mh kc mi bi translated">描述优化的图形</h2><p id="8725" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">让我们检查优化后的图形。运行我们在上面使用的describe_graph方法:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="e792" class="lo jh hi la b fi lp lq l lr ls">optimized_filepath = os.path.join(saved_model_dir,’optimized_model.pb’)<br/>describe_graph(get_graph_def_from_file(optimized_filepath))</span></pre><p id="dacd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到以下输出:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="7184" class="lo jh hi la b fi lp lq l lr ls">models/mnist/cnn_classifier/export/1536341328/optimized_model.pb</span><span id="8a55" class="lo jh hi la b fi mj lq l lr ls">Input Feature Nodes: [u’input_image’]</span><span id="c866" class="lo jh hi la b fi mj lq l lr ls">Unused Nodes: []</span><span id="8380" class="lo jh hi la b fi mj lq l lr ls">Output Nodes: [u’head/predictions/class_ids’]</span><span id="4c84" class="lo jh hi la b fi mj lq l lr ls">Quantization Nodes: []</span><span id="695e" class="lo jh hi la b fi mj lq l lr ls">Constant Count: 29</span><span id="1d16" class="lo jh hi la b fi mj lq l lr ls">Variable Count: 0</span><span id="f79c" class="lo jh hi la b fi mj lq l lr ls">Identity Count: 0</span><span id="b6ed" class="lo jh hi la b fi mj lq l lr ls">Total nodes: 62</span></pre><p id="ae1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优化的图甚至比冻结的图更小，所有的标识节点都被删除了。如果您对优化和结果图的更多细节感兴趣，请尝试将show_nodes=True参数传递给describe_graph方法。</p><p id="817e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优化图形的大小略小于冻结图形:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="d180" class="lo jh hi la b fi lp lq l lr ls">get_size(saved_model_dir, ‘optimized_model.pb’)</span><span id="f620" class="lo jh hi la b fi mj lq l lr ls">models/mnist/cnn_classifier/export/1537571371/optimized_model.pb</span><span id="4263" class="lo jh hi la b fi mj lq l lr ls">Model size: 10698.921 KB</span></pre><p id="6940" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与原始图相比，优化图中的节点数量大幅减少。所有不用的节点都被去掉了，只有一个输出节点，变量和常数的数量明显减少。然而这个图的功能应该完全等同于原始的SavedModel图！</p><p id="3835" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当然，这是一个我们应该“相信，但要核实”的声明您应该总是在您的原始测试集上测试优化的模型，以验证准确性和其他指标没有降低。对于任何给定的预测输入，我们到目前为止所执行的优化都不会改变模型的输出。</p><h1 id="4cdc" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">量化</h1><p id="5124" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">量化是一种既能减小张量流模型的大小又能提高推断速度的技术。有了量化，你可以在推理中获得显著的加速，特别是如果你可以利用为量化模型设计的加速硬件的特殊能力。请看<a class="ae jf" href="https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html" rel="noopener ugc nofollow" target="_blank">这篇文章</a>的例子。通过将服务中使用的权重精度从64位或32位降低到16位或8位，您还可以将模型大小减少2倍或4倍，甚至可能是8倍。如前所述，这是一个太大的主题，无法在这里深入讨论。指出图形转换工具可以用来量化你的模型就足够了。下面是我们如何使用该工具来量化我们当前的模型，例如:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="026a" class="lo jh hi la b fi lp lq l lr ls">transforms = [<br/> ‘quantize_nodes’, <br/> ‘quantize_weights’,<br/>]<br/>optimize_graph(saved_model_dir, <strong class="la hj">None</strong>, transforms, ‘head/predictions/class_ids’)</span></pre><p id="7c07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“quantize_weights”转换将模型中的现有权重压缩为8位，然后进行解压缩操作，将单字节权重转换回浮点数。这导致模型大小大大减小，但没有相应的加速，因为计算仍在浮点中执行。更复杂的“quantize_nodes”优化实际上将所有使用权重执行的计算转换为8位，在每次计算之前和之后进行浮点转换。这可以大大加快推断的速度。</p><p id="64f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，由于权重精度的降低，这两种变换都会对模型精度产生影响。当然，在优化前后测试模型性能的最佳实践在这里更加适用。</p><p id="e8fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，如果您正在使用部署到移动设备的模型，请查看TensorFlow Lite，它是专门为移动设备执行量化而定制的。</p><h1 id="1da5" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">将优化的图形转换回SavedModel</h1><p id="adf9" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">下一步，我们将把冻结的、优化的GraphDef转换回SavedModel。基本上，转换过程包括添加回MetaGraphDef信息，以指定模型的输入和输出。使用TensorFlow中的<code class="du kx ky kz la b">saved_model</code>模块的<code class="du kx ky kz la b"><strong class="ih hj">simple_save</strong></code>方法，这变得非常简单。此方法为图形生成默认的MetaGraphDef。</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="li lj l"/></div></figure><p id="e632" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们必须将模型的输入和输出传递给<code class="du kx ky kz la b"><strong class="ih hj">simple_save</strong></code>方法。输出与我们在优化图形时指定的输出相同，通过在图形中查找占位符节点，可以很容易地找到输入。我们像这样调用方法，传递一个新目录来包含我们的SavedModel:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="3837" class="lo jh hi la b fi lp lq l lr ls">optimized_export_dir = os.path.join(export_dir, ‘optimized’)<br/>optimized_filepath = os.path.join(saved_model_dir, ‘optimized_model.pb’)<br/>convert_graph_def_to_saved_model(optimized_export_dir, optimized_filepath)</span></pre><p id="dfe9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们又有了一个SavedModel，我们可以使用<code class="du kx ky kz la b">saved_model_cli</code>输出MetaGraphDef:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="f5fc" class="lo jh hi la b fi lp lq l lr ls">$ saved_models_base=models/mnist/cnn_classifier/export<br/>$ optimized_model_dir=${saved_models_base}/optimized<br/>$ saved_model_cli show — dir=${optimized_model_dir} — all</span></pre><p id="a377" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出是:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="7c73" class="lo jh hi la b fi lp lq l lr ls">MetaGraphDef with tag-set: ‘serve’ contains the following SignatureDefs:</span><span id="34b0" class="lo jh hi la b fi mj lq l lr ls">signature_def[‘serving_default’]:<br/> The given SavedModel SignatureDef contains the following input(s):<br/> inputs[‘serving_input_image’] tensor_info:<br/>     dtype: DT_FLOAT<br/>     shape: (-1, 28, 28)<br/>     name: serving_input_image:0<br/> The given SavedModel SignatureDef contains the following output(s):<br/> outputs[‘softmax’] tensor_info:<br/>     dtype: DT_FLOAT<br/>     shape: (-1, 10)<br/>     name: softmax/Softmax:0<br/> Method name is: tensorflow/serving/predict</span></pre><h1 id="5f8f" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">对优化模型进行基准测试</h1><p id="cfa2" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">最后一步。让我们使用上面讨论的基于TF服务的方法，量化所有优化对推理速度的影响。我们需要重新启动TF服务容器，以指向我们新优化的SavedModel。如果您向<code class="du kx ky kz la b">tfserving.sh</code>脚本传递一个参数，它会将该文件夹名称附加到模型导出路径，并启动一个指向相应模型的TF服务实例:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="4446" class="lo jh hi la b fi lp lq l lr ls">$ docker kill $(docker ps -q)<br/>$ tfserving.sh optimized</span></pre><p id="41c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在优化的模型上重新运行基准测试，</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="7002" class="lo jh hi la b fi lp lq l lr ls">$ python inference_test.py tfserving serving_default</span></pre><p id="59d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到以下结果:</p><pre class="ld le lf lg fd lk la ll lm aw ln bi"><span id="492a" class="lo jh hi la b fi lp lq l lr ls">Total elapsed time: 162.434886 seconds<br/>Batch size 100 repeated 1000 times<br/>Average latency per batch: 0.162434886 seconds</span></pre><p id="aac3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这表示预测速度比原始模型提高了17%。对于运行一些脚本来说，这是一个不错的回报。这种速度上的提高和成本上的降低可以对您的ML应用程序的ROI产生有意义的影响。</p><h1 id="9100" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">影响优化收益的因素</h1><p id="3c8f" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">注意有许多变量会影响我们看到的性能差异的大小，包括预测的批量大小。许多在线预测应用程序使用的批量大小为1，一次将单个示例上传到API。这往往会降低任何模型优化的好处。模型复杂性也有影响。对于更大、更复杂的模型，这些优化可能会带来更大的性能提升。对所有这些因素的更详细的分析将不得不等待未来的帖子。</p><h1 id="0e8d" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">包扎</h1><p id="5adf" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq kg is it iu kh iw ix iy ki ja jb jc hb bi translated">我们希望这是对优化用于预测的张量流模型这一重要主题的有益介绍。一路上，我们了解了一些关于张量流图的底层表示、不同的模型导出格式和基准测试方法。有了我们探索的技术，您可以提高ML生产管道的效率和效用。感谢您的阅读，并祝服务愉快！</p></div></div>    
</body>
</html>