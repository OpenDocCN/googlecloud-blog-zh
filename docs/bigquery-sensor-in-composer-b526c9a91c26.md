# Composer/Áî®Êà∑ÂÆö‰πâÁöÑ‰º†ÊÑüÂô®ÂÆûÁé∞‰∏≠ÁöÑ BigQuery ‰º†ÊÑüÂô®

> ÂéüÊñáÔºö<https://medium.com/google-cloud/bigquery-sensor-in-composer-b526c9a91c26?source=collection_archive---------1----------------------->

ÊÉ≥Ë¶ÅÂú®Ê∫êÊï∞ÊçÆÂ∫ìÂà∑Êñ∞ÂíåÁõÆÊ†áÊï∞ÊçÆÂ∫ìÂä†ËΩΩ‰Ωú‰∏ö‰πãÈó¥ÂàõÂª∫‰æùËµñÂÖ≥Á≥ªÂêóÔºü

ÊÉ≥ËÆ© OLAP Âä†ËΩΩ‰Ωú‰∏ö‰∏ÄÁõ¥Á≠âÂà∞Ê∫ê OLTP Á≥ªÁªüÊõ¥Êñ∞ÂêóÔºüÁªßÁª≠‰∏ãÂéªÔºåÁúãÁúãËøô‰∏™ÊïôÁ®ã„ÄÇ

![](img/c5cb799fbd750f21b4681c55f5726d21.png)

**ÂáÜÂ§á:**ÂèØÈáçÁî®ÊÄß„ÄÅÂçöÂÆ¢„ÄÅÁ§æÂå∫Ë¥°ÁåÆ

**ÂÅáËÆæ**

Êú¨ÊïôÁ®ãÂÅáËÆæÊÇ®ÁÜüÊÇâ:

*   ComposerÔºåBigQueryÔºåPython

# Á§∫‰æã‰ΩøÁî®Ê°à‰æã:

Âè™ÊúâÂú®ÊåáÂÆöÁöÑÊó∂Èó¥ËåÉÂõ¥(ÂèØ‰øÆÊîπ)ÂÜÖÊõ¥Êñ∞Ê∫êË°®Êó∂ÔºåÊâçÂ∫îËØ•ÊâßË°å‰∏Ä‰∫õ‰∏ãÊ∏∏‰ªªÂä°„ÄÇ

Ê∫êÈ°πÁõÆÈôêÂà∂‰∫ÜÂÖÉÊï∞ÊçÆÊü•ÁúãÂô®ÊùÉÈôêÔºåÂÆÉÂèØ‰ª•ÈÄöËøá API Ë∞ÉÁî®Ôºå‰ΩÜÊòØÂÆÉ‰ª¨ÈôêÂà∂‰∫ÜÈÄöËøá Information_schema Êü•ËØ¢ÂÖÉÊï∞ÊçÆ„ÄÇ

ËÆÆÁ®ãÊòØÈÄöËøá bigquery API Ê£ÄÊü•Ë°®ÁöÑ lastmodifiedtimeÔºåÂ¶ÇÊûúË°®Ê≤°ÊúâÂú®ÊåáÂÆöÁöÑËåÉÂõ¥ÂÜÖÊõ¥Êñ∞ÔºåÂàôÈáçÊñ∞Ë∞ÉÂ∫¶ DAG„ÄÇ

poke ÊñπÊ≥ïÂ∞ÜÂú®‰∏äËø∞Êó∂Èó¥Èó¥ÈöîÂêéÈáçËØïÔºåÂπ∂Âú®Ê∫êË°®Êõ¥Êñ∞Êó∂Ëá™Âä®ÊâßË°å‰∏ã‰∏Ä‰∏™‰ªªÂä°„ÄÇ

‰∏çÈúÄË¶Å‰∫∫Â∑•Âπ≤È¢Ñ„ÄÇ

‰∏çÈúÄË¶ÅÂú®Âõ∫ÂÆöÁöÑÊó∂Èó¥Èó¥ÈöîÂÆâÊéí DAG Êù•Ê£ÄÊü•Ëøô‰∏ÄÁÇπ„ÄÇ

**Âú∫ÊôØ:**Êúâ‰∏Ä‰∏™ OLTP Ê∫êË°®‚ÄòOLTP _ customer‚Äô„ÄÇÊúâ‰∏Ä‰∏™ OLAP ÁõÆÊ†áË°® dim_customer„ÄÇÂè™ÊúâÂΩìÊ∫êË°®Âú®Ëøô‰∏ÄÂ∞èÊó∂ÂÜÖË¢´‰øÆÊîπÊó∂Ôºå‰∏ã‰∏Ä‰∏™‰ªéÊ∫êË°®Âä†ËΩΩÁª¥Â∫¶Ë°®ÁöÑ‰ªªÂä°Êâç‰ºöÊâßË°å„ÄÇÂú®‰øÆÊîπÊ∫êË°®‰πãÂâçÔºå‰ªªÂä°Â∞ÜÁªßÁª≠Ë¢´ÈáçÊñ∞Ë∞ÉÂ∫¶„ÄÇ

Â§©Á©∫È¢úËâ≤Ë°®Á§∫ÈáçÊñ∞ËÆ°ÂàíÁöÑÁä∂ÊÄÅ„ÄÇÊØèÂΩìÊõ¥Êñ∞Ê∫êË°®Êó∂ÔºåDAG ÈÉΩ‰ºöËá™Âä®ÂºÄÂßãËøêË°å‰ª•‰∏ã‰ªªÂä°Âπ∂ÂÆåÊàêËØ•ËøáÁ®ã

# **Ëß£ÂÜ≥ÊñπÊ°à:**

## ‰ªãÁªç

Ê∞îÊµÅ‰º†ÊÑüÂô®ÂÖÅËÆ∏ÊÇ®Ê£ÄÊü•ÊòØÂê¶Êª°Ë∂≥ÂÆåÊàêÊ†áÂáÜ„ÄÇ

Bigquery Sensor ÂèØ‰ª•‰Ωú‰∏∫Ê∞îÊµÅÊèí‰ª∂ÂàõÂª∫ÔºåÂπ∂ÂèØ‰ª•‰Ωú‰∏∫ python Ê®°ÂùóÂØºÂÖ•Âà∞Ê∞îÊµÅ DAG ‰∏≠„ÄÇ

ËØ•Ëß£ÂÜ≥ÊñπÊ°àÂü∫‰∫é‰∏Ä‰∏™Âü∫‰∫é BigQuery ÂÆ¢Êà∑Á´ØÁöÑÁî®Êà∑Ëá™ÂÆö‰πâ‰º†ÊÑüÂô®„ÄÇ

Airflow Êúâ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂÜÖÁΩÆÊèí‰ª∂ÁÆ°ÁêÜÂô®ÔºåÂèØ‰ª•ÈÄöËøáÁÆÄÂçïÂú∞Â∞ÜÊñá‰ª∂ÊîæÂÖ•$AIRFLOW_HOME/plugins Êñá‰ª∂Â§π‰∏≠ÔºåÂ∞ÜÂ§ñÈÉ®ÂäüËÉΩÈõÜÊàêÂà∞ÂÖ∂Ê†∏ÂøÉ‰∏≠„ÄÇ

Êèí‰ª∂Êñá‰ª∂Â§π‰∏≠ÁöÑ python Ê®°ÂùóË¢´ÂØºÂÖ•ÔºåÂÆèÂíå web ËßÜÂõæË¢´ÈõÜÊàêÂà∞ Airflow ÁöÑ‰∏ªÈõÜÂêà‰∏≠ÔºåÂèØ‰æõ‰ΩøÁî®„ÄÇ

## Âª∫Á≠ë/ËÆæËÆ°:

**Ê≠•È™§ 1:** ÂΩìÊ∫êË°®Âú®ÊúÄÂêé‰∏Ä‰∏™Â∞èÊó∂Ê≤°ÊúâË¢´‰øÆÊîπÊó∂„ÄÇDAG Â∑≤ÂêØÂä®ÔºåÂÆÉÊ£ÄÊü•Âà∞Ê∫êË°®Êú™Êõ¥Êñ∞ÔºåÂõ†Ê≠§ÂÆÉÈáçÊñ∞ËÆ°Âàí‰∫Ü DAG„ÄÇ‰ªªÂä° IsModifiedSource ÊòØÂ§©Á©∫È¢úËâ≤ÔºåËøôÊÑèÂë≥ÁùÄÂÆÉÂ§Ñ‰∫éÈáçÊñ∞ËÆ°ÂàíÁä∂ÊÄÅ„ÄÇ

![](img/901824d1bac13b506dd3b5ade6a68bc1.png)

**Á¨¨‰∫åÊ≠•:**‰øÆÊîπÊ∫êË°®„ÄÇÊèíÂÖ•‰∏ÄÊù°ËÆ∞ÂΩï

![](img/a11ff58f54b2a7f8035bf9ee1be91522.png)

**Ê≠•È™§ 3:** ÈáçÊñ∞Ë∞ÉÂ∫¶ÁöÑ‰ªªÂä°‚ÄúIsSourceModified‚ÄùÂºÄÂßãËøêË°åÔºå‰º†ÊÑüÂô®ËÑöÊú¨Âú®ÊØè‰∏™ poke_interval ‰πãÂêéÊ£ÄÊü•Ë°®‰øÆÊîπÊó∂Èó¥ÔºåÂè™Ë¶Å‰∏éÂΩìÂâçÂ∞èÊó∂Áõ∏ÂåπÈÖçÔºåÂ∞±ÊâßË°åËØ•‰ªªÂä°

![](img/d07dbb74f0b27c8068468c67240d0ac6.png)

**Ê≠•È™§ 4:**DAG ÂÆåÊàêÔºåÁª¥Â∫¶Ë°®‰ªé oltp Ê∫êË°®Âä†ËΩΩ

![](img/6b0a50d3c605347aaca062ef35d6063d.png)

## ‰ΩøÁî®Ê∞îÊµÅ‰º†ÊÑüÂô®ÁöÑÂ•ΩÂ§Ñ:

‰º†ÊÑüÂô®ÊòØ‰∏Ä‰∏™Êìç‰ΩúÂëò**Âú®‰∏ÄÊÆµÊó∂Èó¥Èó¥ÈöîÂÜÖËØÑ‰º∞ÊòØÂê¶Á¨¶ÂêàÊ†áÂáÜ/Êù°‰ª∂„ÄÇ**Â¶ÇÊûúÊòØÔºåÂàôÊàêÂäüÔºåÂê¶ÂàôÈáçËØïÔºåÁõ¥Âà∞Ë∂ÖÊó∂„ÄÇ

Ê∞îÊµÅ‰ºöÂ∏¶Êù•‰∏çÂêåÁöÑ‰º†ÊÑüÂô®Ôºå‰ª•‰∏ãÊòØÊúÄÂ∏∏Áî®ÁöÑ‰∏Ä‰∫õ‰º†ÊÑüÂô®:

*   *Êñá‰ª∂‰º†ÊÑüÂô®*:Á≠âÂæÖÊñá‰ª∂ÊàñÊñá‰ª∂Â§πËøõÂÖ•Êñá‰ª∂Á≥ªÁªü„ÄÇ
*   *s3key sensor*:Á≠âÂæÖ‰∏ÄÊääÈí•ÂåôÂá∫Áé∞Âú® S3 Ê°∂Èáå„ÄÇ
*   SqlSensor :ÈáçÂ§çËøêË°å‰∏ÄÊù° sql ËØ≠Âè•ÔºåÁõ¥Âà∞Êª°Ë∂≥‰∏Ä‰∏™Ê†áÂáÜ„ÄÇ
*   HivePartitionSensor :Á≠âÂæÖ‰∏Ä‰∏™ÂàÜÂå∫Âá∫Áé∞Âú® Hive ‰∏≠„ÄÇ
*   *ExternalTaskSensor*:Á≠âÂæÖ‰∏çÂêå DAG Êàñ‰∏çÂêå DAG ‰∏≠ÁöÑ‰ªªÂä°Âú®ÁâπÂÆöÊâßË°åÊó•ÊúüÂÆåÊàê„ÄÇ(Êå∫ÊúâÁî®ÁöÑÈÇ£‰∏™ü§ì)
*   *datetime sensor*:Á≠âÂæÖÊåáÂÆöÁöÑÊó•ÊúüÊó∂Èó¥(Áî®‰∫é‰∏∫ÊÇ®ÁöÑ Dag Ê∑ªÂä†‰∏Ä‰∫õÂª∂Ëøü)

## Ê†πÊçÆ‰∏öÂä°ÈÄªËæëÂàõÂª∫Áî®Êà∑ÂÆö‰πâ‰º†ÊÑüÂô®ÔºåÂç≥ BigQuery ‰º†ÊÑüÂô®:

Êàë‰ª¨ËøòÂèØ‰ª•Ê†πÊçÆÊàë‰ª¨ÁöÑ‰∏öÂä°ÈÄªËæëÂàõÂª∫ÂÆöÂà∂ÁöÑ‰º†ÊÑüÂô®„ÄÇ

ËØ•ËµÑ‰∫ßÂü∫‰∫éÁî®Êà∑ÂÆö‰πâÁöÑ‰º†ÊÑüÂô®ÔºåËØ•‰º†ÊÑüÂô®Âü∫‰∫é BigQuery ÂÆ¢Êà∑Á´ØË°® ModifiedTime„ÄÇÊàë‰ª¨ÂèØ‰ª•Âú®Â§ö‰∏™ Dag ‰∏≠ÈáçÁî®Âêå‰∏Ä‰∏™‰º†ÊÑüÂô®Êù•Ê£ÄÊü•Ëøô‰∫õÂÖàÂÜ≥Êù°‰ª∂„ÄÇÊ≠§Â§ñÔºåÈÄöËøá‰ΩøÁî®ËØ•ËÑöÊú¨‰Ωú‰∏∫Ê®°ÊùøÔºåÊàë‰ª¨ÂèØ‰ª•ËΩªÊùæÂàõÂª∫ÂÖ∑Êúâ‰∏çÂêå‰∏öÂä°ÈÄªËæëÁöÑ‰∏çÂêå BigQuery ‰º†ÊÑüÂô®„ÄÇ

*   Ëøô‰∏™‰æãÂ≠êÂåÖÊã¨Â¶Ç‰Ωï‰ªé BigQuery API ËÆøÈóÆÂÖÉÊï∞ÊçÆ‰ø°ÊÅØ
*   ËØ•Á§∫‰æãËøòÂåÖÊã¨‰ΩøÁî® Google Secret Manager Êù•‰øùÊä§ÊúçÂä°Â∏êÊà∑Âá≠ËØÅ„ÄÇ

## ËÑöÊú¨:

**1)bigquerytablemodified sensor . py**

![](img/8afb5561de01c251124511b8e630945b.png)

```
"""This module contains a Google Bigquery SQL sensor."""
from typing import Optional, Sequence, Union
from airflow.hooks import BaseHook
from airflow.sensors import BaseSensorOperator
from airflow.exceptions import AirflowException
import datetime
from google.cloud import bigquery
from google.oauth2 import service_account
from google.cloud import secretmanager
import pendulum
import logging
import json
class BigQueryTableModifiedSensor(BaseSensorOperator):
"""
Checks for the True or False output in Google Bigquery Query Job output.
:param sql: The query to return True or False as Final output
:type sql: str
:param use_legacy_sql: Option to run legacy SQL
:type use_legacy_sql: Boolean
:param bigquery_conn_id: The connection ID to use when connecting to
Google BigQuery.
:type bigquery_conn_id: str
"""
template_fields = (
'sql',
'use_legacy_sql',
'bigquery_conn_id',
'full_table_identifier',
'secret_manager_project_name',
'secret_name'
)
ui_color = '#f0eee2'
# accept table identifiers from main DAG
def __init__(self,*,bigquery_conn_id: str = 'google_cloud_default',
use_legacy_sql: bool = False,sql: str = None,full_table_identifier,secret_manager_project_name,secret_name,**kwargs) -> None:
super().__init__(**kwargs)
self.bigquery_conn_id = bigquery_conn_id
self.sql = None
self.use_legacy_sql = use_legacy_sql
self.full_table_identifier=full_table_identifier
self.secret_manager_project_name=secret_manager_project_name
self.secret_name=secret_name
def access_secret_version(self,project_id, secret_id, version_id):
# Create the Secret Manager client.
client = secretmanager.SecretManagerServiceClient()
# Build the resource name of the secret version.
name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"
logging.info(name)
logging.info("inside get_table_modified before accessing")
# Access the secret version.
response = client.access_secret_version(name)
payload = response.payload.data.decode("UTF-8")
#logging.info(payload)
return payload
def poke(self,context: dict) -> bool:
full_table_identifier=self.full_table_identifier
secret_manager_project_name=self.secret_manager_project_name
secret_name=self.secret_name
key_content=self.access_secret_version(secret_manager_project_name,
secret_name,"1")
service_account_info = json.loads(key_content,strict=False)
logging.info("returned from gsm function")
logging.info(service_account_info)
credentials=
service_account.Credentials.from_service_account_info(service_account_info)
client = bigquery.Client(credentials=credentials,project=credentials.project_id)
table = client.get_table(full_table_identifier)
logging.info(table.modified)
logging.info(table.modified.date())
logging.info(table.modified.hour)
if table.modified.hour == datetime.datetime.now().hour:
logging.info("IsModifiedThisHour TRUE")
else:
logging.info("IsModifiedThisHour False")
return table.modified.hour == datetime.datetime.now().hour
```

**2)Ê†∑Êú¨ DAG ‰ΩøÁî®Ê∞îÊµÅÊèí‰ª∂‰Ωú‰∏∫Áî®Êà∑ÂÆö‰πâÁöÑ‰º†ÊÑüÂô®**

![](img/cb3b6b4d17ec242a6a0d4a20ab744814.png)

```
from sensors.BigQueryTableModifiedSensor import BigQueryTableModifiedSensor
import os
import json
import datetime
import airflow
from airflow import DAG
from airflow import models
from airflow.models import Variable
from airflow.operators import DummyOperator
from airflow.operators import BashOperator
from airflow.contrib.operators import bigquery_to_bigquery
from airflow.contrib.operators import bigquery_to_gcs
from airflow.contrib.operators import gcs_to_bq
from airflow.contrib.operators import bigquery_operator
from airflow.providers.google.cloud.operators.bigquery import BigQueryExecuteQueryOperator
import datetime
from google.cloud import bigquery
from google.oauth2 import service_account
from google.cloud import secretmanager
import pendulum
import logging
import re
from airflow.contrib.operators.bigquery_operator import BigQueryOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from airflow.contrib.operators.gcs_list_operator import GoogleCloudStorageListOperator
from airflow.contrib.operators.gcs_to_gcs import GoogleCloudStorageToGoogleCloudStorageOperator
from google.cloud import storage
from airflow.providers.google.cloud.operators.bigquery import BigQueryExecuteQueryOperator
from airflow.operators import DummyOperator
from airflow.operators.python_operator import PythonOperator
logger = logging.getLogger("airflow.task")
local_tz = pendulum.timezone("Europe/Amsterdam")
secret_manager_project_name=Variable.get('secret_manager_project')
secret_name=Variable.get('secret_name')
project_name=Variable.get('project_name')
dataset_name=Variable.get('dataset_name')
source_table_name=Variable.get('source_table_name')
full_table_identifier=project_name+'.'+dataset_name+'.'+source_table_name
target_table_name=Variable.get('target_table_name')
#Default args for the DAG execution
default_args = {
'email_on_failure': True,
'email_on_retry': False
}
#DAG initialization
#DAG initialization
with airflow.DAG(
'load_oltp_table_to_dimension_table',
start_date=datetime.datetime(2021, 1, 1),
schedule_interval=None) as dag:
logging.info("inside DAG")
start = DummyOperator(
task_id='start',
dag=dag)
finish = DummyOperator(
task_id='finish',
dag=dag)
check_precondition = BigQueryTableModifiedSensor(
task_id ='IsSourceModified',
bigquery_conn_id = 'bigquery_default',
use_legacy_sql = False,
sql = None,
full_table_identifier=full_table_identifier,
secret_manager_project_name=secret_manager_project_name,
secret_name=secret_name,
poke_interval = 60,
timeout = 3600,
mode = 'reschedule',
dag=dag
)
run_new_job = BigQueryOperator(
task_id='RunLoadJobWhenSourceModified',
sql='insert into `'+project_name+'.'+dataset_name+'.'+target_table_name+'` select generate_uuid(),c.customer_id,c.customer_name,c.customer_city,current_timestamp(),null,"Yes" from `'+project_name+'.'+dataset_name+'.'+source_table_name+'` c;',
use_legacy_sql=False,
bigquery_conn_id='bigquery_default',
dag=dag)
start>>check_precondition>>run_new_job>>finish
```

# ‰º†ÊÑüÂô®ËÑöÊú¨ÁöÑËß£Èáä:

ÊâÄÊúâ‰º†ÊÑüÂô®ÈÉΩÁªßÊâøËá™ BaseSensorOperatorÔºåÂπ∂ÂÖ∑Êúâ‰ª•‰∏ãÂèÇÊï∞:

*   Ê®°Âºè:‰º†ÊÑüÂô®Â¶Ç‰ΩïÂ∑•‰Ωú„ÄÇÊúâ‰∏§ÁßçÊ®°Âºè:
*   Êà≥:ËøôÊòØÈªòËÆ§Ê®°Âºè„ÄÇ‰ΩøÁî® poke Êó∂Ôºå‰º†ÊÑüÂô®‰ºöÂú®Êï¥‰∏™ÊâßË°åÊó∂Èó¥ÂÜÖÂç†Áî®‰∏Ä‰∏™Â∑•‰ΩúÊèíÊßΩÔºåÂπ∂Âú®‰∏§Ê¨° poke ‰πãÈó¥‰ºëÁú†„ÄÇ
*   ÈáçÊñ∞ÂÆâÊéí:Â¶ÇÊûú‰∏çÁ¨¶ÂêàÊ†áÂáÜÔºå‰º†ÊÑüÂô®Â∞ÜÈáäÊîæÂÖ∂Â∑•‰ΩúÊèíÊßΩÔºåÂπ∂ÈáçÊñ∞ÂÆâÊéí‰∏ãÊ¨°Ê£ÄÊü•Êó∂Èó¥„ÄÇ
*   poke_interval:‰ΩøÁî® poke Ê®°ÂºèÊó∂ÔºåËøôÊòØ‰º†ÊÑüÂô®Âú®ÂÜçÊ¨°Ê£ÄÊü•Êù°‰ª∂‰πãÂâçÁ≠âÂæÖÁöÑÊó∂Èó¥(Áßí)„ÄÇÈªòËÆ§ÂÄº‰∏∫ 30 Áßí„ÄÇ
*   Ë∂ÖÊó∂:‰º†ÊÑüÂô®Ê£ÄÊü•Áä∂ÂÜµÁöÑÊúÄÈïøÊó∂Èó¥(Áßí)„ÄÇÂ¶ÇÊûúËææÂà∞ËØ•Êó∂Èó¥Êó∂ËøòÊ≤°ÊúâÊª°Ë∂≥Êù°‰ª∂ÔºåÂàô‰ªªÂä°Â§±Ë¥•„ÄÇ

Ê†πÊçÆÁªôÂÆöÁöÑ‰∏öÂä°ÈÄªËæëÔºåÂèØ‰ª•Âú®‰º†ÊÑüÂô®Á±ªÁöÑ __init__ ÊñπÊ≥ï‰∏≠‰º†ÈÄíÂèØÂèòÊï∞ÈáèÁöÑÂèÇÊï∞„ÄÇ

ÁÑ∂ÂêéÔºåÂèØ‰ª•‰ªé‰ªª‰Ωï DAG Ë∞ÉÁî®‰º†ÊÑüÂô®Êù•Ê£ÄÊü•ÊòØÂê¶Êª°Ë∂≥ÂâçÊèêÊù°‰ª∂„ÄÇ

```
check_precondition = BigQueryTableModifiedSensor(
task_id ='IsSourceModified',
bigquery_conn_id = 'bigquery_default',
use_legacy_sql = False,
sql = None,
full_table_identifier=full_table_identifier,
secret_manager_project_name=secret_manager_project_name,
secret_name=secret_name, - - - Only needed if you use Google Secret Manager
poke_interval = 60,
timeout = 3600,
mode = 'reschedule',
dag=dag
)
```

# Â¶Ç‰ΩïÂú® DAG ‰∏≠‰ΩøÁî® BigQuery ‰º†ÊÑüÂô®ËÑöÊú¨

‰ΩøÁî®Á¨¨‰∏Ä‰∏™ËÑöÊú¨ bigquerytablemodifiedssensor . py ÂàõÂª∫‰º†ÊÑüÂô®„ÄÇ

Ê†πÊçÆ‰∏öÂä°ÈÄªËæë‰øÆÊîπËÑöÊú¨„ÄÇ

Â∞ÜËÑöÊú¨Êñá‰ª∂ÊîæÂú®Ê∞îÊµÅÁéØÂ¢ÉÁöÑ‰ª•‰∏ãË∑ØÂæÑ‰∏≠

Ê∞îÊµÅ-ÁéØÂ¢É-Ê°∂/Êèí‰ª∂/‰º†ÊÑüÂô®/bigquerytablemodified sensor**„ÄÇ** py

![](img/57e6ca5a28932deb936a4c8d46d21c08.png)

Â∞Ü BigQueryTableModifiedSensor Ê®°ÂùóÂØºÂÖ•Âà∞ÊÇ®ÊÉ≥Ë¶Å‰ΩøÁî®ÂÆÉÁöÑ DAG ‰∏≠„ÄÇ