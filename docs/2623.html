<html>
<head>
<title>[PySpark] Load data from GCS to Bigtable — using GCP Dataproc Serverless</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[PySpark]将数据从GCS加载到Bigtable —使用GCP Dataproc无服务器</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/pyspark-load-data-from-gcs-to-bigtable-using-gcp-dataproc-serverless-c373430fe157?source=collection_archive---------2-----------------------#2022-12-13">https://medium.com/google-cloud/pyspark-load-data-from-gcs-to-bigtable-using-gcp-dataproc-serverless-c373430fe157?source=collection_archive---------2-----------------------#2022-12-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/36156d0b62d0c7f90c2a56fc763a772e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mR0lvdpsmUHam_lBXIzoHw.png"/></div></div></figure><p id="2b05" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Google Cloud Dataproc平台的最新功能之一，<a class="ae jo" href="https://cloud.google.com/dataproc-serverless/docs/overview" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> Dataproc无服务器</strong> </a>，使客户无需创建或维护集群即可运行Spark工作负载。一旦Spark工作负载参数被指定并且任务被提交给服务，Dataproc Serverless将在后台处理所有必要的基础设施。它使开发人员能够专注于应用程序的基本逻辑，而不是花时间管理框架。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es jp"><img src="../Images/5fea88c822de1ac7819c14cf6f84e146.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*z45oKuSioRn330FRO-tcPg.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">谷歌云数据平台</figcaption></figure><p id="4e39" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于有了<a class="ae jo" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> Dataproc模板</strong> </a>，我们无需从头开始创建它们，就可以使用Java和Python在Dataproc无服务器上运行典型用例。借助这些模板，我们可以轻松定制和运行常见的Spark工作负载。</p><p id="0e59" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您正在寻找一个<strong class="is hj"> <em class="jy"> PySpark模板来使用Dataproc Serverless </em> </strong>将数据从GCS移动到Bigtable，那么这篇博客文章会很有用。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jz"><img src="../Images/83f93236eb5fef3f88de7c9b8be0c310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ji1A2nLDVWx7mv1n5ceQMg.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated"><strong class="bd ka"> GCS到云Bigtable </strong></figcaption></figure><h2 id="d9e4" class="kb kc hi bd ka kd ke kf kg kh ki kj kk jb kl km kn jf ko kp kq jj kr ks kt ku bi translated">主要优势</h2><ul class=""><li id="f814" class="kv kw hi is b it kx ix ky jb kz jf la jj lb jn lc ld le lf bi translated"><a class="ae jo" href="https://github.com/GoogleCloudPlatform/dataproc-templates/tree/main/python/dataproc_templates/gcs" rel="noopener ugc nofollow" target="_blank"><strong class="is hj">GCSToBigTable</strong></a>模板开源，配置驱动，随时可用。</li><li id="6341" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated">通过简单地改变连接参数，这些模板可以相对快速地用于具有相同需求的用例。</li><li id="f6f1" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated">这些模板本质上是可定制的。这意味着只需进行必要的代码修改，GitHub库就可以在未来需要时被快速、轻松地克隆和利用。</li><li id="f544" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated">支持的文件格式有JSON、CSV、Parquet和Avro。</li></ul><h2 id="e9ef" class="kb kc hi bd ka kd ke kf kg kh ki kj kk jb kl km kn jf ko kp kq jj kr ks kt ku bi translated">先决条件</h2><p id="5f26" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">为了运行这些模板，我们需要:</p><ul class=""><li id="8e0f" class="kv kw hi is b it iu ix iy jb lo jf lp jj lq jn lc ld le lf bi translated"><a class="ae jo" href="https://cloud.google.com/sdk/docs/install" rel="noopener ugc nofollow" target="_blank"> Google Cloud SDK </a>已安装并通过认证。你可以在谷歌云控制台中使用云外壳，通过这个<a class="ae jo" href="https://console.cloud.google.com/cloudshell/editor" rel="noopener ugc nofollow" target="_blank">链接</a>获得一个已经配置好的环境。</li><li id="ecde" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated">Python 3.7+已安装并添加到PATH变量中。</li><li id="f8d9" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated">登录到您的GCP项目并启用Dataproc API，如果禁用的话。</li><li id="7a0c" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated">请确保您已经启用了带有专用Google访问的子网。如果你要使用“默认”的VPC网络由GCP生成，你仍然需要启用私人访问如下</li></ul><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/cb75b374b6aecfba1c5076471f951fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/0*Mj8dSrr78PC3Hthi.png"/></div></figure><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="8e39" class="lx kc hi lt b be ly lz l ma mb">gcloud compute networks subnets update default --region=us-central1 --enable-private-ip-google-access</span></pre><h2 id="47a7" class="kb kc hi bd ka kd ke kf kg kh ki kj kk jb kl km kn jf ko kp kq jj kr ks kt ku bi translated">必需的JAR文件</h2><p id="9d79" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">提交作业时，必须提供一些HBase和BigTable依赖项。这些依赖关系必须使用-jars标志提供，或者在Dataproc模板的情况下，使用jars环境变量。</p><p id="8b98" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一些依赖项(jar)必须从<a class="ae jo" href="https://mvnrepository.com/" rel="noopener ugc nofollow" target="_blank"> MVN仓库</a>中下载，并放入您的GCS桶中(创建一个来存储依赖项)。</p><p id="c90c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="https://mvnrepository.com/artifact/org.apache.hbase.connectors.spark/hbase-spark" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> Apache HBase Spark连接器</strong> </a> <strong class="is hj">依赖关系</strong>(已经挂载在Dataproc Serverless中，所以使用file://)引用它们:</p><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="74de" class="lx kc hi lt b be ly lz l mc mb">file:///usr/lib/spark/external/hbase-spark-protocol-shaded.jar<br/>file:///usr/lib/spark/external/hbase-spark.jar</span></pre><p id="bdcd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Bigtable依赖关系</strong></p><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="bb3a" class="lx kc hi lt b be ly lz l mc mb">gs://&lt;your_bucket_to_store_dependencies&gt;/<strong class="lt hj">bigtable-hbase-2.x-hadoop-2.3.0.jar</strong><br/><br/>Download it using <strong class="lt hj">wget</strong> <br/><br/><a class="ae jo" href="https://repo1.maven.org/maven2/com/google/cloud/bigtable/bigtable-hbase-2.x-shaded/2.3.0/bigtable-hbase-2.x-shaded-2.3.0.jar" rel="noopener ugc nofollow" target="_blank">https://repo1.maven.org/maven2/com/google/cloud/bigtable/bigtable-hbase-2.x-shaded/2.3.0/bigtable-hbase-2.x-shaded-2.3.0.jar</a></span></pre><p id="eb2c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> HBase依赖关系</strong></p><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="6233" class="lx kc hi lt b be ly lz l mc mb">gs://&lt;your_bucket_to_store_dependencies&gt;/<strong class="lt hj">hbase-client-2.4.12.jar</strong><br/><br/>Download it using <strong class="lt hj">wget</strong> <br/><br/><a class="ae jo" href="https://repo1.maven.org/maven2/org/apache/hbase/hbase-client/2.4.12/hbase-client-2.4.12.jar" rel="noopener ugc nofollow" target="_blank">https://repo1.maven.org/maven2/org/apache/hbase/hbase-client/2.4.12/hbase-client-2.4.12.jar</a></span></pre><pre class="md ls lt lu bn lv lw bi"><span id="20c2" class="lx kc hi lt b be ly lz l mc mb">gs://&lt;your_bucket_to_store_dependencies&gt;/<strong class="lt hj">hbase-shaded-mapreduce-2.4.12.jar</strong><br/><br/>Download it using <strong class="lt hj">wget</strong> <br/><br/><a class="ae jo" href="https://repo1.maven.org/maven2/org/apache/hbase/hbase-shaded-mapreduce/2.4.12/hbase-shaded-mapreduce-2.4.12.jar" rel="noopener ugc nofollow" target="_blank">https://repo1.maven.org/maven2/org/apache/hbase/hbase-shaded-mapreduce/2.4.12/hbase-shaded-mapreduce-2.4.12.jar</a></span></pre><h2 id="1b5f" class="kb kc hi bd ka kd ke kf kg kh ki kj kk jb kl km kn jf ko kp kq jj kr ks kt ku bi translated">GCSToBigTable模板要求</h2><p id="1446" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">它使用Apache HBase Spark连接器写入BigTable。</p><p id="7c69" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个模板中，您会注意到使用Dataproc Serverless运行PySpark作业有不同的配置步骤，使用HBase接口连接到BigTable。</p><ol class=""><li id="398c" class="kv kw hi is b it iu ix iy jb lo jf lp jj lq jn me ld le lf bi translated">用您的BigTable实例引用配置<a class="ae jo" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/python/dataproc_templates/gcs/hbase-site.xml" rel="noopener ugc nofollow" target="_blank"> hbase-site.xml </a> ( <a class="ae jo" href="https://cloud.google.com/bigtable/docs/hbase-connecting#creating_the_hbase-sitexml_file" rel="noopener ugc nofollow" target="_blank">引用</a>)。hbase-site.xml需要在Dataproc Serverless使用的容器映像的某个路径中可用。为此，您需要在GCP容器注册中心构建并托管一个<a class="ae jo" href="https://cloud.google.com/dataproc-serverless/docs/guides/custom-containers#submit_a_spark_batch_workload_using_a_custom_container_image" rel="noopener ugc nofollow" target="_blank">客户容器映像</a>。</li></ol><ul class=""><li id="56fd" class="kv kw hi is b it iu ix iy jb lo jf lp jj lq jn lc ld le lf bi translated">将以下层添加到<a class="ae jo" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/python/dataproc_templates/gcs/Dockerfile" rel="noopener ugc nofollow" target="_blank"> Dockerfile </a>中，以便它将您的本地hbase-site.xml复制到容器映像中(已经完成) :</li></ul><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="6bc2" class="lx kc hi lt b be ly lz l ma mb">COPY hbase-site.xml /etc/hbase/conf/</span></pre><ul class=""><li id="14e8" class="kv kw hi is b it iu ix iy jb lo jf lp jj lq jn lc ld le lf bi translated">构建<a class="ae jo" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/python/dataproc_templates/gcs/Dockerfile" rel="noopener ugc nofollow" target="_blank"> Dockerfile </a>，构建并将其推送到GCP集装箱注册中心:</li></ul><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="9800" class="lx kc hi lt b be ly lz l ma mb">IMAGE=gcr.io/&lt;your_project&gt;/&lt;your_custom_image&gt;:&lt;your_version&gt;<br/>docker build -t "${IMAGE}" .<br/>docker push "${IMAGE}"</span></pre><ul class=""><li id="0007" class="kv kw hi is b it iu ix iy jb lo jf lp jj lq jn lc ld le lf bi translated">提交作业时，SPARK_EXTRA_CLASSPATH环境变量也应该设置为相同的路径</li></ul><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="4270" class="lx kc hi lt b be ly lz l ma mb">--container-image="gcr.io/&lt;your_project&gt;/&lt;your_custom_image&gt;:&lt;your_version&gt;"  # image with hbase-site.xml in /etc/hbase/conf/<br/>--properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/'</span></pre><p id="c17d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.将所需的HBase目录json配置为作为参数传递(表引用和模式)。应使用-GCS . bigtable . h base . catalog . JSON传递hbase-catalog.json</p><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="0963" class="lx kc hi lt b be ly lz l ma mb">--gcs.bigtable.hbase.catalog.json='''{<br/>                    "table":{"namespace":"default","name":"&lt;table_id&gt;"},<br/>                    "rowkey":"key",<br/>                    "columns":{<br/>                    "key":{"cf":"rowkey", "col":"key", "type":"string"},<br/>                    "name":{"cf":"cf", "col":"name", "type":"string"}<br/>                    }<br/>                }'''</span></pre><p id="aab1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.<a class="ae jo" href="https://cloud.google.com/bigtable/docs/managing-tables" rel="noopener ugc nofollow" target="_blank">创建和管理</a>您的Bigtable表模式、列族等，以匹配所提供的HBase目录。</p><h2 id="2d69" class="kb kc hi bd ka kd ke kf kg kh ki kj kk jb kl km kn jf ko kp kq jj kr ks kt ku bi translated">配置参数</h2><p id="bc22" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">该模板包括以下参数来配置执行:</p><ul class=""><li id="a653" class="kv kw hi is b it iu ix iy jb lo jf lp jj lq jn lc ld le lf bi translated"><code class="du mf mg mh lt b">gcs.bigtable.input.location</code>:输入文件的GCS位置(格式:<code class="du mf mg mh lt b">gs://&lt;bucket&gt;/...</code>)</li><li id="a77c" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated"><code class="du mf mg mh lt b">gcs.bigtable.input.format</code>:输入文件格式(avro、parquet、csv、json中的一种)</li><li id="23c3" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated"><code class="du mf mg mh lt b">gcs.bigtable.hbase.catalog.json</code>:h基本目录内联json</li></ul><h2 id="896a" class="kb kc hi bd ka kd ke kf kg kh ki kj kk jb kl km kn jf ko kp kq jj kr ks kt ku bi translated">执行GCSToBigTable Dataproc模板的步骤</h2><p id="704e" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">1.创建一个GCS存储桶，用作Dataproc的暂存位置。这个桶将用于存储运行我们的无服务器集群所需的依赖项/ Jar文件。</p><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="25ff" class="lx kc hi lt b be ly lz l ma mb">export STAGING_BUCKET=&lt;gcs-staging-bucket-folder&gt;<br/>gsutil mb gs://$STAGING_BUCKET</span></pre><p id="9cce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.克隆Dataproc模板存储库，并导航到Python模板的目录。</p><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="9b5c" class="lx kc hi lt b be ly lz l ma mb">git clone https://github.com/GoogleCloudPlatform/dataproc-templates.git<br/><br/>cd dataproc-templates/python</span></pre><p id="5c3c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.获取身份验证凭据(以提交作业)。</p><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="958f" class="lx kc hi lt b be ly lz l ma mb">gcloud auth application-default login</span></pre><p id="608e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.配置Dataproc无服务器作业</p><p id="e6bd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了将作业提交给Dataproc Serverless，我们将使用提供的bin/start.sh脚本。该脚本要求我们使用环境变量来配置Dataproc无服务器集群。</p><p id="8d79" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">强制配置包括:</p><ul class=""><li id="7f07" class="kv kw hi is b it iu ix iy jb lo jf lp jj lq jn lc ld le lf bi translated"><code class="du mf mg mh lt b">GCP_PROJECT</code>:在其上运行Dataproc Serverless的GCP项目。</li><li id="a7d2" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated"><code class="du mf mg mh lt b">REGION</code>:运行Dataproc无服务器的区域。</li><li id="1fb5" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated"><code class="du mf mg mh lt b">GCS_STAGING_LOCATION</code>:一个GCS位置，Dataproc将在此存储登台资产。应该在我们之前创建的桶内。</li></ul><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="8b36" class="lx kc hi lt b be ly lz l ma mb">export GCP_PROJECT=&lt;project_id&gt;<br/>export REGION=&lt;region&gt;<br/>export GCS_STAGING_LOCATION=gs://$STAGING_BUCKET/staging</span></pre><p id="2028" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">导出jar的环境变量。您还可以选择将JAR文件存储在您自己的存储桶中。</p><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="0e95" class="lx kc hi lt b be ly lz l ma mb">export JARS="gs://&lt;your_bucket_to_store_dependencies&gt;/bigtable-hbase-2.x-hadoop-2.3.0.jar, \<br/>             gs://&lt;your_bucket_to_store_dependencies&gt;/hbase-client-2.4.12.jar, \<br/>             gs://&lt;your_bucket_to_store_dependencies&gt;/hbase-shaded-mapreduce-2.4.12.jar, \<br/>             file:///usr/lib/spark/external/hbase-spark-protocol-shaded.jar, \<br/>             file:///usr/lib/spark/external/hbase-spark.jar"</span></pre><p id="11b8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5.执行GCS到Bigtable Dataproc模板</p><p id="0b34" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">配置作业后，我们就可以触发它了。我们将运行<code class="du mf mg mh lt b"><strong class="is hj">bin/start.sh</strong></code>脚本，指定我们想要运行的模板和执行的参数值。</p><pre class="jq jr js jt fd ls lt lu bn lv lw bi"><span id="3706" class="lx kc hi lt b be ly lz l ma mb">./bin/start.sh \<br/>--container-image="gcr.io/&lt;your_project&gt;/&lt;your_custom_image&gt;:&lt;your_version&gt;" \<br/>--properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/' \ # image with hbase-site.xml in /etc/hbase/conf/<br/>-- --template=GCSTOBIGTABLE \<br/>   --gcs.bigtable.input.format="&lt;json|csv|parquet|avro&gt;" \<br/>   --gcs.bigtable.input.location="&lt;gs://bucket/path&gt;" \<br/>   --gcs.bigtable.hbase.catalog.json='''{<br/>                        "table":{"namespace":"default","name":"my_table"},<br/>                        "rowkey":"key",<br/>                        "columns":{<br/>                        "key":{"cf":"rowkey", "col":"key", "type":"string"},<br/>                        "name":{"cf":"cf", "col":"name", "type":"string"}<br/>                        }<br/>                    }'''</span></pre><p id="b22e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">6.监控Spark批处理作业</p><p id="949f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">提交作业后，我们将能够在<a class="ae jo" href="https://console.cloud.google.com/dataproc/batches" rel="noopener ugc nofollow" target="_blank"> Dataproc批处理UI </a>中看到。从那里，我们可以查看作业的指标和日志。</p><h2 id="92a1" class="kb kc hi bd ka kd ke kf kg kh ki kj kk jb kl km kn jf ko kp kq jj kr ks kt ku bi translated">参考</h2><ul class=""><li id="22a2" class="kv kw hi is b it kx ix ky jb kz jf la jj lb jn lc ld le lf bi translated"><a class="ae jo" href="https://cloud.google.com/dataproc-serverless/docs/overview" rel="noopener ugc nofollow" target="_blank"> Dataproc无服务器</a></li><li id="bd44" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated"><a class="ae jo" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> Dataproc模板库</a></li></ul><p id="5675" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如有任何疑问/建议，请联系:<strong class="is hj">data proc-templates-support-external</strong>@ Google groups . com</p></div></div>    
</body>
</html>