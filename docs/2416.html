<html>
<head>
<title>Processing and migrating large data tables from Hive to GCS using Java and Dataproc Serverless</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Java和Dataproc无服务器处理大型数据表并将其从Hive迁移到GCS</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/processing-and-migrating-large-data-tables-from-hive-to-gcs-using-java-and-dataproc-serverless-b6dbbae61c5d?source=collection_archive---------1-----------------------#2022-10-11">https://medium.com/google-cloud/processing-and-migrating-large-data-tables-from-hive-to-gcs-using-java-and-dataproc-serverless-b6dbbae61c5d?source=collection_archive---------1-----------------------#2022-10-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/4b0cba1ab6b1039c0183a22d4d6c720f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*ftdqA9tnVbAOU8s3.png"/></div></figure><p id="1662" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi jk translated">ataproc是一个完全托管的云服务，用于在Google云平台上运行Apache Spark工作负载。它会创建临时集群，而不是为我们的所有作业配置和维护一个集群。</p><p id="063d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jt" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/README.md" rel="noopener ugc nofollow" target="_blank"> Dataproc模板</a>提供了一种灵活易用的机制，用于在Dataproc无服务器上管理和执行用例，而无需开发它们。</p><p id="4571" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这些模板实现了常见的Spark工作负载，让我们可以轻松地定制和运行它们。</p><h1 id="5eaa" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">目标</h1><p id="cdd0" class="pw-post-body-paragraph im in hi io b ip ks ir is it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj hb bi translated">这篇博客文章阐述了如何使用Dataproc Serverless处理大量工作负载并将其从<a class="ae jt" href="https://docs.cloudera.com/runtime/7.2.1/hive-hms-overview/topics/hive-hms-introduction.html" rel="noopener ugc nofollow" target="_blank"> Apache Hive Metastore </a>迁移到Google cloud。</p><h1 id="688a" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">先决条件</h1><p id="ffd6" class="pw-post-body-paragraph im in hi io b ip ks ir is it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj hb bi translated">为了运行这些模板，我们需要:</p><ul class=""><li id="7bfc" class="kx ky hi io b ip iq it iu ix kz jb la jf lb jj lc ld le lf bi translated">Google Cloud SDK已安装并通过验证</li><li id="30d6" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated">启用了专用Google访问的VPC子网。默认子网是合适的，只要启用了私有Google访问。您可以在此查看所有Dataproc无服务器网络需求<a class="ae jt" href="https://cloud.google.com/dataproc-serverless/docs/concepts/network" rel="noopener ugc nofollow" target="_blank">。</a></li></ul><h1 id="7139" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">主要优势</h1><ul class=""><li id="5e9c" class="kx ky hi io b ip ks it kt ix ll jb lm jf ln jj lc ld le lf bi translated">使用Dataproc Serverless运行Spark batch工作负载，而无需提供和管理您自己的集群。</li><li id="8234" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated">Hive to GCS模板是开源的，完全可定制，并可用于简单的工作。</li><li id="5846" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated">您可以从Hive摄取AVRO、CSV、ORC和JSON格式的数据到GCS。</li></ul><h1 id="4f58" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">配置参数</h1><p id="ecb9" class="pw-post-body-paragraph im in hi io b ip ks ir is it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj hb bi translated">模板中包含以下属性来配置执行—</p><ul class=""><li id="16b4" class="kx ky hi io b ip iq it iu ix kz jb la jf lb jj lc ld le lf bi translated"><code class="du lo lp lq lr b">spark.sql.warehouse.dir=&lt;warehouse-path&gt;</code>:Spark SQL Hive仓库的位置路径，Spark SQL在该仓库中保存表。</li><li id="eb73" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated"><code class="du lo lp lq lr b">hive.gcs.output.path=&lt;gcs-output-path&gt;</code> : GCS输入路径。</li><li id="68fb" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated"><code class="du lo lp lq lr b">hive.gcs.output.path=&lt;gcs-output-path&gt;</code> : GCS输出路径。</li><li id="a312" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated"><code class="du lo lp lq lr b">hive.input.table=&lt;hive-input-table&gt;</code> : Hive输入表名。</li><li id="1cfe" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated"><code class="du lo lp lq lr b">hive.input.db=&lt;hive-output-db&gt;</code> : Hive输入数据库名称。</li><li id="dc30" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated"><code class="du lo lp lq lr b">hive.gcs.output.format=avro</code> : GCS输出文件格式。这可以是avro，csv，paraquet，json，orc。默认输出路径设置为avro。</li><li id="0497" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated"><code class="du lo lp lq lr b">hive.partition.col=&lt;hive-partition-col&gt;</code>:对配置单元数据进行分区的列名。</li><li id="cf43" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated"><code class="du lo lp lq lr b">hive.gcs.save.mode=overwrite</code>:将写入模式设置为GCS。默认参数值是overwrite。你可以在这里了解每种保存模式的表现<a class="ae jt" href="https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes" rel="noopener ugc nofollow" target="_blank">。</a></li></ul><p id="cd49" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">“Hive to GCS”模板还有另外两个可选属性，用于在加载到GCS之前应用spark sql转换</p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="bef6" class="ma jv hi lr b fi mb mc l md me">--templateProperty hive.gcs.temp.table='temporary_view_name' <br/>--templateProperty hive.gcs.temp.query='select * from global_temp.temporary_view_name'</span></pre><blockquote class="mf mg mh"><p id="acbe" class="im in mi io b ip iq ir is it iu iv iw mj iy iz ja mk jc jd je ml jg jh ji jj hb bi translated"><strong class="io hj">注意:</strong>当使用转换属性时，Spark临时视图的名称和查询中的表的名称应该完全匹配，以避免“table view not found”错误。</p></blockquote><h1 id="15c9" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">使用</h1><ol class=""><li id="bfe1" class="kx ky hi io b ip ks it kt ix ll jb lm jf ln jj mm ld le lf bi translated">在GCS中创建一个暂存桶，以存储运行无服务器集群所需的依赖项。</li></ol><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="0ab3" class="ma jv hi lr b fi mb mc l md me">export GCS_STAGING_BUCKET=”my-gcs-staging-bucket”<br/>gsutil mb gs://$GCS_STAGING_BUCKET</span></pre><p id="0cca" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.克隆Dataproc模板库并导航到Java模板文件夹。</p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="0ceb" class="ma jv hi lr b fi mb mc l md me">git clone <a class="ae jt" href="https://github.com/GoogleCloudPlatform/dataproc-templates.git" rel="noopener ugc nofollow" target="_blank">https://github.com/GoogleCloudPlatform/dataproc-templates.git</a><br/>cd dataproc-templates/java</span></pre><p id="3354" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">3.通过导出提交所需的变量来配置Dataproc无服务器作业—</p><p id="5f8c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将使用提供的<code class="du lo lp lq lr b">bin/start.sh</code>脚本，并使用以下强制环境变量进行配置，以将作业提交给dataproc无服务器:</p><ul class=""><li id="6038" class="kx ky hi io b ip iq it iu ix kz jb la jf lb jj lc ld le lf bi translated"><code class="du lo lp lq lr b">GCP_PROJECT</code>:运行Dataproc无服务器的GCP项目id。</li><li id="3545" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated"><code class="du lo lp lq lr b">REGION</code>:运行Dataproc无服务器的区域。</li><li id="444f" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated"><code class="du lo lp lq lr b">SUBNET</code>:Hive warehouse所在的子网，以便在同一个子网中启动作业。</li><li id="a892" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated"><code class="du lo lp lq lr b">GCS_STAGING_BUCKET</code> : GCS staging bucket位置，Dataproc将在此存储staging资产(应该在之前创建的bucket内)。</li></ul><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="b6e6" class="ma jv hi lr b fi mb mc l md me"># GCP project id to run the dataproc serverless job<br/>GCP_PROJECT=&lt;gcp-project-id&gt; </span><span id="9904" class="ma jv hi lr b fi mn mc l md me"># GCP region where the job needs to be submitted<br/>REGION=&lt;region&gt;</span><span id="95f2" class="ma jv hi lr b fi mn mc l md me"># Subnet where the hive warehouse exists<br/>SUBNET=&lt;subnet&gt;</span><span id="34bf" class="ma jv hi lr b fi mn mc l md me"># Staging storage location for Dataproc Serverless (Already done in step 1)<br/>GCS_STAGING_LOCATION=&lt;gcs-staging-bucket-folder&gt;</span></pre><p id="0a8e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">4.执行配置单元到GCS Dataproc模板—</p><p id="a1e6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">配置作业后，我们现在将触发<code class="du lo lp lq lr b">bin/start.sh</code>,指定我们想要运行的模板以及执行的参数值。</p><blockquote class="mf mg mh"><p id="fd0e" class="im in mi io b ip iq ir is it iu iv iw mj iy iz ja mk jc jd je ml jg jh ji jj hb bi translated">注意:提交作业时应该启用Dataproc API。</p></blockquote><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="ad67" class="ma jv hi lr b fi mb mc l md me">bin/start.sh \<br/>--properties=spark.hadoop.hive.metastore.uris=thrift://&lt;hostname-or-ip&gt;:9083 \<br/>-- --template HIVETOGCS \<br/>--templateProperty hive.input.table=&lt;table&gt; \<br/>--templateProperty hive.input.db=&lt;database&gt; \<br/>--templateProperty hive.gcs.output.path=&lt;gcs-output-path&gt;</span></pre><p id="790b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">5.监控和查看Spark批处理作业</p><p id="7e74" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在<a class="ae jt" href="https://console.cloud.google.com/dataproc/batches" rel="noopener ugc nofollow" target="_blank"> Dataproc Batches UI </a>中提交作业后，您可以监控日志并查看指标。</p><h1 id="8289" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">参考</h1><ul class=""><li id="241d" class="kx ky hi io b ip ks it kt ix ll jb lm jf ln jj lc ld le lf bi translated"><a class="ae jt" href="https://cloud.google.com/dataproc-serverless/docs/overview" rel="noopener ugc nofollow" target="_blank"> Dataproc无服务</a></li><li id="f3b2" class="kx ky hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated"><a class="ae jt" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> Dataproc模板库</a></li></ul></div></div>    
</body>
</html>