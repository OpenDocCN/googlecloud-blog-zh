<html>
<head>
<title>Ingress load balancing issues on Google’s GKE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谷歌GKE的入口负载平衡问题</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/ingress-load-balancing-issues-on-googles-gke-f54c7e194dd5?source=collection_archive---------0-----------------------#2019-04-12">https://medium.com/google-cloud/ingress-load-balancing-issues-on-googles-gke-f54c7e194dd5?source=collection_archive---------0-----------------------#2019-04-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8edd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常我在这里的帖子都是关于一些我认为我可能已经想通了并想分享的事情。今天的帖子是关于一件事，我很确定我还没有弄明白，想分享一下。我想谈谈我们在过去几周一直在努力解决的一个问题；我们可以提出潜在的解决方法，但还不知道其根本原因。简而言之，如果你在GKE<a class="ae jd" href="https://cloud.google.com/kubernetes-engine/" rel="noopener ugc nofollow" target="_blank">的GCE入口后运行某些类型的服务，即使你的pod没有准备好，比如在部署期间，你也可能会获得流量。在我进入细节之前，这里有一个发现的故事。如果你只是想要TL；博士和建议跳到最后。</a></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/344373e1ac1f63639b0e640ca7b4d783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJD6grf_etTlDEGaoxHTFw.png"/></div></div></figure><p id="5b0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【2019年4月17日更新——谷歌内部测试证实，这是前端保持开放连接到kube-proxy并重用它们的问题。由于netfilter NAT表规则仅适用于新连接，这有效地缩短了kubernetes的内部服务负载平衡，并将所有流量定向到同一pod的给定节点/节点端口。谷歌还证实，从服务器响应中删除keep-alive头是一个变通办法，我们已经在内部证实了这一点。如果您需要keep-alive报头，那么下一个最佳选择是使用一个<a class="ae jd" rel="noopener" href="/google-cloud/vpc-native-clusters-on-google-kubernetes-engine-b7c022c07510"> VPC本地集群</a>进行容器本地负载平衡，因为这将节点端口跳从等式中去掉。不幸的是，如果你的集群不是VPC本地的，那就意味着要建立一个新的集群。这就是解决办法…如果你对这个故事仍然感兴趣，请继续读下去！]</p><p id="698d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在过去的几个月里，我们一直在为迁移到GKE做准备，这是我们最重要的服务之一。该服务部分由一个http守护进程组成，该守护进程处理来自我们的javascript客户端的<a class="ae jd" href="https://en.wikipedia.org/wiki/Push_technology#Long_polling" rel="noopener ugc nofollow" target="_blank">长轮询</a>请求，并在90个GCE实例上运行。这些实例在峰值负载时每秒处理大约15k个请求。因为这些请求中有许多是超时30秒的长轮询，所以我们需要能够优雅地关闭该服务的实例。为了实现这一点，我们可以发送一个命令，使服务退出循环，等待60秒，等待所有现有的长轮询完成，然后退出。</p><p id="3595" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了在kubernetes中复制这一点，我们利用了三个重要而有用的特性:<a class="ae jd" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/" rel="noopener ugc nofollow" target="_blank">就绪探测器</a>、<a class="ae jd" href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/" rel="noopener ugc nofollow" target="_blank">生命周期挂钩</a>，以及用于pod的<a class="ae jd" href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods" rel="noopener ugc nofollow" target="_blank">终止宽限期</a>。就绪探测器允许您定义一个命令，kubelet调用这个命令来确定pod是否应该接收流量，在我们的例子中是一个http端点。生命周期钩子指定了一个在kubelet向它发送SIGTERM on delete之前要在容器中执行的命令。这是我们发送关闭命令的方式。终止宽限期设置了kubelet在发送SIGTERM之前等待生命周期挂钩完成的最长时间。</p><p id="862c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有这一切似乎在舞台上表现得相当好，但唉，我们都知道那里的交易。在我们在GCE类入口后面启动一些生产pod并开始向它们迁移流量后，这项服务的首席工程师Nick MacInnis开始发现一些奇怪的事情:在部署期间，未准备好的pod仍在接收流量。这是一件坏事，因为这意味着当pod最终被删除时，我们将有未完成的连接，我们的客户将会看到一连串断开的连接错误。测试证实了他的观察。这些豆荚不仅获得了流量，还获得了新的连接。事实上，我们可以简单地将负载平衡器端点卷成一个循环，并将流量发送到未准备好的pods。</p><p id="817f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们问自己的第一个问题是:豆荚真的没有准备好吗？当一个pod的就绪性探测失败时(通过向http探测返回200以外的值，从命令探测返回1，或者如果没有定义探测则退出),它应该被主节点标记为未就绪，它的端点应该被<a class="ae jd" href="https://kubernetes.io/docs/concepts/overview/components/#kube-controller-manager" rel="noopener ugc nofollow" target="_blank">端点控制器</a>标记为未就绪，端点应该从服务的端点列表中删除，每个节点上的kube-proxy应该从向pod分发流量的NAT表链中删除该端点。</p><p id="e3c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的测试证实，所有这些事情正如所描述的那样发生了:pod被标记为未准备好，端点控制器将端点标记为未准备好，服务将其删除。因为从入口(即通过<a class="ae jd" href="https://kubernetes.io/docs/concepts/services-networking/service/#nodeport" rel="noopener ugc nofollow" target="_blank">节点端口</a>)到服务的流量被<a class="ae jd" href="https://www.juniper.net/documentation/en_US/contrail4.0/topics/task/configuration/snat-vnc.html" rel="noopener ugc nofollow" target="_blank">SNAT</a>到接收节点上的pod网络，所以很容易看到流量来自哪个节点。当我们识别并连接到一个源节点时，我们可以看到kube-proxy一获得更新就从NAT表中删除了pod的端点，这通常是在pod本身被标记为未准备就绪后的5-10秒。</p><p id="4c27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，它获得了流量。我们在谷歌云支持下打开了一个案例，并继续测试。我应该在这里指出，我们一直有良好的体验与谷歌的支持。他们关心服务，即使是复杂的问题，他们也能迅速解决。在过去的几年里，他们帮助我们解决了很多问题，但他们和我们一样对此感到困惑。在内部，我们致力于在更加可控的环境中重现问题。在我们看来，这个http服务器或入口没有什么特别的，所以如果这是一件事，它应该是任何类似配置的GKE服务的事情。</p><p id="2aa3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们首先使用python3和flask创建了一个简单的服务器，有三个端点:</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="c296" class="jw jx hi js b fi jy jz l ka kb">from flask import Flask<br/>from flask import Response</span><span id="1dc6" class="jw jx hi js b fi kc jz l ka kb">app = Flask("ready-test")</span><span id="2651" class="jw jx hi js b fi kc jz l ka kb">READY = True</span><span id="13ef" class="jw jx hi js b fi kc jz l ka kb"><a class="ae jd" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route('/', methods=['GET'])<br/>def handle_root():<br/>  resp = Response("hello to you sir or madam")<br/>  return resp</span><span id="4c30" class="jw jx hi js b fi kc jz l ka kb"><a class="ae jd" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route('/ready/', methods=['GET'])<br/>def handle_ready():<br/>  global READY<br/>  if READY:<br/>    return "ready"<br/>  else:<br/>    return Response("unready", status=503)</span><span id="e987" class="jw jx hi js b fi kc jz l ka kb"><a class="ae jd" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route('/unready/', methods=['GET'])<br/>def handle_unready():<br/>  global READY<br/>  if READY:<br/>    READY = False<br/>    return "server is unready"<br/>  else:<br/>    READY = True<br/>    return "server is ready"</span></pre><p id="4de5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根路径由测试客户端调用，只返回一些垃圾和http 200。/ready/ path旨在从就绪探测器中调用，并根据全局变量的状态返回200或503。/unready/ path只是切换该变量的状态，是我们控制pod就绪状态的手段。</p><p id="ba4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们添加了部署、服务和入口:</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="810b" class="jw jx hi js b fi jy jz l ka kb">kind: Deployment<br/>apiVersion: apps/v1<br/>metadata:<br/>  name: ready-test-http<br/>  labels:<br/>    app: ready-test<br/>spec:<br/>  selector:<br/>    matchLabels:<br/>      deployment: ready-test-http<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: ready-test<br/>        deployment: ready-test-http<br/>    spec:<br/>      containers:<br/>      - name: ready-test<br/>        image: ready-test<br/>        ports:<br/>        - containerPort: 8080<br/>          name: http<br/>        readinessProbe:<br/>          httpGet:<br/>            path: /ready/<br/>            port: 8080<br/>          periodSeconds: 10<br/>          timeoutSeconds: 5<br/>          successThreshold: 1<br/>          failureThreshold: 2<br/>---</span><span id="bcae" class="jw jx hi js b fi kc jz l ka kb">kind: Service<br/>apiVersion: v1<br/>metadata:<br/>  name: ready-test<br/>  labels:<br/>    app: ready-test<br/>spec:<br/>  type: NodePort<br/>  selector:<br/>    deployment: ready-test-http<br/>  ports:<br/>    - name: http<br/>      port: 80<br/>      targetPort: http<br/>---<br/>apiVersion: extensions/v1beta1<br/>kind: Ingress<br/>metadata:<br/>  name: ready-test<br/>  labels:<br/>    app: ready-test<br/>  annotations:<br/>    kubernetes.io/ingress.class: "gce"<br/>spec:<br/>  rules:<br/>  - http:<br/>      paths:<br/>      - path: /<br/>        backend:<br/>          serviceName: ready-test<br/>          servicePort: 80<br/>      - path: /ready/<br/>        backend:<br/>          serviceName: ready-test<br/>          servicePort: 80<br/>      - path: /unready/<br/>        backend:<br/>          serviceName: ready-test<br/>          servicePort: 80</span></pre><p id="261f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">准备就绪探测的具体时间被选择来复制我们在产品服务中所做的事情，其余的几乎是简单的样板文件。我们进行了部署，确认我们可以切换就绪状态，启动一个循环来弯曲根路径，并遵循pod日志。然后，我们切换就绪状态，看到pod被标记为未就绪，大约5秒钟后，看到节点上的NAT表得到更新…然后负载平衡器的流量停止。我们唯一能看到的是来自库伯莱和GCLB前端的战备探测。</p><p id="e6d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们回去仔细查看了生产服务。有些事情明显不同了。在四处探索之后，我们意识到我们正在使用的<a class="ae jd" href="https://twistedmatrix.com/trac/" rel="noopener ugc nofollow" target="_blank"> twisted http server </a>默认设置了“Connection: keep-alive”头，而我们的测试应用程序没有。为此，我们添加了代码，将根路径处理程序的实现改为:</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="7eda" class="jw jx hi js b fi jy jz l ka kb"><a class="ae jd" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route('/', methods=['GET'])<br/>def handle_root():<br/>  resp = Response("hello to you sir or madam")<br/>  resp.headers['Connection'] = 'Keep-Alive'<br/>  resp.headers['Keep-Alive'] = 'timeout=30'<br/>  return resp</span></pre><p id="6d57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“Keep-alive: timeout=30”头是我在测试中添加的。我不确定它是否相关，但我会把它留在那里，因为它在我们进行的下一次测试中出现过。我们重新部署了测试服务，重复了上面的步骤……并且在pod没有准备好之后继续看到请求。让我们仔细看看这个测试。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kd"><img src="../Images/54f8819a88f3cc7e99b622ac41013148.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I8gmxPnIOgucfynVZN0GSQ.png"/></div></div></figure><p id="4e88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在此屏幕中，您可以看到测试箱已准备就绪，我们只接收来自kube let(10 . 0 . 30 . 1上)和GCLB(来自10.130.0.0/20网络上的节点IP的请求)的请求。然后，我们使用以下命令启动curl:</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="6c68" class="jw jx hi js b fi jy jz l ka kb">$ while true; do curl -m 1 <a class="ae jd" href="http://35.244.243.109/" rel="noopener ugc nofollow" target="_blank">http://xxx.xxx.xxx.xxx/</a>; done</span></pre><p id="7f47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“-m 1”参数的目的是让curl不会等待30秒的GCLB超时，而是继续尝试每秒至少发送一次请求。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ke"><img src="../Images/a907dcf5294f8908f62d6c94c34f0992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-GucyEwXNIl6AElGHDirUg.png"/></div></div></figure><p id="ca52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个屏幕中，pod仍然处于就绪状态，您可以看到curl客户端的请求。请注意，由于前面提到的陷阱，这些似乎也来自节点。然后，我们将pod状态切换为未准备好。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kd"><img src="../Images/dc2f51db2b4608445592eb85aa7debc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GH0uj5T2FdtkBQkD2Z0icA.png"/></div></div></figure><p id="3cf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，您可以看到pod未准备好，我们将503返回到准备就绪探测器，但是我们仍然从curl客户端收到对/ endpoint的请求。请注意，尽管我们这里只有一个pod，而且这个pod还没有准备好，但是当这些请求通过时，客户端不会看到错误。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kf"><img src="../Images/40a494b99423e8445b66216c91a1a77b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V1kdemkNS7kT-kFBrkgvEQ.png"/></div></div></figure><p id="c438" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在curl命令的这个输出中，您可以看到，在大多数情况下，它在请求的1000毫秒内超时，但有时它会得到响应。我们让这个测试运行了几分钟，继续看到零星的请求。测试客户端在最坏的情况下运行大约1 rps，因此您可以很容易地想象如果您扩展到15k rps会有什么影响。</p><p id="5e05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们又做了几轮测试，并满意地确认，当我们在服务器响应中返回keep-alive报头时，一些请求将会通过，当我们删除该报头时，事情会按预期工作，并且只要所有节点的NAT表都更新，流量就会停止。</p><p id="78cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么tl是什么；博士在吗？有时未准备好的pod会收到请求。我们仍在支持下解决此问题，GKE团队正在调查。当我们了解到根本原因时，我会在这里跟进。我们有自己的关于连接在链中的某个地方被打开和重用的理论(主要是因为NAT表规则只适用于新连接)，但实际上这只是推测，我们不知道。通过入口的负载平衡是复杂的，有两个独立级别的健康检查和循环连接平衡。如果这是kube-proxy的问题，我们应该可以看到。到目前为止，情况似乎不是这样的，我们在GCLBs上没有相同水平的可见性。我们将不得不把这留给谷歌团队。</p><p id="af1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与此同时，如果您在GCE类入口<em class="jq">和</em>后面的GKE上运行http服务，您的服务可能会在保持活动的同时变得未准备就绪(即可以通过活性探测，但未通过准备就绪探测)<em class="jq">和</em>您的服务器正在返回keep-alive标头，您可能需要仔细看看部署期间发生了什么。我们看不到任何迹象表明这与我们的应用程序或环境的细节有关，如果在其他地方也没有发生这种情况，那将是令人惊讶的。我们也肯定<em class="jq">不</em>知道移除keep-alive头是一个完整的修复。</p><p id="edb8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢Olark的Nick MacInnis和Kyle Owens对这个问题的深入研究，感谢Google Cloud Support的Yahir迄今为止的所有帮助。</p></div></div>    
</body>
</html>