# TensorFlow 词汇表/备忘单

> 原文：<https://medium.com/google-cloud/a-tensorflow-glossary-cheat-sheet-382583b22932?source=collection_archive---------0----------------------->

已经有很多很棒的 TensorFlow 教程、Jupyter 笔记本和 MOOCs 了。作为一个今年经历了很多的人，一个绊脚石是许多教程会使用我不熟悉的术语。此外，许多教程使用不同级别的 TensorFlow API，或者不同的高级包装器，这增加了我的困惑。我想我应该写一个我经常遇到的术语表来帮忙。

我不是 TF 团队的一员，我还是 TF 和 ML 的学生，所以下面的一些可能不准确，请在评论中添加更正。

我还把一般的 ML 概念放在了最底层，假设许多试图使用 TF 的人已经对 ML 有了一些基本的熟悉。如果没有，Coursera 课程可能是最好的高级入门课程。最后，这只是一个快速总结，这些主题中的大多数在维基百科、研究论文、在线课程等上都有更完整的解释。

在我开始之前，我将重复一些 TF/ML 教程的链接:

*   官方 TF 入门和教程当然在[tensorflow.org](https://www.tensorflow.org)上
*   [Awesome Tensorflow](https://github.com/jtoy/awesome-tensorflow) 是一个庞大的 TF 教程汇编
*   T4 的 Keras 例子是流行架构实现的很好的例子
*   [Coursera](http://coursera.com) 有一个世界闻名的 ML classs，现在又增加了一个深度学习类。斯坦福大学更深入的数学课程也在 ITunes U 上。
*   [Udacity](http://Udacity) 有基于 TensorFlow 的深度学习类和自动驾驶汽车类
*   [Google Cloud ML sample repo](https://github.com/googlecloudplatform/cloudml-samples)有几个教程，包括用不同级别的 API 解决相同问题的教程，这是一个有用的比较点。

好了，在这个列表中，一些定义使用了后来定义的词汇:

*   **首字母缩写** : TF = TensorFlow。ML =机器学习

# TensorFlow API 级别

*   **底层 API** :制作 TF Ops 的 TF 图的 TF 代码。正如 [Mandelbrot 教程](https://www.tensorflow.org/tutorials/mandelbrot)所证明的，你可以将这个 API 用于 ML 之外的目的。如果你对解决一个 ML 问题比创造新的 ML 方法更感兴趣，更高级别的 API 是一个更好的起点，尽管理解这个级别也是非常有帮助的。
*   **Estimator API-** 一个更高级别的 API，也是自 TF Github repo 以来最受官方支持的 API。它提供了常见的 ML 模型的“估计量”。它大致基于[scikitlearn](http://scikit-learn.org/)(sk learn)Python ML API。**非常令人困惑的一点:在某种程度上，这被称为“tf learn”API，因为代码在 TF repo 的** `**tf.contrib.learn**` 包中，但它与下面提到的 TF learn 项目没有关系。您也可以使用低级 API 来实现您自己的估算器。用低级 API 自己实现 LinearClassifier 是一个非常好的学习练习。
*   **实验 API-** 官方 TensorFlow API，设计用于运行长时间运行的 ML 实验。如果训练一个模型需要几个小时或几天的时间，你需要一些标准的方法来跟踪度量，比如准确性、损失、保存模型检查点的方法等等。如果你正在使用 Google Cloud ML 引擎运行 TensorFlow，这是最好的支持方法，我强烈推荐阅读 Lak 使用它的这个[教程，以及查看](/google-cloud/how-to-do-time-series-prediction-using-rnns-and-tensorflow-and-cloud-ml-engine-2ad2eeb189e8) [Cloud ML samples repo](https://github.com/googlecloudplatform/cloudml-samples) 。最后，因为这个包在 contrib 中，所以它是一个不稳定的 API，可能会有很大的变化。
*   **TF learn**—tflearn.org[的又一高水平](http://tflearn.org)独立于 TF 团队的 TF API。**与 Estimator API 无关，官方是 TF 的一部分，或者是 Google 出品。我不太了解这个 API**
*   另一个高级 API，可能是最流行的。早于 TF，但支持 TF 作为一种可能的后端。也松散地模仿 sklearn。自从谷歌聘用了 TensorFlow 的创建者以来，这已经成为 tensor flow 的一个更加官方的入口。
*   **层**-Core tensor flow 和 Keras 都提供层，它们是构建神经网络层的便利函数，如卷积层。这些通常介于高级 API 和低级 API 之间，因为您直接将操作添加到图中，但是使用通用模式。
*   **TF slim**——又一个像 Keras 一样的高级 API，它在官方回购中，但不是很受欢迎
*   **Theano/PyTorch/Caffe —** 与 TF 竞争的流行深度学习框架。

# 核心张量流概念

*   **张量** —多维矩阵，底层 API 的基础部分。对数学家来说，这可能更微妙，但在 TF 中，它总是一个多维矩阵。
*   **Graph/Ops/Session /Node—** 通常，使用 TF 低级 API，看起来像是在编写 Python，但实际上，您只是在构建稍后将运行的 Ops(操作)图。进程是运行图表和维护状态的方式。节点是由 Ops 创建的图形的一部分。TensorFlow 的一个主要混淆点是，您编写的代码通常是在运行图形后运行，而不是在创建操作后运行。
*   **稠密张量** —其值通常在连续范围内的张量，意味着单个实值特征就足够了
*   **稀疏张量—** 实际上是一组张量，可以更容易地表示展开值或分类值
*   **要素列**-通常给定一组输入数据，您会希望在将输入要素输入网络之前对其进行组合或修改。通常，您需要将分类特征或字符串特征映射到某种数值(实值或多维张量)。这第一层称为特性列。
*   **特征交叉—** 两个特征的组合，这在两个特征之间的关系很重要时很有用。参见[大比例线性模型](https://www.tensorflow.org/tutorials/linear)教程。
*   **队列—** Tensorflow 有一个[队列的概念。这对于读入输入特征很有用，因为输入太大而无法读入内存，所以取而代之的是一个工人将输入加载到一个队列中，另一个操作将它从队列中取出进行处理。](https://www.tensorflow.org/programmers_guide/threading_and_queues)

# 核心分类特征概念

通常在 TF 中处理实值输入函数比分类特征简单得多。我们通常也将文本等自由形式的输入视为分类特征(例如，英语输入是分类的，可能的值是英语)。

一种尝试是将单词映射成数值，也许是用散列函数，或者只是从 0 到 N 排序，其中 N 是特征的数量。但是有一个问题。给定 3 个类别，红色、绿色和蓝色，如果我们将红色映射到 0，将绿色映射到 1，将蓝色映射到 2，我们会使绿色和蓝色在价值上比红色和蓝色“更接近”网络，即使我们不是故意的。有各种各样的工具来处理这个问题。

*   **词汇—** 分类特征的可能值的集合。它用于训练嵌入或为分类特征创建哈希桶。
*   **散列桶—** 映射分类特征的最简单方法是将它们的值散列，可能是散列到一组桶中，桶的数量是确定的或者是超参数。
*   **一个热门的编码—** 给定一个分类输入特征，如果我们简单地将可能的值映射到数字(比如红色—>1，蓝色—>2，绿色—>3)我们会将红色和蓝色的含义放得更近(2–1 = 1)，即使它们在含义上并不比红色和绿色更近(3–1 = 2)。相反，一个热编码特征列将为红色、蓝色和绿色创建 3 个单独的节点，并且除了实际值为 0 和实际值为 1 之外，每个节点都有。这样一来，红色就不会比绿色更接近蓝色。
*   **Softmax —** 类似于 sigmoid 函数，但对于多个输入值，它将一组数值映射到一组总和为 1 的概率。它应该表示输入实例是给定类别的概率。一种非常常见的模式是，网络的最后一层具有与可能的类的数量相同的节点数，然后对 logits 运行 softmax 函数来生成每个类的概率。
*   映射到概率的非常大或非常小的数字。从技术上讲，logit 是 logistic (softmax)函数的反函数，但在实践中，logit 是指通过对概率运行 logit 函数而获得的值。
*   **单词嵌入-** 从技术上讲，从单词中生成的任何种类的数字，但是在 TF 上下文中，单词嵌入通常指的是从分类特征中创建向量并训练这些向量。参见这里的 [word2vec](https://www.tensorflow.org/tutorials/word2vec) 教程。它提供了比一次性编码更多的语义，因为相似的输入要素将具有更多相似的值。您使用固定大小的向量(深度),而不是类别的每个可能值的节点。向量是由网络训练的，在 word2vec 示例中，它是基于文本中的接近度来使用的，因此相似的向量值实际上在意义上更接近。这种方法的一个有趣的副作用是，您可以对嵌入进行矢量数学运算(例如，king-queen = man)。

# 流行建筑

*   **线性分类器—** 采用输入特征并将其与权重和偏差相结合以预测输出值的简单架构。一个内置的 Esimators。
*   **DNNClassifier —** 深度神经网络分类器。涉及代表“隐藏特征”的节点中间层和代表非线性的激活函数。一个内置的估计器。
*   **广度和深度—** 一个由[谷歌论文](https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html)推广的架构，结合了线性分类器和深度神经网络分类器。直觉是“宽”的线性部分代表记忆具体的例子，而“深”的部分代表理解高层次的特征。例如，英语语法的许多部分都有基于词类的规则(由深部分学习)，但许多常见的例子打破了这些规则(由宽部分学习)。内置估值器之一。[见本教程。](https://www.tensorflow.org/versions/master/tutorials/wide_and_deep)
*   **卷积神经网络** —卷积神经网络。一种流行的影像分类架构，使用横跨输入影像的网格来生成隐藏图层。例子包括 LeNet、Nvidia、ImageNet
*   **转移学习—** 使用现有训练模型作为起点，并为特定用例添加附加层的模型。直觉是经过高度训练的现有模型知道许多一般特征，这些特征是在特定示例上训练小型网络的良好起点。 [TensorFlow for Poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/) 教程是一个很好的图像识别例子。
*   **RNN** —递归神经网络，一种为处理具有序列“记忆”的输入序列而设计的架构。LSTMs 是 RNNs 的一个奇特版本。非常受自然语言处理(NLP)用例的欢迎
*   **甘** —一般对抗性神经网络，一个模型创建伪样本，另一个模型同时接受伪样本和真实样本，并被要求区分。使用 ML 生成新数据的流行方法。

# 部署注意事项

*   **GPU/TPU —** 图形处理器单元/张量流处理单元。GPU 是为矩阵数学优化的游戏卡制造的芯片。它们对深度学习项目或多或少是必不可少的，因为它们大大加快了训练速度。看这个 [HN 关于购买自己的](https://news.ycombinator.com/item?id=14396075)的讨论，或者在 AWS 或谷歌云上租赁。谷歌宣布 TPU 是更专门用于深度学习的芯片，最终将在谷歌云上可用。
*   **监视器/会话脱钩—** 通常在训练您的模型时，您会希望关注您的指标，如损失和准确性。通过附加 Monitor，您可以在许多高级 API 中实现这一点。SessionRunHooks 通过避免长时间运行的线程，取代了问题较少的监视器。
*   **tensor board**—tensor flow 附带的 GUI，用于显示训练期间的许多相关指标
*   **Apache Spark/Apache Beam/Apache Hadoop**—常用于预处理特性的流行大数据框架
*   **Apache Airflow** —一个 Airbnb 项目，让编排所有的监控/记录/预处理/模型评估变得更加容易。
*   **模型陈旧性—** 由于输入数据已经过时，实时模型的性能可能会随着时间的推移而发生漂移，因此在部署过程中，通常会使用新模型替换现有模型，或者使用新数据训练相同的模型
*   **Google Cloud ML Engine —** 托管 Tensorflow 环境，在分布式集群上提供训练和预测

# **一般 ML 概念**

*   **特性**—ML 模型使用的输入数据
*   **特征工程—** 转换输入特征，使其对模型更有用。通常包括将类别映射到桶、将值正则化到-1 和 1 之间、删除空值等。与特征工程相关的是理解你的输入数据，验证训练数据和生产数据足够相似，等等。
*   **Condas/Numpy/Scipy/Jupyter notebooks/Numpy/pandas—**对特征工程有用的 Python 工具和库。Jupyter 笔记本作为一种与他人分享实验的方式特别有用。
*   **训练/评估/测试** —训练是用于优化模型的数据，评估是用于在用新数据进行训练期间根据新数据评估模型，测试是用于提供最终结果
*   **分类/回归—** 回归是预测一个数字(如房价)，分类是从一组类别或输出类进行预测(如从红/蓝/绿预测房子的颜色)。
*   **线性回归-** 通过将输入特征与权重和偏差相乘并求和来预测输出的经典方法。对回归有用
*   **逻辑回归—** 类似于线性回归，但预测概率，用于分类。
*   **神经网络—** 类似于线性/逻辑回归，但增加了一个激活函数，使得预测不是输入的线性组合的输出成为可能。经常使用中间层节点进行“深度学习”。
*   **梯度下降/随机梯度下降(SGD)/反向传播** —基本损耗优化器算法，其他优化器通常基于该算法。SGD 是梯度下降法，但它是对一批训练数据而不是全部数据进行的。反向传播类似于梯度下降，但用于神经网络
*   **Loss/LogLoss** —表示模型有多好的度量，以及优化器使用的度量。损失日志(logloss)通常被 Kaggle(数据科学竞赛网站)用来对模型进行排名。
*   **优化器—** 改变权重和偏差以减少损失的操作。通常是阿达格拉德或亚当。
*   **权重/偏差**-权重是与输入要素相乘以预测输出值的值。偏差是给定权重为 0 的输出值。
*   **收敛—** 收敛的算法最终会达到最优解，即使非常慢。一个不收敛的算法可能永远达不到最优解。
*   **学习率**——优化者改变权重和偏差的速度。一般来说，高学习率训练得更快，但有不收敛的风险，而低学习率训练得更慢
*   **过度拟合—** 当模型对输入数据表现出色，但对评估或测试数据表现不佳时
*   **偏差/方差**—输出的多少由特性决定。更多的差异通常意味着过度拟合，更多的偏差可能意味着一个坏的模型
*   **正则化** —减少过拟合的各种方法，包括将权重添加到损失函数中，随机丢弃层(丢弃)。
*   **学习曲线—** 打印出训练/评估指标随时间变化的图表，以评估模型质量。可以用 TensorBoard 完成
*   **时期** —您对训练数据运行了多少次优化
*   **批量大小—** 一次优化多少个训练样本
*   **超参数(HParams)——**ML 模型采用许多参数，您必须猜测其中最适合的参数。超参数是您也将训练的参数，例如，如果您不知道最佳学习率，您可以将其设为超参数，网络将根据损失尝试找到最佳的参数。[参见本指南](https://cloud.google.com/ml-engine/docs/concepts/hyperparameter-tuning-overview)关于超参数调谐。
*   **激活函数—** 将非线性引入网络的数学函数。最受欢迎的是 RELU，其次是谭。
*   **Sigmoid 函数-** 将非常负数映射到非常接近 0 的数、接近 1 的巨数、0 映射到. 5 的函数。用于在逻辑回归中将数字映射到概率。
*   **准确度/精确度/召回率—** 准确度通常不能很好地代表性能，例如，如果 95%的时间都在下雨，那么预测每天都会下雨的模型就有 95%的准确度不是很好。参见维基百科**。**
*   **混淆矩阵—** 一种表示准确度/精确度/召回率的方式
*   **AUC(曲线下面积)、ROC(接收操作特性)** —另一种可视化准确度/精确度/权衡指标的方法
*   **MSE(均方误差)——**最常见的回归损失函数
*   **交叉熵**——最常见的分类损失函数
*   **集成学习** —用不同的参数训练多个模型来解决同一个问题
*   **数值不稳定—** 由于计算机中浮点数表示的限制，许多深度学习算法可以运行具有非常大或非常小的值的问题
*   **梯度爆炸**——数值不稳定的常见情况

同样，请在评论中留下任何更正、补充或反馈。在推特上找我 [@waprin_io](http://twitter.com/waprin_io) 。感谢 [Eli Bixby](https://github.com/elibixby) 查看并提供一些修复。