<html>
<head>
<title>Processing databricks Delta Lake data in Google Cloud Dataproc Serverless for Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Google Cloud Dataproc中处理databricks Delta Lake数据</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/processing-databricks-delta-lake-data-in-google-cloud-dataproc-serverless-for-spark-1cc1405a3ee4?source=collection_archive---------1-----------------------#2022-04-12">https://medium.com/google-cloud/processing-databricks-delta-lake-data-in-google-cloud-dataproc-serverless-for-spark-1cc1405a3ee4?source=collection_archive---------1-----------------------#2022-04-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8b93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">今年早些时候，谷歌宣布了面向Spark 的<a class="ae jd" href="https://cloud.google.com/dataproc-serverless/docs" rel="noopener ugc nofollow" target="_blank"> Dataproc无服务器版(Dataproc s8s)的正式发布，它允许您在Dataproc上运行Spark作业，而不必启动和管理自己的Spark集群。</a></p><p id="873a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最近，我在一个项目中与一位客户合作，该客户希望将他们目前使用databricks执行的一些工作转移到Dataproc s8s上，目的是减少和简化他们的操作。另一方面，该客户希望维护其基于databricks Delta Lake的当前数据湖。</p><blockquote class="je jf jg"><p id="88de" class="if ig jh ih b ii ij ik il im in io ip ji ir is it jj iv iw ix jk iz ja jb jc hb bi translated">Delta Lake是由databricks开发的开放式存储层，基于Apache Parquet，为Apache Spark和其他大数据引擎带来可扩展的ACID事务(参考:<a class="ae jd" href="https://delta.io/" rel="noopener ugc nofollow" target="_blank"> delta.io </a>，<a class="ae jd" href="https://github.com/delta-io/delta" rel="noopener ugc nofollow" target="_blank"> github </a>)。</p></blockquote><p id="67fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管工作迁移过程非常简单，但有几个问题需要特别注意:</p><ul class=""><li id="4835" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">连通性</li><li id="6e21" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">运行时版本</li><li id="a318" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">其他库和依赖项</li></ul><h1 id="440a" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">连通性</strong></h1><p id="06dc" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">Spark驱动程序和执行器实例将拥有内部IP地址。因此，如果您计划从Spark代码访问任何Google API，您需要确保您正在使用的子网启用了私有Google Access。此外，如果您需要访问互联网(即下载一个包或访问另一个云存储桶)确保您已经在子网中部署了云NAT或您有可用的互联网代理。</p><p id="00da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，确保子网具有防火墙规则，允许所有子网入口使用所有端口上的所有协议进行通信。</p><p id="7d0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更多信息<a class="ae jd" href="https://cloud.google.com/dataproc-serverless/docs/concepts/network" rel="noopener ugc nofollow" target="_blank">点击这里</a>。</p><h1 id="1a0a" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">运行时版本</strong></h1><p id="3e7f" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">Dataproc s8s目前支持Spark 3.2.1、Python 3.9、Java 11和Scala 2.12。如果您的代码使用以前的版本，您将需要检查它在所需的运行时版本下是否运行正常。点击查看全部详情<a class="ae jd" href="https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-versions" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="badf" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">附加库和依赖关系</strong></h1><p id="f086" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">默认情况下，Dataproc s8s使用一个容器映像，其中包含与运行时发布版本相关联的默认Spark、Java、Python和R包。为了达到<a class="ae jd" href="https://cloud.google.com/dataproc-serverless/docs/overview#for_spark_compared_to_on" rel="noopener ugc nofollow" target="_blank"> 60s的启动时间</a>，为Dataproc s8s提供的运行时在支持的Java库和python包方面进行了非常优化(详见这里的<a class="ae jd" href="https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-versions" rel="noopener ugc nofollow" target="_blank"/>)。</p><p id="5391" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，有几种机制可以包含额外的Java库或python包:</p><h2 id="bd7a" class="lc ka hi bd kb ld le lf kf lg lh li kj iq lj lk kn iu ll lm kr iy ln lo kv lp bi translated"><strong class="ak"> a)启动过程中的供应依赖关系</strong></h2><p id="65f8" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">Spark batches API的Dataproc s8s支持几个参数来指定额外的JAR文件和归档。</p><p id="658c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于pyspark，您可以在gCloud CLI中使用以下参数来指定依赖项:</p><p id="cfef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lq lr ls lt b">--jars</code> =[ 【T1，…]要提供给类路径的以逗号分隔的jar文件列表。</p><p id="89f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lq lr ls lt b">--py-files</code>=[【T3]，…]要传递给PySpark框架的Python脚本的逗号分隔列表。支持的文件类型:<code class="du lq lr ls lt b">.py</code>、<code class="du lq lr ls lt b">.egg</code>和<code class="du lq lr ls lt b">.zip.</code></p><p id="0d2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lq lr ls lt b">--archives</code>=[【T8]，…]要提取到工作目录中的档案。支持的文件类型:。罐子，。tar，. tar.gz，.tgz和. zip。</p><p id="bd7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，您可以使用spark运行时属性<em class="jh"> spark.jars.packages </em>来包含一个逗号分隔的jar的Maven坐标列表，以包含在驱动程序和执行器类路径中。</p><p id="54ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要安装所需的delta lake库，您需要在使用gCloud CLI运行作业时指定以下属性:</p><pre class="lu lv lw lx fd ly lt lz ma aw mb bi"><span id="5053" class="lc ka hi lt b fi mc md l me mf">--properties=”spark.jars.packages=io.delta:delta-core_2.12:1.1.0"</span></pre><p id="3d6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不需要额外的python包，因为所需的Python模块已经打包在<em class="jh">io . delta:delta-core</em>jar文件中。</p><h2 id="0e57" class="lc ka hi bd kb ld le lf kf lg lh li kj iq lj lk kn iu ll lm kr iy ln lo kv lp bi translated">b)在自定义容器中包含依赖关系</h2><p id="d55c" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">Spark batches API的Dataproc s8s允许您使用定制的容器图像来代替默认图像。通常，自定义容器映像会添加默认容器映像不提供的Spark workload Java或Python依赖项。</p><p id="1e68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Internet访问受限的环境，或者当所需的附加软件包数量较高时，这将是推荐的方法。</p><p id="61b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">点击查看全部详情<a class="ae jd" href="https://cloud.google.com/dataproc-serverless/docs/guides/custom-containers" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="16cd" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">End2end示例—备选项1:在引导过程中提供依赖关系</h1><p id="4ded" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">一旦我们回顾了上下文并介绍了一些理论，我们就开始动手做例子吧！</p><p id="f3b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将首先回顾如何在引导过程中运行此示例资源调配依赖关系。在下一节中，我们将使用自定义容器执行相同的操作。</p><h2 id="42c8" class="lc ka hi bd kb ld le lf kf lg lh li kj iq lj lk kn iu ll lm kr iy ln lo kv lp bi translated">1-Google Cloud项目和资源</h2><p id="2960" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">为了运行这个示例，我们需要:</p><ul class=""><li id="6eb7" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">拥有所有资源的GCP项目</li><li id="df40" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">由Dataproc s8s使用的VPC网络和子网</li><li id="22e4" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">允许Dataproc所需的内部子网通信的防火墙规则</li><li id="b31c" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">云NAT，需要从Dataproc计算资源到达互联网</li><li id="5919" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">具有Dataproc所需的最低权限的服务帐户</li><li id="1c06" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">一个GCS存储桶，用于存放Delta Lake文件以及Dataproc所需的其他分段对象。</li></ul><p id="08d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有这些设置都可以使用terraform自动部署。在这篇文章的末尾，你可以找到不同的terraform文件以及后续的过程。</p><h2 id="4ae3" class="lc ka hi bd kb ld le lf kf lg lh li kj iq lj lk kn iu ll lm kr iy ln lo kv lp bi translated">2-Pyspark工作</h2><p id="a9bc" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">一旦资源可用，我们就可以在Dataproc s8s中运行以下pyspark作业:</p><figure class="lu lv lw lx fd mg"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="7527" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">python代码基本上是用一些数据创建一个虚拟文件，使用增量格式将其保存在GCS存储桶中，然后读取它。</p><h2 id="baa9" class="lc ka hi bd kb ld le lf kf lg lh li kj iq lj lk kn iu ll lm kr iy ln lo kv lp bi translated">3–启动工作</h2><p id="266c" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">最后，使用gCloud CLI启动该作业:</p><figure class="lu lv lw lx fd mg"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">使用默认容器运行作业</figcaption></figure><p id="18a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作业完成后，您将能够在Google云控制台中访问作业详细信息(data roc-&gt;批处理)。</p><figure class="lu lv lw lx fd mg er es paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="er es mn"><img src="../Images/76a1ccc1f40a532be8273aaed1ebf657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dVaDafB6we6UjvfWJH8AWg.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">GCP控制台中的工作详细信息</figcaption></figure><p id="388c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，通过使用“<em class="jh"> labels=job= </em>”参数，您将能够在GCP计费控制台中查询作业的成本(该数据将在大约24小时后出现。)</p><figure class="lu lv lw lx fd mg er es paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="er es mu"><img src="../Images/c9a0293ed6cf5540e1695c77b3e6250a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PX84yv1LiJi7ImQQPX8o-A.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">账单详情</figcaption></figure><h1 id="f358" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">结束2结束范例—替代2:在自订容器中包含相依性</h1><p id="618a" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">在集群创建/运行时期间安装Python包是不确定的，并且根据包和依赖关系，此过程可能非常缓慢，会影响工作者和执行者的引导时间。</p><p id="cac9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不过，还有其他方法可以达到同样的目的:使用自定义图像或分发Python env包作为归档文件(<a class="ae jd" href="https://spark.apache.org/docs/latest/api/python/user_guide/python_packaging.html" rel="noopener ugc nofollow" target="_blank">参考</a>)。</p><p id="5927" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本例中，我们将探讨创建自定义容器的过程。</p><h2 id="9318" class="lc ka hi bd kb ld le lf kf lg lh li kj iq lj lk kn iu ll lm kr iy ln lo kv lp bi translated">Google Cloud项目和资源</h2><p id="d65e" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">在这个例子中，将利用上面描述的相同Google Cloud项目和资源。</p><h2 id="972c" class="lc ka hi bd kb ld le lf kf lg lh li kj iq lj lk kn iu ll lm kr iy ln lo kv lp bi translated">创建自定义容器</h2><p id="928e" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">使用带有Dataproc s8s的定制容器在官方指南(<a class="ae jd" href="https://cloud.google.com/dataproc-serverless/docs/guides/custom-containers" rel="noopener ugc nofollow" target="_blank">链接</a>中有很好的解释。我们将更新提供的示例，并包括所需的Delta Lake Java库和python包。</p><p id="ef5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可以通过添加以下代码来实现:</p><pre class="lu lv lw lx fd ly lt lz ma aw mb bi"><span id="9a17" class="lc ka hi lt b fi mc md l me mf">COPY delta-core_2.12–1.1.0.jar “${SPARK_EXTRA_JARS_DIR}”</span><span id="824d" class="lc ka hi lt b fi mv md l me mf">(…)</span><span id="527e" class="lc ka hi lt b fi mv md l me mf">RUN ${CONDA_HOME}/bin/pip install ‘delta-spark’</span></pre><p id="dd41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是完整的Dockerfile:</p><figure class="lu lv lw lx fd mg"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">Dockerfile</figcaption></figure><p id="e20c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了构建容器，我们可以使用Google Cloud Build:</p><figure class="lu lv lw lx fd mg"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">构建自定义容器</figcaption></figure><h2 id="00be" class="lc ka hi bd kb ld le lf kf lg lh li kj iq lj lk kn iu ll lm kr iy ln lo kv lp bi translated">启动作业</h2><p id="24b2" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">如前所述，作业使用gCloud CLI启动，但添加了<em class="jh">容器图像</em>参数:</p><figure class="lu lv lw lx fd mg"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">使用自定义容器运行作业</figcaption></figure><p id="0ad5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">仅此而已！我们有同样的例子，但是这次是在一个定制容器中运行，依赖项包含在Dockerfile文件中。</p><p id="ff93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望这些例子能够在Dataproc s8s中部署作业的过程中帮助您。</p><h1 id="020d" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">附件-地形文件和部署</h1><p id="a576" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">您可以在下面找到terraform文件以及执行它们的过程:</p><figure class="lu lv lw lx fd mg"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">main.tf</figcaption></figure><figure class="lu lv lw lx fd mg"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">变量. tf</figcaption></figure><p id="9d0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要运行terraform代码:</p><ul class=""><li id="0e76" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">将<code class="du lq lr ls lt b">main.tf</code>和<code class="du lq lr ls lt b">variables.tf</code>的内容复制到自己选择的文件夹中(内容在本帖末尾)。</li><li id="171c" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">创建一个<code class="du lq lr ls lt b">terraform.tfvars</code>文件并指定所需的变量:</li></ul><pre class="lu lv lw lx fd ly lt lz ma aw mb bi"><span id="5751" class="lc ka hi lt b fi mc md l me mf">admins = [<br/>  "user:admin@xxxxxx.com"<br/>]<br/>billing_account_id = "1234-1234-1234"<br/>root_node          = "folders/12345678"<br/>prefix             = "myprefix"</span></pre><figure class="lu lv lw lx fd mg"><div class="bz dy l di"><div class="mh mi l"/></div></figure><ul class=""><li id="6dc3" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">确保您拥有正确的身份验证设置(应用程序默认凭据或服务帐户密钥)</li><li id="2181" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">运行<code class="du lq lr ls lt b">terraform init</code>和<code class="du lq lr ls lt b">terraform apply</code></li></ul></div></div>    
</body>
</html>