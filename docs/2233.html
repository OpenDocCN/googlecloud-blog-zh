<html>
<head>
<title>Serverless Spark ML pipeline in GCP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GCP的无服务器Spark ML管道</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/serverless-spark-ml-pipeline-in-gcp-4bc28d178ed0?source=collection_archive---------1-----------------------#2022-06-26">https://medium.com/google-cloud/serverless-spark-ml-pipeline-in-gcp-4bc28d178ed0?source=collection_archive---------1-----------------------#2022-06-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5083" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在关于无服务器Spark系列的上一篇文章中，我们描述了如何开发一个示例ETL管道。在本文中，我们将对此进行扩展，以进一步了解如何开发ML管道并以无服务器Spark方式进行编排。</p><p id="8009" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将建立一个简单的回归模型来预测企鹅的体重。此解决方案的数据集可在BigQuery公共数据集中找到，位于，<strong class="ih hj">big query-public-data . ml _ datasets . penguins .</strong>我们将根据企鹅的物种、<strong class="ih hj"> </strong>和性别来预测企鹅的体重。</p><p id="9399" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">样本数据集可以在这里查看，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/42291a2cfbe80c550cefc532933f119f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tQZGJwuInrXj_BJML4OFPQ.png"/></div></div></figure><p id="8191" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">整个管道的解决方案架构如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jp"><img src="../Images/6aa3cc0843b953ca5221b77f1b6d42a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0esZm0qymSD8wH3esbmnxA.png"/></div></div></figure><p id="be6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要开始以交互方式处理无服务器Spark会话，请导航到Cloud Dataproc控制台&gt;创建新会话&gt;打开Jupyter会话</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jq"><img src="../Images/e64de13360d13f00309f2e146fd5d607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lMLo358aK7e65cKLDOJplg.png"/></div></div></figure><p id="7955" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在笔记本&gt;新启动器&gt;无服务器Spark &gt;中，您可以选择创建的会话并开始编写pyspark代码。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jr"><img src="../Images/ce48b72977ac14370d08731f4c810754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AOD5iYt9zM0K3rnyvX2k7Q.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es js"><img src="../Images/639bbab16bb97ca9a04efca352e54a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-gJl9Q_c7WZiBQ9R_d633g.png"/></div></div></figure><p id="cde1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据集</strong>:企鹅数据在<strong class="ih hj">bigquery-public-data . ml _ datasets . penguins中可用。</strong>为了模拟生产管道，我们将假设数据在GCS中，因此我们将把这个数据从big query导出到GCS bucket，作为penguins.csv</p><p id="d684" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> model_building.py : </strong>我们将使用Linearregressor模型建立一个回归模型来预测企鹅的体重。回归模型的代码如下所示，</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="63b4" class="jy jz hi ju b fi ka kb l kc kd">from pyspark.sql import SparkSession<br/>import pyspark<br/>from pyspark.sql.functions import *<br/>from datetime import datetime<br/>from pyspark.sql.functions import monotonically_increasing_id, row_number<br/>from pyspark.sql import Window<br/>import hashlib<br/>from random import randint<br/>from pyspark.ml.feature import VectorAssembler, StringIndexer<br/>from pyspark.sql.functions import regexp_replace<br/>from pyspark.sql import functions as f<br/>from pyspark.sql.functions import isnan, when, count, col<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>from pyspark.sql.types import IntegerType<br/>from pyspark import SparkContext<br/>from pyspark.sql import SQLContext<br/>from pyspark.ml.feature import VectorAssembler<br/>import pandas as pd<br/>from pyspark.sql.functions import isnan, when, count, col<br/>from google.cloud import bigquery<br/>from pyspark.ml import PipelineModel<br/>from pyspark.ml import Pipeline<br/>from pyspark.ml.feature import OneHotEncoder, StringIndexer<br/>from pyspark.ml.feature import RFormula<br/>from pyspark.ml.regression import LinearRegression<br/>from pyspark.ml.evaluation import RegressionEvaluator</span><span id="b004" class="jy jz hi ju b fi ke kb l kc kd">#Building a Spark session to read and write to and from BigQuery<br/>spark = SparkSession.builder.appName('pyspark-penguin-ml').config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar').enableHiveSupport().getOrCreate()</span><span id="b109" class="jy jz hi ju b fi ke kb l kc kd">#Reading the arguments and storing them in variables<br/>project_name=&lt;&lt; replace your project id&gt;&gt;<br/>dataset_name=&lt;&lt; replace your dataset id&gt;&gt;<br/>bucket_name=&lt;&lt; replace your bucket name&gt;&gt;<br/>user_name=&lt;&lt; replace your username&gt;&gt;</span><span id="451f" class="jy jz hi ju b fi ke kb l kc kd">#Reading the penguins source data<br/>p_df = spark.read.option("header",True).csv("gs://"+bucket_name+"/01-datasets/penguins.csv")<br/>p_df.printSchema()</span><span id="e47c" class="jy jz hi ju b fi ke kb l kc kd">p_df.select("species", "island", "culmen_length_mm", "culmen_depth_mm",<br/> "flipper_length_mm", "body_mass_g","sex").show(5)</span><span id="651e" class="jy jz hi ju b fi ke kb l kc kd">## Convert columns of type string to float<br/>p_df=p_df.withColumn('culmen_length_mm',p_df['culmen_length_mm'].cast("float").alias('culmen_length_mm'))<br/>p_df=p_df.withColumn('culmen_depth_mm',p_df['culmen_depth_mm'].cast("float").alias('culmen_depth_mm'))<br/>p_df=p_df.withColumn('flipper_length_mm',p_df['flipper_length_mm'].cast("float").alias('flipper_length_mm'))<br/>p_df=p_df.withColumn('culmen_length_mm',p_df['culmen_length_mm'].cast("float").alias('culmen_length_mm'))<br/>p_df=p_df.withColumn('culmen_depth_mm',p_df['culmen_depth_mm'].cast("float").alias('culmen_depth_mm'))<br/>p_df=p_df.withColumn('body_mass_g',p_df['body_mass_g'].cast("float").alias('body_mass_g'))</span><span id="a910" class="jy jz hi ju b fi ke kb l kc kd">p_df.printSchema()</span><span id="a280" class="jy jz hi ju b fi ke kb l kc kd">## Split the data into train and test in the ration 80:20<br/>p_trainDF, p_testDF = p_df.randomSplit([.8, .2], seed=42)<br/>print(f"""There are {p_trainDF.count()} rows in the training set,<br/>and {p_testDF.count()} in the test set""")</span><span id="833b" class="jy jz hi ju b fi ke kb l kc kd">## VectorAssembler takes a list of input columns and creates a new DataFrame with an additional column, which we will call features. It combines<br/>##the values of those input columns into a single vector<br/>vecAssembler = VectorAssembler(inputCols=["culmen_length_mm", "culmen_depth_mm",<br/> "flipper_length_mm"], outputCol="features")<br/>vecTrainDF = vecAssembler.transform(p_trainDF)<br/>vecTrainDF.select("species", "island", "culmen_length_mm", "culmen_depth_mm",<br/> "flipper_length_mm","sex","features", "body_mass_g").show(10)</span><span id="8080" class="jy jz hi ju b fi ke kb l kc kd">## We will identify columns which are categorical and one-hot-code them<br/>categoricalCols = [field for (field, dataType) in p_trainDF.dtypes<br/> if dataType == "string"]<br/>indexOutputCols = [x + "Index" for x in categoricalCols]<br/>oheOutputCols = [x + "OHE" for x in categoricalCols]<br/>stringIndexer = StringIndexer(inputCols=categoricalCols,<br/> outputCols=indexOutputCols,<br/> handleInvalid="skip")<br/>oheEncoder = OneHotEncoder(inputCols=indexOutputCols,<br/> outputCols=oheOutputCols)<br/>numericCols = [field for (field, dataType) in p_trainDF.dtypes<br/> if ((dataType == "float") &amp; (field != "body_mass_g"))]<br/>assemblerInputs = oheOutputCols + numericCols<br/>vecAssembler = VectorAssembler(inputCols=assemblerInputs,<br/> outputCol="features")</span><span id="b27c" class="jy jz hi ju b fi ke kb l kc kd">##we put all the feature preparation and model building into the pipeline, and<br/>##apply it to our data set<br/>lr = LinearRegression(labelCol="body_mass_g", featuresCol="features")<br/>pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])</span><span id="b691" class="jy jz hi ju b fi ke kb l kc kd">pipelineModel = pipeline.fit(p_trainDF)<br/>predDF = pipelineModel.transform(p_testDF)<br/>predDF.select("features", "body_mass_g", "prediction").show(5)<br/>predDF.show(5)</span><span id="2d49" class="jy jz hi ju b fi ke kb l kc kd">## Evaluate the Model <br/>regressionEvaluator = RegressionEvaluator(<br/> predictionCol="prediction",<br/> labelCol="body_mass_g",<br/> metricName="rmse")<br/>rmse = regressionEvaluator.evaluate(predDF)<br/>print(f"RMSE is {rmse:.1f}")<br/>r2 = regressionEvaluator.setMetricName("r2").evaluate(predDF)<br/>print(f"R2 is {r2}")<br/>pipelineModel.write().overwrite().save('gs://'+bucket_name+'/'+user_name+'_dt_model/dt_model.model')</span><span id="2e14" class="jy jz hi ju b fi ke kb l kc kd">#Writing the output of the model evaluation to BigQuery<br/>spark.conf.set("parentProject", project_name)<br/>bucket = bucket_name<br/>spark.conf.set("temporaryGcsBucket",bucket)<br/>predDF.write.format('bigquery') .mode("overwrite").option('table', project_name+':'+dataset_name+'.'+user_name+'penguin_model_eval_output') .save()</span><span id="7fef" class="jy jz hi ju b fi ke kb l kc kd">print('Job Completed Successfully!')</span></pre><p id="65de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型保存在GCS中，模型预测结果写入BigQuery。</p><p id="b4d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代码“model_building.py”保存在GCS文件夹中。</p><p id="e3b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">server less _ spark _ ml . py:</strong>我们将使用cloud composer dag来编排这个过程。虽然这是一个包含从GCS到pandas dataframe &gt;模型构建&amp;评估&gt;将模型结果导出到BigQuery的3步流程，但生产ML管道可能有额外的步骤，尤其是围绕数据准备、CI/CD和模型服务，因此需要使用Cloud Composer等编排工具。</p><p id="1eb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dag代码如下所示，</p><pre class="je jf jg jh fd jt ju jv jw aw jx bi"><span id="925c" class="jy jz hi ju b fi ka kb l kc kd">import os<br/>from airflow.models import Variable<br/>from datetime import datetime<br/>from airflow import models<br/>from airflow.providers.google.cloud.operators.dataproc import (DataprocCreateBatchOperator,DataprocGetBatchOperator)<br/>from datetime import datetime<br/>from airflow.utils.dates import days_ago<br/>import string<br/>import random # define the random module<br/>S = 10  # number of characters in the string.<br/># call random.choices() string module to find the string in Uppercase + numeric data.<br/>ran = ''.join(random.choices(string.digits, k = S))<br/>project_id = models.Variable.get("project_id")<br/>region = models.Variable.get("region")<br/>subnet=models.Variable.get("subnet")<br/>phs_server=Variable.get("phs")<br/>code_bucket=Variable.get("code_bucket")<br/>bq_dataset=Variable.get("bq_dataset")<br/>umsa=Variable.get("umsa")</span><span id="de89" class="jy jz hi ju b fi ke kb l kc kd">name="rmanj"</span><span id="9be2" class="jy jz hi ju b fi ke kb l kc kd">dag_name= "serverless_spark_ml"<br/>service_account_id= &lt;&lt;enter your service account&gt;&gt;</span><span id="27c9" class="jy jz hi ju b fi ke kb l kc kd">penguin_model= "gs://"+code_bucket+"/00-scripts/penguin_model.py"</span><span id="8287" class="jy jz hi ju b fi ke kb l kc kd">BATCH_ID = "penguin-weight-"+str(ran)</span><span id="7b95" class="jy jz hi ju b fi ke kb l kc kd">BATCH_CONFIG1 = {<br/>    "pyspark_batch": {<br/>        "main_python_file_uri": penguin_model,<br/>        "args": [<br/>          project_id,<br/>          bq_dataset,<br/>          code_bucket,<br/>          name<br/>        ],<br/>        "jar_file_uris": [<br/>      "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar"<br/>    ]<br/>    },<br/>    "environment_config":{<br/>        "execution_config":{<br/>              "service_account": service_account_id,<br/>            "subnetwork_uri": subnet<br/>            },<br/>        "peripherals_config": {<br/>            "spark_history_server_config": {<br/>                "dataproc_cluster": f"projects/{project_id}/regions/{region}/clusters/{phs_server}"<br/>                }<br/>            },<br/>        },<br/>}</span><span id="b270" class="jy jz hi ju b fi ke kb l kc kd">with models.DAG(<br/>    dag_name,<br/>    schedule_interval=None,<br/>    start_date = days_ago(2),<br/>    catchup=False,<br/>) as dag_serverless_batch:<br/>    # [START how_to_cloud_dataproc_create_batch_operator]<br/>    create_serverless_batch1 = DataprocCreateBatchOperator(<br/>        task_id="model_building",<br/>        project_id=project_id,<br/>        region=region,<br/>        batch=BATCH_CONFIG1,<br/>        batch_id=BATCH_ID,<br/>    )<br/>    <br/>    create_serverless_batch1</span></pre><p id="c1ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦您将它保存在dags文件夹中，它应该会出现在Composer UI中。然后，您可以触发管道，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kf"><img src="../Images/1a99a257f6e44f46e558d2c269e7c932.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MjlBNtpqjO3UGilzn8pYaw.png"/></div></div></figure><p id="8a39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">执行后，您可以在BigQuery中看到模型预测结果，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kg"><img src="../Images/038cf22f432f380ddbb2509d558a820c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYmBUO9NZqPCyMq3Gulj_Q.png"/></div></div></figure><p id="c68b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GCS中的模型，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kh"><img src="../Images/590cc7a270cfcb150c91bb27e09ca5d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cCwdSY4-hIrhnRHisKsJqA.png"/></div></div></figure><p id="385a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">&lt;<make sure="" to="" delete="" the="" resources="" especially="" composer="" after="" pipeline="" runs="">&gt;</make></p><p id="6d4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以通过云构建进一步扩展这一渠道，以包括CI/CD。通过这种方式，您可以将spark工作负载迁移到GCP，获得Spark on GCP的自动扩展、无服务器功能的全部优势，同时仍然保留非粘性或非锁定实施的优势。</p></div></div>    
</body>
</html>