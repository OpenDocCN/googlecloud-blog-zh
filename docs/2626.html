<html>
<head>
<title>Using Dataproc Serverless to migrate your Hbase data to GCS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Dataproc Serverless将您的Hbase数据迁移到GCS</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/using-dataproc-serverless-to-migrate-your-hbase-data-to-gcs-bf1ccf4ab945?source=collection_archive---------5-----------------------#2022-12-13">https://medium.com/google-cloud/using-dataproc-serverless-to-migrate-your-hbase-data-to-gcs-bf1ccf4ab945?source=collection_archive---------5-----------------------#2022-12-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c203" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated"><span class="l je jf jg bm jh ji jj jk jl di">我们</span>可以使用Dataproc Serverless来运行Spark batch工作负载，而无需配置和管理我们自己的集群。我们可以指定工作负载参数，然后将工作负载提交给Dataproc无服务器服务。</p><p id="fc21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jm" href="https://cloud.google.com/dataproc-serverless/docs" rel="noopener ugc nofollow" target="_blank"> <em class="jn"> Dataproc无服务器</em> </a> <em class="jn">帮助用户完成整个基础设施管理工作—执行他们的</em><a class="ae jm" href="http://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"><em class="jn">Apache Spark</em></a><em class="jn">工作负载用户是</em> <strong class="ih hj"> <em class="jn">而不是</em> </strong> <em class="jn"> </em> <strong class="ih hj"> <em class="jn">需要</em> </strong> <em class="jn">先创建一个集群才能执行任何操作。用户只需根据自己的使用情况选择一个模板，只需点击几下鼠标和几个命令，即可完成各自的工作。</em></p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jo"><img src="../Images/65aa8699769ff33e97c4ff9afb6411b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_zTfE_x8Z2FJeC-lB35G9Q.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">使用Dataproc无服务器从Hbase迁移到GCS</figcaption></figure></div><div class="ab cl ke kf gp kg" role="separator"><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj"/></div><div class="hb hc hd he hf"><h2 id="f5c8" class="kl km hi bd kn ko kp kq kr ks kt ku kv iq kw kx ky iu kz la lb iy lc ld le lf bi translated">目标</h2><p id="508d" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">这篇博客文章将分享关于如何使用“<a class="ae jm" href="https://github.com/GoogleCloudPlatform/dataproc-templates/tree/main/java/src/main/java/com/google/cloud/dataproc/templates/hbase" rel="noopener ugc nofollow" target="_blank"> Hbase to GCS </a> Dataproc无服务器模板”进行数据迁移的完整细节。该模板将数据从Hbase表移动到GCS存储桶。</p><h2 id="a274" class="kl km hi bd kn ko kp kq kr ks kt ku kv iq kw kx ky iu kz la lb iy lc ld le lf bi translated">设置您的GCP项目和基础设施</h2><ol class=""><li id="a21a" class="ll lm hi ih b ii lg im lh iq ln iu lo iy lp jc lq lr ls lt bi translated">登录到您的GCP项目并启用Dataproc API(如果它被禁用的话)</li><li id="ce89" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">确保子网启用了私有Google访问，如果您要使用GCP生成的“默认”VPC网络，那么也必须启用私有访问，如下所示:</li></ol><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es lz"><img src="../Images/1d96118efc365e205a42ee7736e81987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPcitxbKUL7HHL3-fib42w.png"/></div></div></figure><pre class="jp jq jr js fd ma mb mc bn md me bi"><span id="01cd" class="mf km hi mb b be mg mh l mi mj">gcloud compute networks subnets update default --region=us-central1 --enable-private-ip-google-access</span></pre><p id="3952" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.为jar文件创建一个GCS存储桶和暂存位置。</p><pre class="jp jq jr js fd ma mb mc bn md me bi"><span id="c697" class="mf km hi mb b be mg mh l mi mj">export GCS_STAGING_BUCKET="my-gcs-staging-bucket"<br/>gsutil mb gs://$GCS_STAGING_BUCKET</span></pre><p id="543b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.要配置Dataproc无服务器作业，您需要导出以下变量</p><p id="b0a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du mk ml mm mb b">GCP_PROJECT</code>:运行Dataproc无服务器的GCP项目id。</p><p id="2aa6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du mk ml mm mb b">REGION</code>:运行Dataproc无服务器的区域。</p><p id="cbfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du mk ml mm mb b">GCS_STAGING_LOCATION</code> : GCS暂存桶位置，Dataproc将在此存储暂存资产(参见步骤3)。</p></div><div class="ab cl ke kf gp kg" role="separator"><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj"/></div><div class="hb hc hd he hf"><h2 id="e4d0" class="kl km hi bd kn ko kp kq kr ks kt ku kv iq kw kx ky iu kz la lb iy lc ld le lf bi translated">执行Dataproc模板的步骤</h2><ol class=""><li id="2bee" class="ll lm hi ih b ii lg im lh iq ln iu lo iy lp jc lq lr ls lt bi translated">克隆Dataproc模板库并导航到Java模板文件夹。</li></ol><pre class="jp jq jr js fd ma mb mc bn md me bi"><span id="6369" class="mf km hi mb b be mg mh l mi mj">git clone https://github.com/GoogleCloudPlatform/dataproc-templates.git<br/>cd dataproc-templates/java</span></pre><p id="e509" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.获取身份验证凭据(以提交作业)。</p><pre class="jp jq jr js fd ma mb mc bn md me bi"><span id="297e" class="mf km hi mb b be mg mh l mi mj">gcloud auth application-default login</span></pre><p id="66f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.通过导出提交所需的变量来配置Dataproc无服务器作业(如下面<em class="jn">“设置您的GCP项目&amp;”</em>的步骤4所述)。</p><pre class="jp jq jr js fd ma mb mc bn md me bi"><span id="d083" class="mf km hi mb b be mg mh l mi mj">export GCP_PROJECT=&lt;project_id&gt; # your Google Cloud project<br/>export REGION=&lt;region&gt; # your region for ex: us-central1<br/>export SUBNET=&lt;subnet&gt; # optional if you are using default<br/># export GCS_STAGING_LOCATION=&lt;gcs-staging-bucket-folder&gt; # already done at step 3(Under Setup your GCP Project &amp; Infra)</span></pre><p id="2a0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.设置HBase依赖关系:-向Dataproc Serverless提交作业时，需要传递一些HBase依赖关系。当为hbase表配置设置了CATALOG环境变量时，脚本会自动设置这些依赖关系。如果不是，那么需要使用— jars标志传递这些依赖关系，或者在使用Dataproc模板的情况下，使用jars环境变量。</p><p id="b6d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jm" href="https://mvnrepository.com/artifact/org.apache.hbase.connectors.spark/hbase-spark" rel="noopener ugc nofollow" target="_blank">Apache h base Spark Connector</a>依赖项(这些已经安装在Dataproc Serverless中，所以可以使用file://)引用它们:</p><ul class=""><li id="e270" class="ll lm hi ih b ii ij im in iq mn iu mo iy mp jc mq lr ls lt bi translated">file:///usr/lib/spark/external/h base-spark-protocol-shaded . jar</li><li id="112b" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc mq lr ls lt bi translated">file:///usr/lib/spark/external/h base-spark . jar</li><li id="1b61" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc mq lr ls lt bi translated">一旦将CATALOG环境变量用于hbase表配置，就会自动下载并设置所有其他依赖项。库链接(用于参考):- hbase-client，hbase-shaded-mapreduce</li></ul><p id="1ea3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.将<a class="ae jm" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/java/src/main/resources/hbase-site.xml" rel="noopener ugc nofollow" target="_blank"> hbase-site.xml </a>传递给作业:-现在，有两种方法可以做到这一点。第一，通过自动过程，第二，通过手动为dataproc集群创建一个定制容器。这两种情况如下所示</p><p id="fdb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">I .创建定制容器的自动过程:-当设置环境变量HBASE _站点_路径时，该过程在启动脚本中自动完成。</p><p id="dca2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">二。手动配置<a class="ae jm" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/java/src/main/resources/hbase-site.xml" rel="noopener ugc nofollow" target="_blank"> hbase-site.xml </a>并创建一个容器。相同的步骤如下所述</p><ul class=""><li id="cd13" class="ll lm hi ih b ii ij im in iq mn iu mo iy mp jc mq lr ls lt bi translated">hbase-site.xml需要在Dataproc Serverless使用的容器映像的某个路径中可用。</li><li id="52fc" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc mq lr ls lt bi translated">通过为hbase.rootdir和hbase.zookeeper.quorum添加各自的值，可以使用引用<a class="ae jm" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/java/src/main/resources/hbase-site.xml" rel="noopener ugc nofollow" target="_blank"> hbase-site.xml </a>。</li><li id="3eed" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc mq lr ls lt bi translated">GCP集装箱注册中心需要一个<a class="ae jm" href="https://cloud.google.com/dataproc-serverless/docs/guides/custom-containers#submit_a_spark_batch_workload_using_a_custom_container_image" rel="noopener ugc nofollow" target="_blank">定制集装箱图像</a>。参考<a class="ae jm" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/java/src/main/java/com/google/cloud/dataproc/templates/hbase/Dockerfile" rel="noopener ugc nofollow" target="_blank">文档</a>进行参考。</li><li id="04fd" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc mq lr ls lt bi translated">将以下层添加到Dockerfile，用于将您的本地hbase-site.xml复制到容器映像(下面的命令被添加到<a class="ae jm" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/java/src/main/java/com/google/cloud/dataproc/templates/hbase/Dockerfile" rel="noopener ugc nofollow" target="_blank"> Dockerfile </a>以供参考):</li></ul><pre class="jp jq jr js fd ma mb mc bn md me bi"><span id="2621" class="mf km hi mb b be mg mh l mi mj">COPY hbase-site.xml /etc/hbase/conf/</span></pre><p id="be65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以使用并修改上述指南中的Dockerfile文件，通过以下方式将其构建并推送到GCP集装箱注册中心:</p><pre class="jp jq jr js fd ma mb mc bn md me bi"><span id="c8c1" class="mf km hi mb b be mg mh l mi mj">IMAGE=gcr.io/&lt;your_project&gt;/&lt;your_custom_image&gt;:&lt;your_version&gt;<br/>docker build -t "${IMAGE}" .<br/>docker push "${IMAGE}"</span></pre><p id="5f32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.执行以下命令:-</p><p id="28ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jn">注意:-在此设置目录环境变量以提供hbase连接并让脚本下载所需的依赖项是很重要的。</em></p><pre class="jp jq jr js fd ma mb mc bn md me bi"><span id="ddf6" class="mf km hi mb b be mg mh l mi mj">export GCP_PROJECT=&lt;gcp-project-id&gt;<br/>export REGION=&lt;region&gt;<br/>export SUBNET=&lt;subnet&gt;<br/>export GCS_STAGING_LOCATION=&lt;gcs-staging-bucket-folder&gt;<br/>export IMAGE_NAME_VERSION=&lt;name:version of image&gt;<br/>export HBASE_SITE_PATH=&lt;path to hbase-site.xml&gt;<br/>export CATALOG=&lt;catalog of hbase table&gt;<br/>export IMAGE=gcr.io/${GCP_PROJECT}/${IMAGE_NAME_VERSION} #use the image which was created to configure hbase-site.xml<br/><br/>bin/start.sh \<br/>--container-image=$IMAGE \<br/>--properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/'  \<br/>-- --template HBASETOGCS \<br/>--templateProperty hbasetogcs.output.fileformat=&lt;avro|csv|parquet|json|orc&gt;  \<br/>--templateProperty hbasetogcs.output.savemode=&lt;Append|Overwrite|ErrorIfExists|Ignore&gt; \<br/>--templateProperty hbasetogcs.output.path=&lt;output-gcs-path&gt;<br/>--templateProperty hbasetogcs.table.catalog=$CATALOG</span></pre><h2 id="943d" class="kl km hi bd kn ko kp kq kr ks kt ku kv iq kw kx ky iu kz la lb iy lc ld le lf bi translated">样本执行</h2><p id="33de" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">请参考下面的执行示例:-</p><pre class="jp jq jr js fd ma mb mc bn md me bi"><span id="29e1" class="mf km hi mb b be mg mh l mi mj">export GCP_PROJECT=myproject<br/>export REGION=us-central1<br/>export GCS_STAGING_LOCATION=gs://staging_bucket<br/>export JOB_TYPE=SERVERLESS <br/>export SUBNET=projects/myproject/regions/us-central1/subnetworks/default<br/>export IMAGE_NAME_VERSION=dataproc-hbase:1<br/>export HBASE_SITE_PATH=src/main/resources/hbase-site.xml<br/>export CATALOG='{"table":{"namespace":"default","name":"my_table"},"rowkey":"key","columns":{"key":{"cf":"rowkey","col":"key","type":"string"},"name":{"cf":"cf","col":"name","type":"string"}}}'<br/>export IMAGE=gcr.io/${GCP_PROJECT}/${IMAGE_NAME_VERSION}  #set this to pass custom image during job submit<br/><br/>bin/start.sh \<br/>--container-image=$IMAGE \<br/>--properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/'  \<br/>-- --template HBASETOGCS \<br/>--templateProperty hbasetogcs.output.fileformat=csv \<br/>--templateProperty hbasetogcs.output.savemode=append \<br/>--templateProperty hbasetogcs.output.path=gs://myproject/output  \<br/>--templateProperty hbasetogcs.table.catalog=$CATALOG</span></pre><p id="6751" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">样本目录:-</p><pre class="jp jq jr js fd ma mb mc bn md me bi"><span id="ba2c" class="mf km hi mb b be mg mh l mi mj">{<br/>   "table":{<br/>      "namespace":"default",<br/>      "name":"my_table"<br/>   },<br/>   "rowkey":"key",<br/>   "columns":{<br/>      "key":{<br/>         "cf":"rowkey",<br/>         "col":"key",<br/>         "type":"string"<br/>      },<br/>      "name":{<br/>         "cf":"cf",<br/>         "col":"name",<br/>         "type":"string"<br/>      }<br/>   }<br/>}</span></pre><p id="2f44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jn">此外，如果您需要</em> <a class="ae jm" href="https://cloud.google.com/dataproc-serverless/docs/concepts/properties" rel="noopener ugc nofollow" target="_blank"> <em class="jn">指定Dataproc Serverless支持的spark属性</em> </a> <em class="jn">，例如:调整驱动程序、内核、执行器等的数量，您可以编辑start.sh文件中的OPT_PROPERTIES值。</em></p><p id="67fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7.监控Spark批处理作业</p><p id="eb9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">提交作业后，我们将能够在<a class="ae jm" href="https://console.cloud.google.com/dataproc/batches" rel="noopener ugc nofollow" target="_blank"> Dataproc批处理UI </a>中看到它。从那里，我们可以查看作业的指标和日志。</p></div><div class="ab cl ke kf gp kg" role="separator"><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj"/></div><div class="hb hc hd he hf"><h2 id="eea1" class="kl km hi bd kn ko kp kq kr ks kt ku kv iq kw kx ky iu kz la lb iy lc ld le lf bi translated">参考</h2><ul class=""><li id="87f5" class="ll lm hi ih b ii lg im lh iq ln iu lo iy lp jc mq lr ls lt bi translated"><a class="ae jm" href="https://cloud.google.com/dataproc-serverless/docs/overview" rel="noopener ugc nofollow" target="_blank"> Dataproc无服务器</a></li><li id="19fa" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc mq lr ls lt bi translated"><a class="ae jm" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> Dataproc模板库</a></li></ul><p id="b280" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如有任何疑问/建议，请联系:dataproc-templates-support-external@googlegroups.com</p></div></div>    
</body>
</html>