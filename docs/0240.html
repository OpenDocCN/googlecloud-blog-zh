<html>
<head>
<title>Containerized Jupyter notebooks on GPU on Google Cloud</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谷歌云GPU上的容器化Jupyter笔记本</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/containerized-jupyter-notebooks-on-gpu-on-google-cloud-8e86ef7f31e9?source=collection_archive---------0-----------------------#2017-03-28">https://medium.com/google-cloud/containerized-jupyter-notebooks-on-gpu-on-google-cloud-8e86ef7f31e9?source=collection_archive---------0-----------------------#2017-03-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1dd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在之前的<a class="ae jd" rel="noopener" href="/@durgeshm/running-jupyter-notebooks-on-gpu-on-google-cloud-d44f57d22dbd">帖子</a>中，我列出了在GCP计算引擎的GPU实例上运行Jupyter笔记本的步骤。事实证明，有一种<em class="je">更容易</em>更灵活的方式。使用<em class="je">码头集装箱</em>。</p><p id="5a8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我假设你的谷歌云平台账户允许你创建基于GPU的实例。如果没有，请遵循上一篇<a class="ae jd" rel="noopener" href="/@durgeshm/running-jupyter-notebooks-on-gpu-on-google-cloud-d44f57d22dbd">文章</a>中的步骤1。此外，确保您拥有最新的<code class="du jf jg jh ji b">gcloud</code> SDK。</p><pre class="jj jk jl jm fd jn ji jo jp aw jq bi"><span id="3908" class="jr js hi ji b fi jt ju l jv jw">$ gcloud components update &amp;&amp; gcloud components install beta</span></pre><p id="8c24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">NVIDIA博客上的这篇文章解释了这种设置是如何工作的。GPU实例只需要在主机上安装NVIDIA驱动程序(以及Docker周围一个名为<code class="du jf jg jh ji b">nvidia-docker</code>的薄包装)。所有其他软件，如CUDA toolkit、cuDNN、Python、Jupyter和任何深度学习库，都可以简单地封装成可重用的Docker映像。NVIDIA和大多数深度学习框架的作者(如TensorFlow、Keras、PyTorch)提供了现成的Docker图像，您可以直接使用或作为基础图像。</p><p id="82d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第一步:创建GPU实例</strong></p><p id="6dd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当然，您可以使用云控制台来创建基于GPU的实例。但是，我将使用<code class="du jf jg jh ji b">gcloud</code>命令行工具。</p><pre class="jj jk jl jm fd jn ji jo jp aw jq bi"><span id="be95" class="jr js hi ji b fi jt ju l jv jw">$ gcloud beta compute instances create <strong class="ji hj">gpu-docker-host</strong> --machine-type n1-standard-2 --zone <strong class="ji hj">us-east1-d</strong> --accelerator type=<strong class="ji hj">nvidia-tesla-k80,count=1</strong> --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --boot-disk-size 50GB --maintenance-policy TERMINATE --restart-on-failure </span></pre><p id="d550" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将在<code class="du jf jg jh ji b">us-east1-d</code>区域中创建一个名为<code class="du jf jg jh ji b">gpu-docker-host</code>的实例，使用1个GPU和Ubuntu 16.04(持久磁盘大小为50GB)。</p><p id="72bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦您的GPU实例准备就绪，您就可以通过ssh客户端或<code class="du jf jg jh ji b">gcloud compute ssh gpu-docker-host --zone us-east1-d</code>命令连接到它。</p><p id="173d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第二步:安装NVIDIA驱动，docker和nvidia-docker </strong></p><p id="b401" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在服务器上，下载该脚本以安装依赖项:</p><pre class="jj jk jl jm fd jn ji jo jp aw jq bi"><span id="93ca" class="jr js hi ji b fi jt ju l jv jw">$ curl -O -s <a class="ae jd" href="https://gist.githubusercontent.com/durgeshm/b149e7baec4d4508eb4b2914d63018c7/raw/798aadbb54b451abcaba9bfeb833327fa4b3d53b/deps_nvidia_docker.sh" rel="noopener ugc nofollow" target="_blank">https://gist.githubusercontent.com/durgeshm/b149e7baec4d4508eb4b2914d63018c7/raw/798aadbb54b451abcaba9bfeb833327fa4b3d53b/deps_nvidia_docker.sh</a></span></pre><p id="b1c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该脚本自动执行以下任务(最好看一看，而不是运行陌生人的脚本) :</p><ol class=""><li id="25ca" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc kc kd ke kf bi translated">确认该实例具有来自NVIDIA的GPU，否则退出。</li><li id="bca9" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">如有必要，检查并安装NVIDIA驱动程序(适用于Tesla K80)。</li><li id="362f" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">必要时检查并安装<code class="du jf jg jh ji b">docker</code>。</li><li id="76be" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">如有必要，检查并安装<code class="du jf jg jh ji b">nvidia-docker</code>。</li></ol><pre class="jj jk jl jm fd jn ji jo jp aw jq bi"><span id="29c6" class="jr js hi ji b fi jt ju l jv jw">$ sudo sh deps_nvidia_docker.sh</span></pre><blockquote class="kl km kn"><p id="7267" class="if ig je ih b ii ij ik il im in io ip ko ir is it kp iv iw ix kq iz ja jb jc hb bi translated"><strong class="ih hj">注意</strong>:如果您在本地下载脚本并通过添加<code class="du jf jg jh ji b">--metadata-from-file startup-script=<strong class="ih hj">deps_nvidia_docker.sh</strong></code> <strong class="ih hj">将其作为启动脚本传递给‘g cloud’命令，那么您可以在一个步骤中创建实例时安装这些依赖项。</strong>但是，我在这里把步骤分开了。</p></blockquote><p id="e17d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤3:准备运行任何支持CUDA的docker容器！</strong></p><p id="8b57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦脚本完成安装nvidia-docker，我们就可以从nvidia运行一个简单的测试容器了。</p><pre class="jj jk jl jm fd jn ji jo jp aw jq bi"><span id="f952" class="jr js hi ji b fi jt ju l jv jw">$ sudo nvidia-docker run --rm nvidia/cuda nvidia-smi</span></pre><p id="0bf7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您在控制台中看到GPU和驱动程序信息，那么您的设置已经就绪。(显然，第一次运行时，从Docker hub提取<code class="du jf jg jh ji b">nvidia/cuda</code>图像需要几秒钟的时间)。</p><p id="9476" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们尝试一个TensorFlow/Keras/Jupyter docker容器(<a class="ae jd" href="https://github.com/durgeshm/dockerfiles/blob/master/jupyter-keras-gpu/Dockerfile" rel="noopener ugc nofollow" target="_blank"> Dockerfile </a>)。</p><pre class="jj jk jl jm fd jn ji jo jp aw jq bi"><span id="4569" class="jr js hi ji b fi jt ju l jv jw">$ mkdir notebooks # to persist notebooks on the host</span><span id="c14c" class="jr js hi ji b fi kr ju l jv jw">$ sudo <strong class="ji hj">nvidia-docker</strong> run -it --rm -d -v $(pwd)/notebooks:/notebooks -p 8888:8888 --name keras durgeshm/jupyter-keras-gpu</span></pre><p id="d344" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">检查容器日志以确认Jupyter正在运行:</p><pre class="jj jk jl jm fd jn ji jo jp aw jq bi"><span id="06f6" class="jr js hi ji b fi jt ju l jv jw">~$ sudo docker logs keras</span></pre><p id="3921" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了将来更容易启动容器，我还添加了一个脚本<code class="du jf jg jh ji b">run-keras.sh</code></p><pre class="jj jk jl jm fd jn ji jo jp aw jq bi"><span id="76ca" class="jr js hi ji b fi jt ju l jv jw">~$ echo 'sudo nvidia-docker run -it --rm -d -v $(pwd)/notebooks:/notebooks -p 8888:8888 --name keras durgeshm/jupyter-keras-gpu' &gt; run-keras.sh &amp;&amp; chmod u+x run-keras.sh</span></pre><p id="67c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第四步:SSH隧道转发</strong></p><p id="07da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从本地机器建立一个隧道，通过ssh访问Jupyter。</p><p id="1528" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您已经在服务器上启动了<code class="du jf jg jh ji b">keras</code>容器，那么在您的本地机器上运行以下命令。</p><pre class="jj jk jl jm fd jn ji jo jp aw jq bi"><span id="1aef" class="jr js hi ji b fi jt ju l jv jw">$ ssh -i .ssh/ubuntu_gcp -L 8899:localhost:8888 -f -N ubuntu@&lt;gpu-docker-host&gt;</span></pre><p id="a27b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我为自己定义了一个方便的别名，从本地机器远程启动keras容器并立即打开隧道。</p><pre class="jj jk jl jm fd jn ji jo jp aw jq bi"><span id="62e2" class="jr js hi ji b fi jt ju l jv jw">$ alias <strong class="ji hj">tf-gpu</strong>="ssh gpu-docker-host './run-keras.sh' &amp;&amp; ssh -fNL 8899:localhost:8888 gpu-docker-host"</span><span id="7711" class="jr js hi ji b fi kr ju l jv jw">$ <strong class="ji hj">tf-gpu</strong><br/>66357b10b6b4ec70e53273dc98878f1525f62fa6e6b1ee7d69995486f28bad1e</span><span id="72bb" class="jr js hi ji b fi kr ju l jv jw"># ^ that is the container id that was just started.</span></pre><p id="1839" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第五步:在你的浏览器中本地使用Jupyter</strong></p><p id="4cb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">导航到<a class="ae jd" href="http://localhost:8899/" rel="noopener ugc nofollow" target="_blank"> http://localhost:8899/ </a>并创建一个新笔记本。通过导入keras或tensorflow进行验证。<code class="du jf jg jh ji b">ssh gpu-docker-host "sudo docker logs keras"</code>可以确认CUDA库是否正在加载。</p><figure class="jj jk jl jm fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ks"><img src="../Images/c19856854f477a1799c8cd232290fd97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cidBnO6teKYE3r_FhN42sg.png"/></div></div></figure><figure class="jj jk jl jm fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es la"><img src="../Images/210077b8114f0ef72474b474c14bd2a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TebSsKp2H-sXPyYY9kzqSg.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated">ssh GPU-docker-host " sudo docker logs keras "</figcaption></figure><p id="0668" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦完成，请<strong class="ih hj"> <em class="je">记得停止你的</em> </strong>实例以节省成本。感谢阅读。</p><p id="08b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">备注:</strong></p><ol class=""><li id="fc35" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc kc kd ke kf bi translated">我已经分别为Keras/TensorFlow和PyTorch创建了标记为<code class="du jf jg jh ji b">durgeshm/jupyter-keras-gpu</code> ( <a class="ae jd" href="https://github.com/durgeshm/dockerfiles/blob/master/jupyter-keras-gpu/Dockerfile" rel="noopener ugc nofollow" target="_blank"> Dockerfile </a>)和<code class="du jf jg jh ji b">durgeshm/jupyter-pytorch-gpu</code> ( <a class="ae jd" href="https://github.com/durgeshm/dockerfiles/blob/master/jupyter-pytorch-gpu/Dockerfile" rel="noopener ugc nofollow" target="_blank"> Dockerfile </a>)的Docker图像。</li><li id="6551" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">此外，您可以随时使用<a class="ae jd" href="https://github.com/fchollet/keras/blob/master/docker/Dockerfile" rel="noopener ugc nofollow" target="_blank">https://github . com/fchollet/keras/blob/master/docker/docker file</a>、<a class="ae jd" href="https://github.com/pytorch/pytorch/blob/master/Dockerfile" rel="noopener ugc nofollow" target="_blank">https://github.com/pytorch/pytorch/blob/master/Dockerfile</a>或<a class="ae jd" href="https://hub.docker.com/r/tensorflow/tensorflow/" rel="noopener ugc nofollow" target="_blank">https://hub.docker.com/r/tensorflow/tensorflow/</a>的<em class="je">官方</em> docker图片作为您自己定制的基础图片。</li><li id="b36e" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">请注意，容器中的<code class="du jf jg jh ji b">/notebooks</code>卷是从主机上的<code class="du jf jg jh ji b">~/notebooks</code>装载的。这样，您总是可以删除旧容器，但您的笔记本将保留在主机上。</li><li id="bc29" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">只有<em class="je">步骤1 </em>是针对Google云平台的。其他步骤应该在其他云平台上工作(使用GPU，特别是Tesla K80)</li></ol></div></div>    
</body>
</html>