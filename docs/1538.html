<html>
<head>
<title>Kafka to BigQuery using Dataflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用数据流从Kafka到BigQuery</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/kafka-to-bigquery-using-dataflow-6ec73ec249bb?source=collection_archive---------0-----------------------#2020-07-28">https://medium.com/google-cloud/kafka-to-bigquery-using-dataflow-6ec73ec249bb?source=collection_archive---------0-----------------------#2020-07-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="d42b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">在本文中，我们评估了使用数据流将Kafka连接到BigQuery的两种不同方法</h2></div><blockquote class="ix iy iz"><p id="c716" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">声明:我在谷歌的云团队工作。观点是我自己的，而不是我现在雇主的观点。</p></blockquote><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es jx"><img src="../Images/b60c1a523b5f82e21d13b04faac72099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDu7Z4Of1BG_Y5dkFSrBzw.png"/></div></div></figure><h1 id="ea98" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">流式分析</h1><p id="9187" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">许多组织依靠开源的<strong class="jd hj">流媒体</strong>平台<a class="ae lj" href="https://kafka.apache.org" rel="noopener ugc nofollow" target="_blank"> Kafka </a>来构建实时数据管道和应用。<br/>同样的组织经常寻求现代化他们的IT环境，并采用<a class="ae lj" href="https://cloud.google.com/bigquery" rel="noopener ugc nofollow" target="_blank"> BigQuery </a>来满足他们不断增长的<strong class="jd hj">分析</strong>需求。<br/>通过将Kafka流数据连接到BigQuery分析功能，这些组织可以快速分析和激活数据衍生的洞察，而不是等待批处理过程完成。这种强大的组合支持实时<strong class="jd hj">流分析</strong>用例，例如欺诈检测、库存或车队管理、动态建议、预测性维护、容量规划...</p><h1 id="953b" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">λ、κ和数据流</h1><p id="0ccc" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">组织已经实施了<a class="ae lj" href="http://lambda-architecture.net" rel="noopener ugc nofollow" target="_blank">λ</a>或<a class="ae lj" href="https://www.oreilly.com/radar/questioning-the-lambda-architecture/" rel="noopener ugc nofollow" target="_blank"> Kappa </a>架构来支持批处理和流数据处理。<br/>但是这两种架构都有一些缺点。以Lambda为例，批处理端和流端都需要不同的代码库。有了Kappa，一切都被视为数据流，即使是大文件也必须被送入流处理系统，这有时会影响性能。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lk"><img src="../Images/7a49aa1800395c5cd9e2b245710971da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Vvf-55b6A1jZTuNYQL8dQ.png"/></div></div></figure><p id="5e30" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">最近(2015年)，谷歌发布了<a class="ae lj" href="http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf" rel="noopener ugc nofollow" target="_blank">数据流模型</a>论文，这是一个<strong class="jd hj">批处理和流的统一编程模型。可以说这个模型是一个Lambda架构，但是没有维护两个不同代码库的缺点。<br/> <a class="ae lj" href="https://beam.apache.org" rel="noopener ugc nofollow" target="_blank"> Apache Beam </a>就是这个模型的开源实现。阿帕奇梁支撑<a class="ae lj" href="https://beam.apache.org/documentation/runners/capability-matrix/" rel="noopener ugc nofollow" target="_blank">很多跑者</a>。在Google Cloud中，Beam代码在完全托管的数据处理服务上运行得最好，该服务与上面链接的白皮书同名:<a class="ae lj" href="https://cloud.google.com/dataflow" rel="noopener ugc nofollow" target="_blank"> Cloud Dataflow </a>。</strong></p><p id="8efe" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">以下是如何使用运行在Google Cloud Dataflow上的Apache Beam将Kafka消息摄取到BigQuery中的分步指南。</p><h1 id="08c9" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">环境设置</h1><p id="79d7" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">让我们从安装一个Kafka实例开始。</p><p id="885c" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">导航到<a class="ae lj" href="https://console.cloud.google.com/marketplace" rel="noopener ugc nofollow" target="_blank">谷歌云市场</a>，搜索“卡夫卡”。<br/>在返回的解决方案列表中，选择<strong class="jd hj"> Google Click to Deploy </strong>提供的Kafka解决方案，如下图中蓝色突出显示的。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ll"><img src="../Images/6a2289144591a9a9fd8621a142e78fe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*akMoYFeLDjxs8skrInf9LQ.png"/></div></div></figure><p id="cfe3" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">选择您希望虚拟机位于的地区/区域，例如<code class="du lm ln lo lp b">europe-west1-b</code> 🇧🇪 <br/>保留其他所有内容的默认设置(除非您希望使用自定义网络)，然后单击“部署”。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lq"><img src="../Images/78a5274178c184fd180415c45865e278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SoRwE0yXtvfB7EL9kn9ldw.png"/></div></div></figure><h2 id="4764" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">创建大查询表</h2><p id="a498" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">在部署我们的VM时，让我们定义一个JSON模式并创建我们的BigQuery表。通常最佳实践是在第一个到达的Kafka消息之前创建BigQuery表，而不是创建它。这是因为第一个Kafka消息可能有一些未设置的可选字段。因此，使用<a class="ae lj" href="https://cloud.google.com/bigquery/docs/schema-detect" rel="noopener ugc nofollow" target="_blank">模式自动检测</a>从中推断出的BigQuery模式是不完整的。</p><blockquote class="ix iy iz"><p id="c254" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">注意，如果您的模式因为变化太频繁而无法定义，那么将您的JSON作为BigQuery中的单个字符串列绝对是一个选择。然后您可以使用<a class="ae lj" href="https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions" rel="noopener ugc nofollow" target="_blank"> JSON函数</a>来解析它。</p></blockquote><p id="0d2b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">出于本文的目的，我们将创建一个表来存储多种产品的示例购买事件。<br/>在名为<code class="du lm ln lo lp b">schema.json</code>的文件中复制/粘贴以下JSON:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="3f8a" class="lr kk hi lp b fi mj mk l ml mm">[<br/>  {<br/>    "description": "Transaction time",<br/>    "name": "transaction_time",<br/>    "type": "TIMESTAMP",<br/>    "mode": "REQUIRED"<br/>  },<br/>  {<br/>    "description": "First name",<br/>    "name": "first_name",<br/>    "type": "STRING",<br/>    "mode": "REQUIRED"<br/>  },<br/>  {<br/>    "description": "Last name",<br/>    "name": "last_name",<br/>    "type": "STRING",<br/>    "mode": "REQUIRED"<br/>  },<br/>  {<br/>    "description": "City",<br/>    "name": "city",<br/>    "type": "STRING",<br/>    "mode": "NULLABLE"<br/>  },<br/>  {<br/>    "description": "List of products",<br/>    "name": "products",<br/>    "type": "RECORD",<br/>    "mode": "REPEATED",<br/>    "fields": [<br/>      {<br/>        "description": "Product name",<br/>        "name": "product_name",<br/>        "type": "STRING",<br/>        "mode": "REQUIRED"<br/>      },<br/>      {<br/>        "description": "Product price",<br/>        "name": "product_price",<br/>        "type": "FLOAT64",<br/>        "mode": "NULLABLE"<br/>      }<br/>    ]<br/>  }<br/>]</span></pre><p id="6c0b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">为了创建空的BigQuery表，我们最好使用由CI/CD系统触发的<a class="ae lj" href="https://en.wikipedia.org/wiki/Infrastructure_as_code" rel="noopener ugc nofollow" target="_blank"> IaC </a>工具，如<a class="ae lj" href="https://www.terraform.io/docs/providers/google/r/bigquery_table.html" rel="noopener ugc nofollow" target="_blank"> Terraform </a>。但是这可能是另一篇文章的主题，所以让我们使用<a class="ae lj" href="https://cloud.google.com/bigquery/docs/reference/bq-cli-reference#bq_mk" rel="noopener ugc nofollow" target="_blank"> bq mk </a>命令来创建我们的数据集和表。</p><p id="7ebf" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">打开<a class="ae lj" href="https://cloud.google.com/shell" rel="noopener ugc nofollow" target="_blank">云壳</a>，上传你之前创建的<code class="du lm ln lo lp b">schema.json</code>:</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es mn"><img src="../Images/2f84364f3d8097d528a11353ee1a2ef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jy_1MZqoTN87DbbV5tt_rQ.png"/></div></div></figure><p id="f655" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">然后，在Cloud Shell中运行以下命令来创建我们的<a class="ae lj" href="https://cloud.google.com/bigquery/docs/creating-column-partitions" rel="noopener ugc nofollow" target="_blank">时间戳分区表</a>。不要忘记将下面的<code class="du lm ln lo lp b">&lt;my-project&gt;</code>替换为您的GCP项目ID:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="f4bb" class="lr kk hi lp b fi mj mk l ml mm">gcloud config set project &lt;my-project&gt;<br/>bq mk --location EU --dataset kafka_to_bigquery<br/>bq mk --table \<br/>--schema schema.json \<br/>--time_partitioning_field transaction_time \<br/>kafka_to_bigquery<!-- -->.transactions</span></pre><blockquote class="ix iy iz"><p id="f069" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">除了使用<code class="du lm ln lo lp b">bq mk</code>命令，您还可以使用<a class="ae lj" href="https://cloud.google.com/bigquery/docs/bigquery-web-ui" rel="noopener ugc nofollow" target="_blank"> BigQuery web UI </a>创建数据集和表。</p></blockquote><h2 id="f28e" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">向卡夫卡主题发送消息</h2><p id="7f81" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">我们几乎完成了环境设置！最后一步是创建一个Kafka主题，并向其发送Kafka消息。导航到<a class="ae lj" href="https://console.cloud.google.com" rel="noopener ugc nofollow" target="_blank">谷歌云控制台</a>并打开计算引擎&gt;虚拟机实例。您应该看到我们之前创建的Kafka VM。单击下图中蓝色突出显示的SSH按钮。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es mo"><img src="../Images/9428d877e570a48b65afe5c5b8bfd737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqrj9XjQeJeKLjfMQZkBwA.png"/></div></div></figure><p id="b883" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">在打开的终端窗口中，输入以下命令来创建我们的Kafka主题，命名为<code class="du lm ln lo lp b">txtopic</code>:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="a8a6" class="lr kk hi lp b fi mj mk l ml mm">/opt/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181<!-- --> \<br/>--replication-factor 1<!-- --> \<br/>--partitions 1 --topic txtopic</span></pre><p id="f16f" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">通过列出不同的主题，确认主题已经创建。当输入以下命令时，您应该看到返回的<code class="du lm ln lo lp b">txtopic</code>:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="0f00" class="lr kk hi lp b fi mj mk l ml mm">/opt/kafka/bin/kafka-topics.sh --list --zookeeper localhost:2181</span></pre><p id="ff54" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">现在让我们想象一个购买事件被发送到我们的主题。使用SSH终端中的<code class="du lm ln lo lp b">vi</code>或<code class="du lm ln lo lp b">nano</code>，创建名为<code class="du lm ln lo lp b">message.json</code>的文件，并复制/粘贴下面的示例事务:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="0392" class="lr kk hi lp b fi mj mk l ml mm">{<br/>  "transaction_time": "2020-07-20 15:14:54",<br/>  "first_name": "John",<br/>  "last_name": "Smith",<br/>  "products": [<br/>    {<br/>      "product_name": "Pixel 4",<br/>      "product_price": 799.5<br/>    },<br/>    {<br/>      "product_name": "Pixel Buds 2",<br/>      "product_price": 179<br/>    }<br/>  ]<br/>}</span></pre><p id="8d9f" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">最后，用下面的命令将你的卡夫卡信息发送到<code class="du lm ln lo lp b">txtopic</code>。我们添加了一个Kafka消息密钥，并使用<code class="du lm ln lo lp b">jq</code>来压缩我们的JSON。</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="550a" class="lr kk hi lp b fi mj mk l ml mm">sudo apt-get install jq<br/>(echo -n "1|"; cat message.json | jq . -c) | /opt/kafka/bin/kafka-console-producer.sh \<br/>--broker-list localhost:9092 \<br/>--topic txtopic \<br/>--property "parse.key=true" \<br/>--property "key.separator=|"</span></pre><h1 id="ffd0" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated"><strong class="ak">方法1:使用数据流模板</strong></h1><p id="83f5" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">现在，我们的Kafka实例正在运行，让我们探索将消息发送到BigQuery的第一个方法。</p><h2 id="7394" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">卡夫凯奥</h2><p id="42f7" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">我们将使用Apache Beam内置的<a class="ae lj" href="https://beam.apache.org/releases/javadoc/2.19.0/org/apache/beam/sdk/io/kafka/KafkaIO.Read.html" rel="noopener ugc nofollow" target="_blank"> KafkaIO </a>连接器，它可以读取Kafka主题。<br/>要使用KafkaIO连接器，您可以使用Beam Java SDK实现自己的数据管道(自<a class="ae lj" href="https://beam.apache.org/blog/beam-2.22.0/" rel="noopener ugc nofollow" target="_blank"> Apache Beam 2.22 </a>发布以来，KafkaIO连接器也可用于Beam Python SDK)，或者从Google提供的数据流模板开始，该模板可从以下位置获得:<a class="ae lj" href="https://github.com/GoogleCloudPlatform/DataflowTemplates/tree/master/v2/kafka-to-bigquery" rel="noopener ugc nofollow" target="_blank">https://github . com/Google cloud platform/data flow templates/tree/master/v2/Kafka-To-big query</a></p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es mp"><img src="../Images/4ba75c371aaad779a67f8a4278eb6e6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HO_LvfbzoxraOEQq17fuMg.png"/></div></div></figure><h2 id="96d2" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">数据流弹性模板</h2><p id="b849" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">上面链接的代码使用了名为<a class="ae lj" href="https://cloud.google.com/dataflow/docs/guides/templates/using-flex-templates" rel="noopener ugc nofollow" target="_blank"> Dataflow Flex template </a>的新Dataflow模板机制，它可以将任何Dataflow管道转换为可供他人重用的模板。Flex模板是使用Docker打包的。数据流模板的第一个版本，现在称为<a class="ae lj" href="https://cloud.google.com/dataflow/docs/guides/templates/creating-templates" rel="noopener ugc nofollow" target="_blank">传统模板</a>，有一些已知的限制，因为许多Beam I/O不支持使用<a class="ae lj" href="https://cloud.google.com/dataflow/docs/guides/templates/creating-templates#valueprovider" rel="noopener ugc nofollow" target="_blank"> ValueProvider接口</a>的运行时参数。</p><blockquote class="ix iy iz"><p id="14ec" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">注意，Google在这里提供了另一个Kafka to BigQuery Dataflow Flex模板示例<a class="ae lj" href="https://github.com/GoogleCloudPlatform/java-docs-samples/tree/master/dataflow/flex-templates/kafka_to_bigquery" rel="noopener ugc nofollow" target="_blank">。也许以后两个例子会合并？</a></p></blockquote><h2 id="4420" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">封装数据流模板</h2><p id="dd1f" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">在Cloud Shell中，复制/粘贴以下命令，这些命令将构建容器化的模板代码并将其推送到<a class="ae lj" href="https://cloud.google.com/container-registry" rel="noopener ugc nofollow" target="_blank">容器注册表</a> (GCR)。您可以启用<a class="ae lj" href="https://cloud.google.com/shell/docs/how-cloud-shell-works#boost_mode" rel="noopener ugc nofollow" target="_blank">助推模式</a>使该步骤运行得更快。<br/>确保您的项目启用了GCR API，并且不要忘记在下面用您的GCP项目ID替换<code class="du lm ln lo lp b">&lt;my-project&gt;</code>。此外，您可以选择一个自定义的图像名称，并替换下面代码片段中的<code class="du lm ln lo lp b">&lt;my-image-name&gt;</code>。</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="083e" class="lr kk hi lp b fi mj mk l ml mm">git clone <a class="ae lj" href="https://github.com/GoogleCloudPlatform/DataflowTemplates" rel="noopener ugc nofollow" target="_blank">https://github.com/GoogleCloudPlatform/DataflowTemplates</a><br/>cd DataflowTemplates/v2/</span><span id="29e8" class="lr kk hi lp b fi mq mk l ml mm">export PROJECT=&lt;my-project&gt;<br/>export IMAGE_NAME=&lt;my-image-name&gt;</span><span id="670d" class="lr kk hi lp b fi mq mk l ml mm">export TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}<br/>export BASE_CONTAINER_IMAGE=gcr.io/dataflow-templates-base/java8-template-launcher-base<br/>export BASE_CONTAINER_IMAGE_VERSION=latest<br/>export TEMPLATE_MODULE=kafka-to-bigquery<br/>export APP_ROOT=/template/${TEMPLATE_MODULE}<br/>export COMMAND_SPEC=${APP_ROOT}/resources/${TEMPLATE_MODULE}-command-spec.json</span><span id="887c" class="lr kk hi lp b fi mq mk l ml mm">mvn clean package -Dimage=${TARGET_GCR_IMAGE} \<br/>                  -Dbase-container-image=${BASE_CONTAINER_IMAGE} \<br/>                  -Dbase-container-image.version=${BASE_CONTAINER_IMAGE_VERSION} \<br/>                  -Dapp-root=${APP_ROOT} \<br/>                  -Dcommand-spec=${COMMAND_SPEC} \<br/>                  -am -pl ${TEMPLATE_MODULE}</span></pre><blockquote class="ix iy iz"><p id="afc1" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">⚠️注意，我们克隆了Git存储库的主分支，可能README文件中的一些说明现在已经更新，与2020年7月撰写的这篇文章的内容不匹配。如果你遵循更新的自述文件说明，那可能是最好的。</p></blockquote><p id="7435" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">下一步是创建一个Google云存储(GCS)桶来存储运行我们的模板所需的各种工件。<br/>在云壳中输入以下命令之前，选择一个名称并替换下面的<code class="du lm ln lo lp b">&lt;bucket-name&gt;</code>:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="50aa" class="lr kk hi lp b fi mj mk l ml mm">export BUCKET_NAME=gs://&lt;bucket-name&gt;<br/>gsutil mb -l EU $BUCKET_NAME</span></pre><h2 id="56ce" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">创建等级库文件</h2><p id="64cd" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">要运行Dataflow Flex模板，需要在GCS中创建一个模板规范文件，其中包含运行作业所需的所有信息。<br/>让我们用以下内容创建一个名为<code class="du lm ln lo lp b">kafka-to-bigquery-image-spec.json</code>的文件。在保存文件之前，不要忘记编辑<code class="du lm ln lo lp b">&lt;my-project&gt;</code>和<code class="du lm ln lo lp b">&lt;my-image-name&gt;</code>。</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="02fb" class="lr kk hi lp b fi mj mk l ml mm">{<br/>  "image": "gcr.io/&lt;my-project&gt;/&lt;my-image-name&gt;",<br/>  "sdk_info": {<br/>    "language": "JAVA"<br/>  }<br/>}</span></pre><p id="ac2b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">然后，运行以下命令将文件上传到bucket，并将路径导出到一个环境变量，我们将在稍后启动管道时使用该变量。</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="47cd" class="lr kk hi lp b fi mj mk l ml mm">gsutil cp <!-- -->kafka-to-bigquery-image-spec.json <!-- -->$BUCKET_NAME/images/</span><span id="ca6f" class="lr kk hi lp b fi mq mk l ml mm">export TEMPLATE_IMAGE_SPEC=${BUCKET_NAME}/images/<!-- -->kafka-to-bigquery-<!-- -->image-spec.json</span></pre><p id="b14b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">您也可以使用命令<code class="du lm ln lo lp b">gcloud beta dataflow flex-template build</code>生成规格文件，如这里的<a class="ae lj" href="https://cloud.google.com/dataflow/docs/guides/templates/using-flex-templates#creating_a_flex_template" rel="noopener ugc nofollow" target="_blank">文档所述。</a></p><h2 id="6b20" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">管道参数</h2><p id="44c3" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">为了告诉数据流它应该连接到哪里，我们可以提供Kafka IP地址作为Flex模板的参数。<br/> <a class="ae lj" href="https://cloud.google.com/solutions/processing-messages-from-kafka-hosted-outside-gcp" rel="noopener ugc nofollow" target="_blank">如果你的卡夫卡没有托管在GCP </a>上，你可以阅读这个解决方案。<br/>要使用我们之前部署的Kafka安装，请导航到<a class="ae lj" href="https://console.cloud.google.com" rel="noopener ugc nofollow" target="_blank">谷歌云控制台</a>并打开计算引擎&gt;虚拟机实例。注意你的Kafka虚拟机的内部IP，并编辑下面的<code class="du lm ln lo lp b">&lt;internal-ip&gt;</code>。然后，在云壳中复制/粘贴:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="7c29" class="lr kk hi lp b fi mj mk l ml mm">export TOPICS=txtopic<br/>export BOOTSTRAP=&lt;internal-ip&gt;:9092</span></pre><p id="854d" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">我们的Flex模板还支持添加JavaScript UDF来指定自定义转换。上面的<code class="du lm ln lo lp b">message.json</code>中缺少BigQuery模式的<code class="du lm ln lo lp b">city</code>字段，所以让我们尝试添加它。我们不会检查特定的交易，所以下面的代码添加了城市“纽约”🗽所有交易。<br/>创建一个名为<code class="du lm ln lo lp b">my_function.js</code>的文件，复制/粘贴下面的JavaScript代码。然后将文件上传到前面创建的GCS bucket中。</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="94ca" class="lr kk hi lp b fi mj mk l ml mm">function transform(inJson) {<br/>  var obj = JSON.parse(inJson);<br/>  obj.city = "New York";<br/>  return JSON.stringify(obj);<br/>}</span></pre><blockquote class="ix iy iz"><p id="016a" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">请注意，我们只添加了一个字段以保持简单，但是我们当然可以在JavaScript中做更多的事情。例如，一个想法可以是比较事件时间和事务时间，以捕获后来的事件，并可能为这些事件添加自定义逻辑。</p></blockquote><h2 id="29fd" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">运行数据流模板</h2><p id="e926" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">最后，确保为您的项目启用了Dataflow API，并使用以下命令启动作业。<br/>我们指定<code class="du lm ln lo lp b">europe-west1</code>位置，这是我们决定在本例中使用的<a class="ae lj" href="https://cloud.google.com/dataflow/docs/concepts/regional-endpoints" rel="noopener ugc nofollow" target="_blank">数据流区域端点</a>。如果您想使用另一个区域，在同一个区域中创建您的GCS bucket、您的BigQuery数据集和您的Kafka VM是一个好主意。</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="a0d7" class="lr kk hi lp b fi mj mk l ml mm">export OUTPUT_TABLE=${PROJECT}:kafka_to_bigquery<!-- -->.transactions<br/>export JS_PATH=${BUCKET_NAME}/my_function.js<br/>export JS_FUNC_NAME=transform</span><span id="d387" class="lr kk hi lp b fi mq mk l ml mm">export JOB_NAME="${TEMPLATE_MODULE}-`date +%Y%m%d-%H%M%S-%N`"<br/>gcloud beta dataflow flex-template run ${JOB_NAME} \<br/>        --project=${PROJECT} --region=europe-west1 \<br/>        --template-file-gcs-location=${TEMPLATE_IMAGE_SPEC} \<br/>        --parameters ^~^outputTableSpec=${OUTPUT_TABLE}~inputTopics=${TOPICS}~javascriptTextTransformGcsPath=${JS_PATH}~javascriptTextTransformFunctionName=${JS_FUNC_NAME}~bootstrapServers=${BOOTSTRAP}</span></pre><blockquote class="ix iy iz"><p id="35e0" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">流式作业不需要像批处理作业那样定期触发。流式作业无限期运行，直到停止。例如，这个一次性运行命令可以从CI/CD系统中发出。</p></blockquote><h2 id="14c2" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">Web用户界面</h2><p id="75e9" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">如果您愿意，可以跳过上面的许多步骤，直接通过Google Cloud Console web UI启动相同的数据流模板。<br/>为此，导航至<a class="ae lj" href="https://console.cloud.google.com" rel="noopener ugc nofollow" target="_blank">谷歌云控制台</a>并打开数据流&gt;从模板创建工作。然后，在数据流模板下拉列表中选择“Kafka to BigQuery”。最后，您可以用我们在上面的说明中设置为环境变量的所有参数来填充表单输入字段。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es mr"><img src="../Images/35cfed45535baa464596775b32025446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X7DSujqgyV_4Lea5SpYHgw.png"/></div></div></figure><h2 id="2375" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">结果</h2><p id="c95a" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">我们的流管道现在应该正在运行。要检查是否是这种情况，请导航到<a class="ae lj" href="https://console.cloud.google.com" rel="noopener ugc nofollow" target="_blank">谷歌云控制台</a> &gt;数据流。您应该在列表中看到您的作业，它的状态设置为“正在运行”。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ms"><img src="../Images/cdc8a68dfdcd8c197368a75475f2c874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WKdDV7yKeFfYiYb5WkX14w.png"/></div></div></figure><p id="d001" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">几秒钟后，数据流应该开始从你的Kafka主题中读取，你应该在BigQuery中看到结果。<br/>因为表是在<code class="du lm ln lo lp b">transaction_time</code>字段上分区的，所以确保查询了<code class="du lm ln lo lp b">2020-07-20</code>分区(这是我们在上面的<code class="du lm ln lo lp b">message.json</code>中设置的<code class="du lm ln lo lp b">transaction_time</code>值)。</p><blockquote class="ix iy iz"><p id="f6bc" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">注意，流入分区表<a class="ae lj" href="https://cloud.google.com/bigquery/streaming-data-into-bigquery#streaming_into_partitioned_tables" rel="noopener ugc nofollow" target="_blank">有一些限制</a>(截至2020年7月)。</p></blockquote><p id="c811" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">下面是检索结果的SQL命令。请随意尝试，但不要忘记将<code class="du lm ln lo lp b">&lt;my-project&gt;</code>替换为您的GCP项目ID:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="a956" class="lr kk hi lp b fi mj mk l ml mm">SELECT * FROM `&lt;my-project&gt;.kafka_to_bigquery.transactions` WHERE DATE(transaction_time) = “2020–07–20”</span></pre><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es mt"><img src="../Images/0be38abc7210798b4fb5a0de67e28ddc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sb19zVV1SFmq1hBD9wQASQ.png"/></div></div></figure><p id="9133" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">如果您试图向Kafka主题发送格式错误的消息，作业应该将死信输入保存到另一个表中。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div class="er es mu"><img src="../Images/14118cc7eccda68fbb98b15a7b5f0f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*c93rdwhZU3gXk5tUGyjjhA.png"/></div></figure><p id="13c9" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">恭喜你！您已经完成了本文的第一个方法。我们的卡夫卡信息现在被自动发送到BigQuery。<br/>有了Dataflow，还有另外一种方法可以将Kafka消息发送到BigQuery，下面我们将探讨这第二种方法。</p><h1 id="1277" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">方法2:使用发布/订阅和数据流SQL</h1><p id="46bd" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">我们要尝试的第二种方法是将我们的Kafka消息发送到<a class="ae lj" href="https://cloud.google.com/pubsub" rel="noopener ugc nofollow" target="_blank"> Cloud Pub/Sub </a>，然后直接从BigQuery查询Pub/Sub主题！我们的两种方法有一些共同的特点。方法2也使用在数据流上运行的Apache Beam。但是这一次，管道是用SQL编写的，使用带有<a class="ae lj" href="https://github.com/google/zetasql" rel="noopener ugc nofollow" target="_blank"> ZetaSQL </a>方言的<a class="ae lj" href="https://beam.apache.org/documentation/dsls/sql/overview/" rel="noopener ugc nofollow" target="_blank"> Beam SQL </a>。<br/>因此，当您运行一个数据流SQL查询时，数据流会将查询转换为Apache Beam管道并执行管道。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es mp"><img src="../Images/889b6cf9c27c28cf8fe2588fb3c6132f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lw6RVS5MsVutRDWtZYgvSA.png"/></div></div></figure><p id="4895" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">例如，希望采用<a class="ae lj" href="https://en.wikipedia.org/wiki/Cloud_computing#Hybrid_cloud" rel="noopener ugc nofollow" target="_blank">混合云架构</a>的组织可以在其内部环境中运行Kafka，并使用云发布/订阅来制作和消费GCP上的活动。</p><h2 id="5c69" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">开始之前</h2><p id="4404" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">如果您还没有这样做，请遵循本文第一部分中的环境设置。一旦完成，你应该:<br/> -部署一个Kafka VM <br/> -创建一个BigQuery表<br/> -创建一个Kafka主题<br/> -并向你的主题发送一条Kafka消息。</p><h2 id="edf9" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">创建发布/订阅主题</h2><p id="6117" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">导航到<a class="ae lj" href="https://console.cloud.google.com" rel="noopener ugc nofollow" target="_blank">谷歌云控制台</a>并打开发布/订阅面板。然后，创建一个名为<code class="du lm ln lo lp b">txtopic</code>的主题。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es mv"><img src="../Images/bc68bb696424f32574a7d91f6aeb0395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IxySAmt1DOMaTx5O_5qQJA.png"/></div></div></figure><h2 id="7507" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">创建服务帐户</h2><p id="b4b9" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">遵循自述文件中的“运行前步骤”，如下:<a class="ae lj" href="https://github.com/GoogleCloudPlatform/pubsub/tree/master/kafka-connector" rel="noopener ugc nofollow" target="_blank">https://github . com/Google cloud platform/pubsub/tree/master/Kafka-connector</a></p><p id="a007" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">这些步骤将指导您创建一个服务帐户，并为其分配<strong class="jd hj">发布/订阅管理</strong>角色。<br/>您必须为这个服务帐户创建一个JSON密钥，并将其上传到Kafka VM(SSH窗口的右上角&gt;上传文件)。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es mw"><img src="../Images/e5b3276a4231c8a02e3ab202db94921e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cSpu27ih1JlDrW2y2bIDEQ.png"/></div></div></figure><p id="c029" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">最后，在Kafka VM上设置以下环境变量(通过SSH终端)。更换下面的<code class="du lm ln lo lp b">/path/to/key/file</code>:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="f3a3" class="lr kk hi lp b fi mj mk l ml mm">export GOOGLE_APPLICATION_CREDENTIALS=/path/to/key/file</span></pre><h2 id="7d86" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">Kafka连接设置</h2><p id="90a4" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">我们将使用<a class="ae lj" href="http://kafka.apache.org/documentation.html#connect" rel="noopener ugc nofollow" target="_blank"> Kafka Connect </a>在Kafka和Cloud Pub/Sub之间同步消息。</p><p id="4ff1" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">在Kafka虚拟机上(通过SSH)，运行以下命令来安装云发布/订阅连接器:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="ade5" class="lr kk hi lp b fi mj mk l ml mm">sudo apt install git-all<br/>sudo apt install default-jdk<br/>sudo apt install maven</span><span id="3f93" class="lr kk hi lp b fi mq mk l ml mm">git clone <a class="ae lj" href="https://github.com/GoogleCloudPlatform/pubsub" rel="noopener ugc nofollow" target="_blank">https://github.com/GoogleCloudPlatform/pubsub</a><br/>cd pubsub/kafka-connector/<br/>mvn package</span><span id="8924" class="lr kk hi lp b fi mq mk l ml mm">sudo mkdir /opt/kafka/connectors<br/>sudo cp target/cps-kafka-connector.jar /opt/kafka/connectors/<br/>sudo cp config/cps-sink-connector.properties /opt/kafka/config/</span></pre><p id="a1e1" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">在同一个SSH终端中，编辑刚刚复制到<code class="du lm ln lo lp b">/opt/kafka/config/</code>目录中的<code class="du lm ln lo lp b">cps-sink-connector.properties</code>文件。</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="b601" class="lr kk hi lp b fi mj mk l ml mm">sudo vim /opt/kafka/config/cps-sink-connector.properties</span></pre><p id="6d92" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">确保编辑Kafka和Pub/Sub主题名称，如下所示。在这个例子中，我们为两者取了相同的名字:<code class="du lm ln lo lp b">txtopic</code>💡<br/>下面的代码片段显示了文件应该是什么样子。用您的GCP项目ID替换下面的<code class="du lm ln lo lp b">&lt;my-project&gt;</code>。</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="6c3e" class="lr kk hi lp b fi mj mk l ml mm">name=CPSSinkConnector<br/>connector.class=com.google.pubsub.kafka.sink.CloudPubSubSinkConnector<br/>tasks.max=10<br/>topics=txtopic<br/>cps.topic=txtopic<br/>cps.project=<!-- -->&lt;my-project&gt;</span></pre><p id="bf98" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">我们还需要编辑一个文件来完成Kafka Connect设置:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="43ee" class="lr kk hi lp b fi mj mk l ml mm">sudo vim /opt/kafka/config/connect-standalone.properties</span></pre><p id="7ebe" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">我们需要取消注释并编辑属性<code class="du lm ln lo lp b">plugin.path</code>，以便Kafka Connect可以定位我们之前打包并复制的JAR文件<code class="du lm ln lo lp b">cps-kafka-connector.jar</code>:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="4d97" class="lr kk hi lp b fi mj mk l ml mm">plugin.path=/opt/kafka/connectors</span></pre><p id="cf52" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">仍然编辑<code class="du lm ln lo lp b">connect-standalone.properties</code>文件，我们需要设置以下属性:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="2813" class="lr kk hi lp b fi mj mk l ml mm">key.converter=org.apache.kafka.connect.storage.StringConverter<br/>value.converter=org.apache.kafka.connect.storage.StringConverter</span><span id="71e8" class="lr kk hi lp b fi mq mk l ml mm">key.converter.schemas.enable=false<br/>value.converter.schemas.enable=false</span></pre><ul class=""><li id="33f3" class="mx my hi jd b je jf jh ji ld mz lf na lh nb jw nc nd ne nf bi translated">通过使用<code class="du lm ln lo lp b">StringConverter</code>，我们告诉连接器不要试图解释数据，而是直接将JSON转发到Cloud Pub/Sub。</li><li id="e1d5" class="mx my hi jd b je ng jh nh ld ni lf nj lh nk jw nc nd ne nf bi translated">我们还告诉Kafka Connect不要寻找特定的模式<a class="ae lj" href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas" rel="noopener ugc nofollow" target="_blank">，如这里所解释的</a>。</li></ul><h2 id="91a4" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">运行Kafka Connect</h2><p id="791e" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">您可以使用以下命令运行Kafka Connect。注意<code class="du lm ln lo lp b">&amp;</code>符号指示命令在后台进程中运行。</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="bda0" class="lr kk hi lp b fi mj mk l ml mm">/opt/kafka/bin/connect-standalone.sh /opt/kafka/config/connect-standalone.properties /opt/kafka/config/cps-sink-connector.properties &amp;</span></pre><p id="502b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated"><a class="ae lj" href="https://docs.confluent.io/current/connect/references/restapi.html#connectors" rel="noopener ugc nofollow" target="_blank">如这里所解释的</a>，Kafka Connect旨在作为服务运行，并支持用于管理连接器的REST API。<br/>您可以使用以下命令检查我们的发布/订阅连接器的当前状态:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="9be8" class="lr kk hi lp b fi mj mk l ml mm">curl localhost:8083/connectors/CPSSinkConnector/status | jq</span></pre><h2 id="566c" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">云发布/订阅</h2><p id="5edf" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">在这个阶段，我们的Kafka消息现在应该被转发到Cloud Pub/Sub。让我们检查一下！</p><ul class=""><li id="4166" class="mx my hi jd b je jf jh ji ld mz lf na lh nb jw nc nd ne nf bi translated">创建发布/订阅订阅以查看消息。<br/>导航至<a class="ae lj" href="https://console.cloud.google.com" rel="noopener ugc nofollow" target="_blank">谷歌云控制台</a>并打开发布/订阅&gt;订阅&gt;创建订阅。</li><li id="6958" class="mx my hi jd b je ng jh nh ld ni lf nj lh nk jw nc nd ne nf bi translated">创建订阅后，打开订阅面板，单击您的订阅，然后单击“查看消息”。单击“拉”后，您应该会看到您的消息。如果没有，尝试使用上面环境设置部分的<code class="du lm ln lo lp b">kafka-console-producer.sh</code>命令发送另一个Kafka消息。</li></ul><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es nl"><img src="../Images/a6075244bc52e281aa11e847d94e8d5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dxh5TDccWwZ6TyPgYFUL0A.png"/></div></div></figure><h2 id="b88e" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">发布/订阅到大查询模板</h2><p id="c1f3" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">如果您在Pub/Sub web UI中四处点击，您可能会看到可以轻松地将Pub/Sub主题导出到BigQuery。<br/>此导出作业使用类似于方法1的数据流模板。但是这个特定的模板没有使用KafkaIO，而是使用了<a class="ae lj" href="https://beam.apache.org/releases/javadoc/2.19.0/org/apache/beam/sdk/io/gcp/pubsub/PubsubIO.html" rel="noopener ugc nofollow" target="_blank"> PubsubIO </a> Beam connector。我们不会在本文中讨论这个特定的模板。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es nm"><img src="../Images/ffcc1636969121e3f011dac930aea0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eACbVHV1aKghhsw6sVsUEw.png"/></div></div></figure><h2 id="f0e5" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">BigQuery数据流引擎</h2><p id="6281" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">为了将我们的发布/订阅消息接收到BigQuery，我们将使用数据流SQL。</p><p id="4a58" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated"><a class="ae lj" href="https://cloud.google.com/dataflow/docs/reference/sql/query-syntax" rel="noopener ugc nofollow" target="_blank">数据流SQL查询语法</a>类似于<a class="ae lj" href="https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax" rel="noopener ugc nofollow" target="_blank"> BigQuery标准SQL </a>。您还可以使用<a class="ae lj" href="https://cloud.google.com/dataflow/docs/reference/sql/streaming-extensions" rel="noopener ugc nofollow" target="_blank">data flow SQL streaming extensions</a>从持续更新的数据源(如Pub/Sub)聚合数据。</p><p id="11fb" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">导航到<a class="ae lj" href="https://console.cloud.google.com" rel="noopener ugc nofollow" target="_blank">谷歌云控制台</a>并打开BigQuery。如下截图所示，点击“更多”&gt;查询设置，选择“云数据流引擎”作为查询引擎。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es nn"><img src="../Images/bdf0b6025e250d7650c127641f8a9526.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*kxqNo2XHaBmBWh9axPjmkw.png"/></div></div></figure><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es no"><img src="../Images/19d0df1f7255d1d12a3ef8654bbee306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a8BPxNPMNrMm3D-9j5eUOA.png"/></div></div></figure><p id="4867" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">选择云数据流引擎后，单击“添加数据”&gt;云数据流源。选择您的GCP项目和我们之前创建的发布/订阅主题<code class="du lm ln lo lp b">txtopic</code>。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div class="er es np"><img src="../Images/115d0cb9a700f652abfd078c29652fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*CyllatX619cHdSr2t1Ebvw.png"/></div></figure><h2 id="461e" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">发布/子主题模式</h2><p id="dbc2" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">既然我们已经添加了我们的发布/订阅主题作为源，我们需要给它分配一个模式，正如这里所解释的那样。点击“编辑模式”，逐个添加字段类型，如下图截图所示。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es nq"><img src="../Images/2be26f2ee1f5ea4b11906018a0d95729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eh84Kmv5RpodzxkQqep8PQ.png"/></div></div></figure><p id="6228" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">❗️This模式与我们的BigQuery表<code class="du lm ln lo lp b">schema.json</code>有些不同:</p><ul class=""><li id="36eb" class="mx my hi jd b je jf jh ji ld mz lf na lh nb jw nc nd ne nf bi translated">缺少<code class="du lm ln lo lp b">city</code>字段。我们不打算使用JavaScript UDF来解决这个问题(如方法1所示)。相反，我们稍后将直接在SQL中添加这个字段。</li><li id="f02c" class="mx my hi jd b je ng jh nh ld ni lf nj lh nk jw nc nd ne nf bi translated"><code class="du lm ln lo lp b">products</code>字段的类型是<code class="du lm ln lo lp b">STRUCT</code>。尚不支持类型<code class="du lm ln lo lp b">RECORD</code>。<a class="ae lj" href="https://cloud.google.com/dataflow/docs/reference/sql/data-types" rel="noopener ugc nofollow" target="_blank">参见支持的数据类型</a>。</li><li id="d4ab" class="mx my hi jd b je ng jh nh ld ni lf nj lh nk jw nc nd ne nf bi translated"><code class="du lm ln lo lp b">transaction_time</code>字段的类型为<code class="du lm ln lo lp b">STRING</code>。</li><li id="5057" class="mx my hi jd b je ng jh nh ld ni lf nj lh nk jw nc nd ne nf bi translated">还添加了一个<code class="du lm ln lo lp b">event_timestamp</code>字段。该字段由发布/订阅自动添加，您无需修改我们的<code class="du lm ln lo lp b">message.json</code>。</li></ul><h2 id="d4d6" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">运行数据流SQL作业</h2><p id="0da9" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">我们准备运行我们的数据流SQL作业。在SQL编辑器中，输入以下SQL代码:</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="fa66" class="lr kk hi lp b fi mj mk l ml mm">SELECT *,"New York" as city FROM pubsub.topic.&lt;my-project&gt;.txtopic</span></pre><p id="b170" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">用您的GCP项目ID替换上面SQL中的<code class="du lm ln lo lp b">&lt;my-project&gt;</code>。注意，我们在这里添加了<code class="du lm ln lo lp b">city</code>字段。然后点击“创建云数据流作业”。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div class="er es nr"><img src="../Images/a4a3cae4ad6660affcebbe6818256697.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*NFzHrqN5q1NCa-wZg_TYmA.png"/></div></figure><p id="ea3e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">在打开的滑动面板中，选择一个区域端点，例如在<strong class="jd hj"> Europe </strong>(与本文中创建的其他服务保持一致)。并输入以下参数:</p><ul class=""><li id="80c6" class="mx my hi jd b je jf jh ji ld mz lf na lh nb jw nc nd ne nf bi translated">输出类型:<code class="du lm ln lo lp b">BigQuery</code></li><li id="a2ca" class="mx my hi jd b je ng jh nh ld ni lf nj lh nk jw nc nd ne nf bi translated">数据集ID: <code class="du lm ln lo lp b">kafka_to_bigquery</code>。让我们重用之前创建的数据集。</li><li id="50c3" class="mx my hi jd b je ng jh nh ld ni lf nj lh nk jw nc nd ne nf bi translated">表名:<code class="du lm ln lo lp b">transactions2</code>。让我们创建一个新表。请注意，该表没有分区，将由第一条发布/订阅消息创建。理论上，应该可以重用为方法1创建的BigQuery表<code class="du lm ln lo lp b">transactions</code>。但是你必须添加<code class="du lm ln lo lp b">event_timestamp</code>一栏。也许在新字段上对表进行分区是个好主意？</li></ul><p id="4f2a" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">保留其他所有内容的默认设置，然后单击“创建”。</p><p id="7cc2" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">您的流式作业现在应该正在运行。为了确保它确实在运行，导航到<a class="ae lj" href="https://console.cloud.google.com" rel="noopener ugc nofollow" target="_blank">谷歌云控制台</a>并打开BigQuery &gt;作业历史&gt;云数据流。您应该看到一个流作业正在运行，您的BigQuery表<code class="du lm ln lo lp b">transactions2</code>应该开始被填充。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ns"><img src="../Images/33a42825a98c5c6b466c39a427ffe689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AU_aml6brnkEHb9WrjiihQ.png"/></div></div></figure><p id="aa59" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">您可以尝试使用<code class="du lm ln lo lp b">kafka-console-producer.sh</code>命令发送更多的Kafka消息(参见上面的环境设置部分)。然后，检查新消息是否最终进入了<code class="du lm ln lo lp b">transactions2</code>表。</p><p id="7816" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">要查询BigQuery表，不要忘记将查询设置切换回“BigQuery engine ”,并输入以下SQL(用您的GCP项目ID替换<code class="du lm ln lo lp b">&lt;my-project&gt;</code>):</p><pre class="jy jz ka kb fd mf lp mg mh aw mi bi"><span id="f0ec" class="lr kk hi lp b fi mj mk l ml mm">SELECT * FROM `&lt;my-project&gt;.kafka_to_bigquery.transactions2`</span></pre><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es nt"><img src="../Images/32e987e5e89000f745ef72abe9cd1f5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CkPyqrs4toZhKPHsruTtFA.png"/></div></div></figure><p id="3eb4" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">我们的方法2到此结束！我们使用Pub/Sub和Dataflow SQL成功地将Kafka消息发送到BigQuery。</p><h1 id="73f8" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">你应该使用哪种方法？</h1><p id="8241" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">谷歌云通常有多个产品可以用来实现相同的目标。例如，如果您希望针对快速应用程序开发、可伸缩性或成本进行优化，您可能会选择不同的途径……<br/>要评估将Kafka导入BigQuery的不同方法，以下是建议的标准列表:</p><p id="edf0" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated"><strong class="jd hj">🔨维护</strong>:您需要供应和维护底层基础设施吗？<br/> ✅ <strong class="jd hj">更新</strong>:你能轻松<a class="ae lj" href="https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline" rel="noopener ugc nofollow" target="_blank">更新正在进行的流媒体作业</a>吗？<br/>🔀<strong class="jd hj">转换</strong>:解决方案是否支持<a class="ae lj" href="https://en.wikipedia.org/wiki/Extract,_transform,_load" rel="noopener ugc nofollow" target="_blank"> ETL </a>，在数据进入BigQuery之前转换数据？<br/>T13】🍻协作:管道可以进行版本控制吗？<br/> <strong class="jd hj">🏁快速应用开发</strong>:你能多快开发出一个最小可行的产品？<br/>T19】💡所需技能:你需要了解一门编程语言吗？<br/> <strong class="jd hj">🚀可扩展性</strong>:如果你开始每秒推送数百万条Kafka消息会怎样？数据流支持<a class="ae lj" href="https://cloud.google.com/dataflow/docs/resources/faq#streaming_autoscaling" rel="noopener ugc nofollow" target="_blank">流式自动缩放</a>。<a class="ae lj" href="https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-engine" rel="noopener ugc nofollow" target="_blank">数据流引擎</a>和<a class="ae lj" href="https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#dataflow-shuffle" rel="noopener ugc nofollow" target="_blank">数据流混洗</a>提供了一个响应更快的自动缩放。<br/> <strong class="jd hj">💃灵活性</strong>:能否对其进行调整以支持不同的场景，例如有状态处理、多输入/输出或令牌化，例如使用<a class="ae lj" href="https://github.com/GoogleCloudPlatform/dlp-dataflow-deidentification" rel="noopener ugc nofollow" target="_blank">云DLP </a>？<br/> <strong class="jd hj">💰价格</strong>:这个解决方案要花多少钱？数据流服务的使用以每秒的增量计费。⛔️死信:它能多好地处理错误？<br/> <strong class="jd hj">🍰分区</strong>:管道可以基于时间或范围创建不同的<a class="ae lj" href="https://cloud.google.com/bigquery/docs/partitioned-tables" rel="noopener ugc nofollow" target="_blank"> BigQuery分区</a>吗？<br/>🔓<strong class="jd hj">可移植性</strong>:解决方案是否可以迁移到另一个云或者在本地运行，并且支持不同的接收器，只需做最小的改动？Beam是开源的，有很多<a class="ae lj" href="https://beam.apache.org/documentation/io/built-in/" rel="noopener ugc nofollow" target="_blank">连接器</a>。</p><blockquote class="ix iy iz"><p id="79d1" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">上面的列表显然并不详尽，您可以在评估中考虑更多的标准，例如:对模式注册的支持、模式演变、编排、度量、<a class="ae lj" href="https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring" rel="noopener ugc nofollow" target="_blank">监控</a>、<a class="ae lj" href="https://beam.apache.org/documentation/pipelines/test-your-pipeline/" rel="noopener ugc nofollow" target="_blank">测试</a>、吞吐量、延迟、<a class="ae lj" href="https://beam.apache.org/documentation/programming-guide/#windowing" rel="noopener ugc nofollow" target="_blank">窗口</a>、排序、加入主题、灾难恢复等。</p></blockquote><h1 id="3d5d" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">其他支持的方法</h1><p id="2cc6" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">数据流并不是将Kafka消息发送到BigQuery的唯一方式，还有许多其他方法，各有利弊。</p><p id="21ed" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">这里列出了另外三种受支持的方法:</p><h2 id="f9df" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">方法3</h2><p id="52a1" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated"><a class="ae lj" href="https://cloud.google.com/data-fusion" rel="noopener ugc nofollow" target="_blank">云数据融合</a>是一个完全托管的、无代码的数据集成服务，帮助用户高效地构建和管理ETL/ELT数据管道。它采用开源内核(<a class="ae lj" href="https://cdap.io/" rel="noopener ugc nofollow" target="_blank"> CDAP </a>)来实现管道移植。</p><p id="1ba0" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">GitHub链接到Kafka插件:<a class="ae lj" href="https://github.com/data-integrations/kafka-plugins" rel="noopener ugc nofollow" target="_blank">https://github.com/data-integrations/kafka-plugins</a></p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es nu"><img src="../Images/ceb681d672841a8bbf13099098a490bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Eq5UvOXjVjt0zKhHriQVyw.png"/></div></div></figure><h2 id="e8ce" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">方法4</h2><p id="7860" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated"><a class="ae lj" href="https://fivetran.com" rel="noopener ugc nofollow" target="_blank"> Fivetran </a>提供基于云的零维护ETL/ELT数据管道，可以将你所有的Apache Kafka原始数据加载到Google BigQuery中，并不断更新。<br/> Fivetran支持<a class="ae lj" href="https://fivetran.com/directory" rel="noopener ugc nofollow" target="_blank"> 150+连接器</a>，可以自动适应模式和API的变化。如果Kafka不是您唯一的数据源，并且您有10、20、……100或更多的管道要构建，以便将来自不同来源的数据接收到BigQuery中，那么这种方法就特别有意思。在<a class="ae lj" href="https://console.cloud.google.com/marketplace/" rel="noopener ugc nofollow" target="_blank">谷歌云市场</a>中寻找“fivetran”。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es nv"><img src="../Images/29850a5686e9e0a6c5af75cba4b1df8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cicnGeEvTBeLFZw3Vh8ckw.png"/></div></div></figure><h2 id="5c58" class="lr kk hi bd kl ls lt lu kp lv lw lx kt ld ly lz kv lf ma mb kx lh mc md kz me bi translated">方法5</h2><p id="2fef" class="pw-post-body-paragraph ja jb hi jd b je lb ij jg jh lc im jj ld le jm jn lf lg jq jr lh li ju jv jw hb bi translated">由最初创建Apache Kafka的团队创建的Confluent提供了一个连接器，用于<a class="ae lj" href="https://docs.confluent.io/current/cloud/connectors/cc-gcp-bigquery-sink.html#" rel="noopener ugc nofollow" target="_blank">向BigQuery </a>发送Kafka消息。</p><p id="c739" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">如果您的组织也在寻找一个强大的、基于云的、完全托管的Kafka即服务选项，这种方法是无可匹敌的。<br/> <a class="ae lj" href="https://www.confluent.io/gcp/" rel="noopener ugc nofollow" target="_blank">融合云</a>提供了一个简单、可扩展、灵活且安全的事件流平台，因此您的组织可以专注于构建应用，而不是管理Kafka集群。</p><p id="2970" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">Confluent构建了一个完整的流媒体平台，包括:</p><ul class=""><li id="f695" class="mx my hi jd b je jf jh ji ld mz lf na lh nb jw nc nd ne nf bi translated">托管连接器</li><li id="60db" class="mx my hi jd b je ng jh nh ld ni lf nj lh nk jw nc nd ne nf bi translated"><a class="ae lj" href="https://www.confluent.io/product/ksql/" rel="noopener ugc nofollow" target="_blank"> KSQL </a>用于实时事件处理</li><li id="dfdc" class="mx my hi jd b je ng jh nh ld ni lf nj lh nk jw nc nd ne nf bi translated">确保数据兼容性的托管模式注册表</li><li id="8216" class="mx my hi jd b je ng jh nh ld ni lf nj lh nk jw nc nd ne nf bi translated">融合复制器可以将主题从一个Kafka集群复制到另一个集群。例如，该产品可以帮助支持<a class="ae lj" href="https://en.wikipedia.org/wiki/Cloud_computing#Hybrid_cloud" rel="noopener ugc nofollow" target="_blank">混合</a>或<a class="ae lj" href="https://en.wikipedia.org/wiki/Cloud_computing#Multicloud" rel="noopener ugc nofollow" target="_blank">多云</a>场景。</li><li id="9962" class="mx my hi jd b je ng jh nh ld ni lf nj lh nk jw nc nd ne nf bi translated">还有更多功能…阅读更多关于<a class="ae lj" href="https://www.confluent.io/project-metamorphosis" rel="noopener ugc nofollow" target="_blank">项目变形</a>的内容。</li></ul><p id="88a4" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">在<a class="ae lj" href="https://console.cloud.google.com/marketplace/" rel="noopener ugc nofollow" target="_blank">谷歌云市场</a>中寻找“汇合”开始吧。</p><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es nw"><img src="../Images/902be1876b21793942073e74fff72944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1h9m-ySfxej8fW2mlueWhg.png"/></div></div></figure><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es nx"><img src="../Images/9fd66b41f4ef2eff9d45b6935cb7160e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SABnYfT-DtAuwGOgkqs79g.jpeg"/></div></div><figcaption class="ny nz et er es oa ob bd b be z dx translated"><a class="ae lj" href="https://unsplash.com/@sortino?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Joshua Sortino </a>在<a class="ae lj" href="https://unsplash.com/s/photos/speed-data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="e1b8" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj ld jl jm jn lf jp jq jr lh jt ju jv jw hb bi translated">感谢阅读！<br/>随时在推特上联系<a class="ae lj" href="https://twitter.com/tdelazzari" rel="noopener ugc nofollow" target="_blank"> @tdelazzari </a></p></div></div>    
</body>
</html>