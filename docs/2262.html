<html>
<head>
<title>Processing data from Hive to BigQuery using PySpark and Dataproc Serverless</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PySpark和Dataproc无服务器处理从Hive到BigQuery的数据</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/processing-data-from-hive-to-bigquery-using-pyspark-and-dataproc-serverless-217c7cb9e4f8?source=collection_archive---------1-----------------------#2022-07-14">https://medium.com/google-cloud/processing-data-from-hive-to-bigquery-using-pyspark-and-dataproc-serverless-217c7cb9e4f8?source=collection_archive---------1-----------------------#2022-07-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="b2d3" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/c1b94e02a76138a164a90ec60264e5bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*MEPOQoB8pW_vqPBy"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">Google Cloud Dataproc徽标</figcaption></figure><h2 id="a792" class="jp ig hi bd ih jq jr js il jt ju jv ip jw jx jy it jz ka kb ix kc kd ke jb kf bi translated">目标</h2><p id="9ed1" class="pw-post-body-paragraph kg kh hi ki b kj kk kl km kn ko kp kq jw kr ks kt jz ku kv kw kc kx ky kz la hb bi translated">这篇博文解释了如何使用PySpark、Google Cloud上的Dataproc Serverless和Dataproc模板运行批处理工作负载，以处理从Apache Hive表到BigQuery表的数据。</p><p id="ffdf" class="pw-post-body-paragraph kg kh hi ki b kj lb kl km kn lc kp kq jw ld ks kt jz le kv kw kc lf ky kz la hb bi translated">两个使用案例涵盖如下:</p><ol class=""><li id="d6cd" class="lg lh hi ki b kj lb kn lc jw li jz lj kc lk la ll lm ln lo bi translated">将您的Hive表和数据从Hadoop环境迁移到Google Cloud。</li><li id="f366" class="lg lh hi ki b kj lp kn lq jw lr jz ls kc lt la ll lm ln lo bi translated">同时处理来自包括Apache Hive和BigQuery在内的数据源的数据。</li></ol><h2 id="7934" class="jp ig hi bd ih jq jr js il jt ju jv ip jw jx jy it jz ka kb ix kc kd ke jb kf bi translated">Dataproc无服务器</h2><p id="4492" class="pw-post-body-paragraph kg kh hi ki b kj kk kl km kn ko kp kq jw kr ks kt jz ku kv kw kc kx ky kz la hb bi translated">正如<a class="ae lu" href="https://cloud.google.com/blog/products/data-analytics/spark-jobs-that-autoscale-and-made-seamless-for-all-data-users" rel="noopener ugc nofollow" target="_blank">在2021年末宣布的</a>，Dataproc Serverless让我们能够<a class="ae lu" href="https://cloud.google.com/hadoop-spark-migration" rel="noopener ugc nofollow" target="_blank">在Google Cloud </a>上运行Spark工作负载，而不必担心集群管理和调优。</p><h2 id="041b" class="jp ig hi bd ih jq jr js il jt ju jv ip jw jx jy it jz ka kb ix kc kd ke jb kf bi translated">Dataproc模板</h2><p id="c1c5" class="pw-post-body-paragraph kg kh hi ki b kj kk kl km kn ko kp kq jw kr ks kt jz ku kv kw kc kx ky kz la hb bi translated"><a class="ae lu" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> Dataproc模板</a>允许我们运行涵盖Spark工作负载常见用例的预建模板，并根据特定需求定制它们。他们使用Dataproc Serverless作为运行这些工作负载的执行环境。模板目前有Java和Python两种版本。</p><p id="e816" class="pw-post-body-paragraph kg kh hi ki b kj lb kl km kn lc kp kq jw ld ks kt jz le kv kw kc lf ky kz la hb bi translated"><strong class="ki hj">在本帖</strong>中，我们将讨论如何使用PySpark <strong class="ki hj"> Hive来BigQuery模板</strong>。</p><p id="963f" class="pw-post-body-paragraph kg kh hi ki b kj lb kl km kn lc kp kq jw ld ks kt jz le kv kw kc lf ky kz la hb bi translated">花时间阅读这篇文章<a class="ae lu" rel="noopener" href="/@ppaglilla/getting-started-with-dataproc-serverless-pyspark-templates-e32278a6a06e">也是一个好主意，这篇文章</a>是开始使用来自另一个PySpark用例的Dataproc模板的好参考:<a class="ae lu" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/python/dataproc_templates/gcs/README.md#gcs-to-bigquery" rel="noopener ugc nofollow" target="_blank"><strong class="ki hj">GCS to big query</strong></a></p><h2 id="74b2" class="jp ig hi bd ih jq jr js il jt ju jv ip jw jx jy it jz ka kb ix kc kd ke jb kf bi translated">配置单元到BigQuery</h2><p id="d5d6" class="pw-post-body-paragraph kg kh hi ki b kj kk kl km kn ko kp kq jw kr ks kt jz ku kv kw kc kx ky kz la hb bi translated">执行时，<a class="ae lu" href="https://github.com/GoogleCloudPlatform/dataproc-templates/tree/main/python/dataproc_templates/hive" rel="noopener ugc nofollow" target="_blank"> Hive to BigQuery </a>模板将从您的<a class="ae lu" href="https://hive.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Hive </a>表中读取数据，并将其写入<a class="ae lu" href="https://cloud.google.com/bigquery" rel="noopener ugc nofollow" target="_blank"> BigQuery </a>表。</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h1 id="5695" class="if ig hi bd ih ii mc ik il im md io ip iq me is it iu mf iw ix iy mg ja jb jc bi translated">先决条件</h1><ul class=""><li id="4138" class="lg lh hi ki b kj kk kn ko jw mh jz mi kc mj la mk lm ln lo bi translated"><a class="ae lu" href="https://cloud.google.com/sdk/docs/install" rel="noopener ugc nofollow" target="_blank"> Google Cloud SDK </a>安装并认证。你可以在Google Cloud控制台中使用Cloud Shell，通过这个<a class="ae lu" href="https://console.cloud.google.com/cloudshell/editor" rel="noopener ugc nofollow" target="_blank">链接</a>来配置一个环境。</li><li id="2f0a" class="lg lh hi ki b kj lp kn lq jw lr jz ls kc lt la mk lm ln lo bi translated">Python 3.7+已安装并添加到PATH变量中。</li><li id="a651" class="lg lh hi ki b kj lp kn lq jw lr jz ls kc lt la mk lm ln lo bi translated">启用了专用Google访问的VPC子网。只要启用了私有Google访问，默认子网可能就足够了。您可以在这个<a class="ae lu" href="https://cloud.google.com/dataproc-serverless/docs/concepts/network" rel="noopener ugc nofollow" target="_blank">链接</a>查看Dataproc无服务器网络需求。</li></ul><pre class="je jf jg jh fd ml mm mn mo aw mp bi"><span id="5d80" class="jp ig hi mm b fi mq mr l ms mt"># Example updating default network to enable Private Google Access<br/>gcloud compute networks subnets update default — region=us-central1 — enable-private-ip-google-access</span></pre></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h1 id="21b7" class="if ig hi bd ih ii mc ik il im md io ip iq me is it iu mf iw ix iy mg ja jb jc bi translated">使用</h1><p id="b361" class="pw-post-body-paragraph kg kh hi ki b kj kk kl km kn ko kp kq jw kr ks kt jz ku kv kw kc kx ky kz la hb bi translated">在Dataproc Serverless中提交py spark作业时，最好查看可能的<a class="ae lu" href="https://cloud.google.com/sdk/gcloud/reference/dataproc/batches/submit/pyspark" rel="noopener ugc nofollow" target="_blank">参数的文档。</a></p><p id="000d" class="pw-post-body-paragraph kg kh hi ki b kj lb kl km kn lc kp kq jw ld ks kt jz le kv kw kc lf ky kz la hb bi translated">注意，我们将把<a class="ae lu" href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector" rel="noopener ugc nofollow" target="_blank"> Spark BigQuery连接器</a>作为作业提交中的. jar依赖项传递，以允许Spark与BigQuery连接。</p><p id="bd1e" class="pw-post-body-paragraph kg kh hi ki b kj lb kl km kn lc kp kq jw ld ks kt jz le kv kw kc lf ky kz la hb bi translated">此处的所有代码片段都获得以下许可:</p><blockquote class="mu mv mw"><p id="6a39" class="kg kh mx ki b kj lb kl km kn lc kp kq my ld ks kt mz le kv kw na lf ky kz la hb bi translated"><em class="hi">版权所有2022谷歌公司。</em></p><p id="424f" class="kg kh mx ki b kj lb kl km kn lc kp kq my ld ks kt mz le kv kw na lf ky kz la hb bi translated"><em class="hi">SPDX-许可证-标识符:Apache-2.0 </em></p></blockquote><h2 id="7e0e" class="jp ig hi bd ih jq jr js il jt ju jv ip jw jx jy it jz ka kb ix kc kd ke jb kf bi translated">步伐</h2><ol class=""><li id="69d4" class="lg lh hi ki b kj kk kn ko jw mh jz mi kc mj la ll lm ln lo bi translated">首先，在GCS中创建一个staging bucket，在Dataproc Serverless中提交一个PySpark作业，以便它存储我们的工作负载的依赖项。我们将使用<strong class="ki hj"> gsutil mb </strong>命令创建一个存储桶:</li></ol><pre class="je jf jg jh fd ml mm mn mo aw mp bi"><span id="7e53" class="jp ig hi mm b fi mq mr l ms mt">export GCS_STAGING_LOCATION="gcs-staging-bucket-folder"gsutil mb<br/>gs://$GCS_STAGING_LOCATION</span></pre><p id="e311" class="pw-post-body-paragraph kg kh hi ki b kj lb kl km kn lc kp kq jw ld ks kt jz le kv kw kc lf ky kz la hb bi translated"><strong class="ki hj"> 2。</strong>克隆Dataproc模板库，进入python文件夹:</p><pre class="je jf jg jh fd ml mm mn mo aw mp bi"><span id="74a2" class="jp ig hi mm b fi mq mr l ms mt">git clone <a class="ae lu" href="https://github.com/GoogleCloudPlatform/dataproc-templates.git" rel="noopener ugc nofollow" target="_blank">https://github.com/GoogleCloudPlatform/dataproc-templates.git</a><br/>cd dataproc-templates/python</span></pre><p id="84e5" class="pw-post-body-paragraph kg kh hi ki b kj lb kl km kn lc kp kq jw ld ks kt jz le kv kw kc lf ky kz la hb bi translated"><strong class="ki hj"> 3。</strong>执行Hive to BigQuery模板:</p><p id="0bcf" class="pw-post-body-paragraph kg kh hi ki b kj lb kl km kn lc kp kq jw ld ks kt jz le kv kw kc lf ky kz la hb bi translated"><strong class="ki hj"> a. </strong>导出工作提交所需的变量:</p><pre class="je jf jg jh fd ml mm mn mo aw mp bi"><span id="7a99" class="jp ig hi mm b fi mq mr l ms mt">export GCP_PROJECT=&lt;project_id&gt; <em class="mx"># your Google Cloud project</em><br/>export REGION=&lt;region&gt; <em class="mx"># your region ex: us-central1</em><br/>export JARS="gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar" <em class="mx"># .jar dependencies</em><br/>export SUBNET=&lt;subnet&gt; <em class="mx"># optional if you are using default</em><br/># export GCS_STAGING_LOCATION=&lt;gcs-staging-bucket-folder&gt; <em class="mx"># already done at step 1</em></span></pre><p id="f692" class="pw-post-body-paragraph kg kh hi ki b kj lb kl km kn lc kp kq jw ld ks kt jz le kv kw kc lf ky kz la hb bi translated"><strong class="ki hj"> b. </strong>运行Dataproc模板shell脚本，该脚本将读取上述变量，创建一个Python包，并将作业提交给Dataproc Serverless。</p><p id="25a4" class="pw-post-body-paragraph kg kh hi ki b kj lb kl km kn lc kp kq jw ld ks kt jz le kv kw kc lf ky kz la hb bi translated"><strong class="ki hj">注意</strong>:确保“python”可执行文件在您的路径中</p><pre class="je jf jg jh fd ml mm mn mo aw mp bi"><span id="5e48" class="jp ig hi mm b fi mq mr l ms mt">./bin/start.sh \<br/>- properties=spark.hadoop.hive.metastore.uris=thrift://&lt;hostname-or-ip&gt;:9083 \<br/>- - template=HIVETOBIGQUERY \<br/>- hive.bigquery.input.database="&lt;database&gt;" \<br/>- hive.bigquery.input.table="&lt;table&gt;" \<br/>- hive.bigquery.output.dataset="&lt;dataset&gt;" \<br/>- hive.bigquery.output.table="&lt;table&gt;" \<br/>- hive.bigquery.output.mode="&lt;append|overwrite|ignore|errorifexists&gt;" \<br/>- hive.bigquery.temp.bucket.name="&lt;temp-bq-bucket-name&gt;"</span></pre><p id="cb83" class="pw-post-body-paragraph kg kh hi ki b kj lb kl km kn lc kp kq jw ld ks kt jz le kv kw kc lf ky kz la hb bi translated">您可能已经注意到，在提交模板时需要填写一些参数。这些参数在<a class="ae lu" href="https://github.com/GoogleCloudPlatform/dataproc-templates/tree/main/python/dataproc_templates/hive" rel="noopener ugc nofollow" target="_blank">模板文件</a>中有描述。</p><p id="9eb9" class="pw-post-body-paragraph kg kh hi ki b kj lb kl km kn lc kp kq jw ld ks kt jz le kv kw kc lf ky kz la hb bi nb translated">成功了！作业将会运行，您可以从<a class="ae lu" href="https://console.cloud.google.com/dataproc/batches" rel="noopener ugc nofollow" target="_blank"> Dataproc无服务器UI </a>对其进行监控。</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h1 id="c7ba" class="if ig hi bd ih ii mc ik il im md io ip iq me is it iu mf iw ix iy mg ja jb jc bi translated">笔记</h1><ul class=""><li id="4029" class="lg lh hi ki b kj kk kn ko jw mh jz mi kc mj la mk lm ln lo bi translated">要定制作业，您可以从模板(之前克隆的)中更改<a class="ae lu" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/python/dataproc_templates/hive/hive_to_bigquery.py" rel="noopener ugc nofollow" target="_blank"> PySpark代码</a>。</li><li id="75e1" class="lg lh hi ki b kj lp kn lq jw lr jz ls kc lt la mk lm ln lo bi translated">为了快速拥有一个Hive环境来测试模板执行，您可以用Apache Hive 创建一个<a class="ae lu" href="https://cloud.google.com/architecture/using-apache-hive-on-cloud-dataproc" rel="noopener ugc nofollow" target="_blank"> Dataproc集群，并连接到它。</a></li><li id="287f" class="lg lh hi ki b kj lp kn lq jw lr jz ls kc lt la mk lm ln lo bi translated">假设Apache Hive的身份验证不是通过Kerberos进行的。否则，您必须使用必要的属性修改代码，以便使用Kerberos进行身份验证。</li><li id="a736" class="lg lh hi ki b kj lp kn lq jw lr jz ls kc lt la mk lm ln lo bi translated">BigQuery表(输出)不需要预先创建，但是数据集需要。</li><li id="51b6" class="lg lh hi ki b kj lp kn lq jw lr jz ls kc lt la mk lm ln lo bi translated">您可能已经注意到我们需要两个临时存储桶，一个用于Dataproc Serverless中的作业提交依赖项，另一个用作将数据加载到big query(hive . big query . temp . bucket . name)的临时位置。</li><li id="e77e" class="lg lh hi ki b kj lp kn lq jw lr jz ls kc lt la mk lm ln lo bi translated">如果出现错误“当‘internal _ IP _ only’设置为‘true’时，子网‘default’不支持Dataproc集群所需的私有Google访问”,请确保查看并遵循前提条件部分的建议。在子网上启用私人Google访问“默认”或将“internal_ip_only”设置为“false”。</li><li id="4b54" class="lg lh hi ki b kj lp kn lq jw lr jz ls kc lt la mk lm ln lo bi translated">HiveToBigQuery模板将从指定的表中读取所有分区，但是如果您想要处理来自某个特定分区的数据，更改代码很简单。</li></ul></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h2 id="643b" class="jp ig hi bd ih jq jr js il jt ju jv ip jw jx jy it jz ka kb ix kc kd ke jb kf bi translated">也是开源项目<a class="ae lu" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> Dataproc模板</a>的贡献者！</h2></div></div>    
</body>
</html>