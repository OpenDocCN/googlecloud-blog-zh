<html>
<head>
<title>Building your own conversational voice AI with Dialogflow &amp; Speech to Text in web apps.(Part I)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在web应用程序中使用Dialogflow &amp;语音转文本构建您自己的对话式语音人工智能。(第一部分)</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/building-your-own-conversational-voice-ai-with-dialogflow-speech-to-text-in-web-apps-part-i-b92770bd8b47?source=collection_archive---------0-----------------------#2020-04-08">https://medium.com/google-cloud/building-your-own-conversational-voice-ai-with-dialogflow-speech-to-text-in-web-apps-part-i-b92770bd8b47?source=collection_archive---------0-----------------------#2020-04-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="e0ea" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">将音频从浏览器麦克风流式传输到Dialogflow的最佳实践&amp;谷歌云语音转文本。</strong></h1><p id="c59d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这是该系列的第一篇博客:</p><p id="bc97" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">将音频从浏览器麦克风流式传输到Dialogflow的最佳实践&amp;谷歌云语音转文本。</strong></p><p id="3620" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在第一篇博客中，我将阐述为什么客户会集成他们自己的对话式人工智能，而不是为谷歌助手构建。我将介绍Google Cloud中所有的对话式人工智能组件，以及每个组件的用途。</p><p id="0793" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在这个博客系列的后面，我将向您展示如何在您的web应用程序中集成HTML5麦克风。如何将音频流传送到(Node.js)后端。如何使用Dialogflow API进行音频流？如何使用语音API？最后，如何将音频(文本到语音)返回给客户端，以便在浏览器中播放。</p><p id="977c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这些博客包含<a class="ae kg" href="https://github.com/dialogflow/selfservicekiosk-audio-streaming/tree/master/examples" rel="noopener ugc nofollow" target="_blank">简单的代码片段</a>，以及一个演示应用程序；<a class="ae kg" href="https://github.com/dialogflow/selfservicekiosk-audio-streaming/" rel="noopener ugc nofollow" target="_blank">机场自助服务亭</a>，将用作参考架构。</p><h1 id="aed7" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">谷歌助手与定制对话式人工智能</h1><p id="c634" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">我经常与客户交流，他们希望将谷歌助手纳入他们的商务网络应用。除非你是做电视机顶盒或者耳机的厂商，我总是回答；</strong></p><p id="fcbc" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">“这真的是你想要的吗？或者你的意思是你想用对话式人工智能来扩展你自己的应用程序？”</strong></p><p id="0506" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"><em class="kh">——“呃？”</em>T15】</strong></p><p id="b60f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">如果你有以下一个或多个需求，你可能想直接使用Google Cloud Speech和Dialogflow APIs，而不是</strong> <a class="ae kg" href="https://developers.google.com/assistant" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hj">将你的语音AI打包成谷歌助手</strong> </a> <strong class="jf hj">或</strong> <a class="ae kg" href="https://developers.google.com/assistant/sdk/overview" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hj">中的一个动作，在你的应用</strong> </a> <strong class="jf hj">中包装谷歌助手。</strong></p><ul class=""><li id="4109" class="ki kj hi jf b jg kb jk kc jo kk js kl jw km ka kn ko kp kq bi translated"><strong class="jf hj">该应用程序不应公开。</strong></li><li id="5792" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated"><strong class="jf hj">这个应用不需要在谷歌助手/ Nest Home上可用。</strong></li><li id="f4cd" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated">你不会想用唤醒词来启动你的应用程序:“嘿，谷歌，跟我的应用程序说话”。</li><li id="5aed" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated"><strong class="jf hj">应用不需要回答原生的谷歌助手问题，比如:“阿姆斯特丹的天气如何”。</strong></li><li id="5801" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated"><strong class="jf hj">应用只能利用谷歌云条款&amp;条件，而不能与谷歌助手的消费者条款&amp;条件结合使用。</strong></li></ul><p id="1008" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">确信你想通过集成语音人工智能功能来扩展你自己的(移动)网络应用？这是最终的开发者指南，关于实现从web应用到Google Cloud Speech和Dialogflow的语音流。</strong></p><h1 id="8e26" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">对话流与文本到语音的API与语音到文本的API</h1><p id="2ffc" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj"> Dialogflow </strong>是一款人工智能工具，用于构建基于文本和语音的对话界面，如聊天机器人和语音应用。它使用自然语言理解等机器学习模型来检测对话的意图。</p><p id="4310" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">对话流意图检测的工作方式是，它首先尝试理解用户话语。然后，它将根据训练短语检查包含意图(或聊天流)的Dialogflow代理。具有最佳匹配(最高置信度得分)的意图将返回答案，该答案可以是文本响应，也可以是通过履行从系统得到的响应。</p><p id="6bbc" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">尽管我们中的许多人会使用Dialogflow进行文本输入，但对于网络或社交媒体聊天机器人来说，它也可以将你的声音作为音频输入进行意图匹配，甚至可以将口语文本(TTS)作为音频结果返回。</p><p id="407b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">Dialogflow语音检测和输出将与云语音到文本API (STT)和云文本到语音(TTS)有一些重叠。就连API调用看起来都差不多！然而，这些服务是不同的，它们被用在不同的用例中。</p><p id="61a5" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">语音转文本(STT) </strong>将口语文字转录为书面文本。当你想在视频中生成字幕，从会议中生成文字记录等时，这是非常有用的。你也可以将它与Dialogflow聊天机器人(从文本转录中检测意图)结合起来，合成聊天机器人的答案，但是STT不像Dialogflow那样进行意图检测。STT是非常强大的，因为API调用响应将返回具有最高置信度得分的书面抄本，并且它将返回具有可选抄本选项的数组。</p><p id="1fb9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">使用<strong class="jf hj">文本到语音(TTS) </strong>，您可以发送文本或SSML(带语音标记的文本)输入，它将返回音频字节，您可以使用这些字节创建mp3文件或直接传输到音频播放器(在您的浏览器中)。</p><p id="4053" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">与<strong class="jf hj">谷歌助手</strong>相比，通过使用上述工具手动扩展带有对话式人工智能的应用，你不再是谷歌助手生态系统的一部分。如果你正在构建消费者或活动应用(语音操作)，这个生态系统很好，每个人都可以通过调用它来找到它。但是当你是一个企业时，整个生态系统可能会被过度破坏。</p><figure class="kx ky kz la fd lb er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es kw"><img src="../Images/23cd0fad6307c8ff69c8efcf7e4f755f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2F5EKjTXs5CGpfrFx_WoWA.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated">对谷歌生态系统的行动</figcaption></figure><p id="041f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">对于一个希望在自己的应用程序中集成语音人工智能的企业来说，完整的谷歌助手生态系统可能有点矫枉过正。</p><h1 id="9936" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">谷歌云联络中心人工智能</h1><p id="966c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">还有另一个谷歌解决方案，叫做<strong class="jf hj">谷歌云联络中心AI </strong> (CCAI)。</p><p id="1a72" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">该解决方案适用于希望在现有电话联络中心(IVR)中部署语音AI的企业。Dialogflow和Cloud Speech APIs是由电话合作伙伴(如Genesys、Avaya、Cisco等)开发的架构中的关键部分。)由于联络中心AI是一个开箱即用的解决方案，你不需要自己实现这些API。</p><h1 id="bce5" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">关于演示应用程序；机场自助服务亭</h1><p id="90cf" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，您已经知道了所有对话式GCP组件之间的差异，让我们看看如何在端到端web应用程序中实现这些组件。对于本指南，我将使用一个演示应用程序，这是一个机场的自助服务亭。(自助服务亭在零售或金融行业也很常见。)</p><p id="6903" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">你可以问自助服务亭；如果可以在你的手提包里带一个打火机，或者什么时候登机。结果会显示在屏幕上，还会大声说出来:</p><figure class="kx ky kz la fd lb er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lm"><img src="../Images/4391305c556f11c12d80cddb363f5277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hd82tZTnxus6sDBR"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated">这是我的演示应用的截图:机场自助服务亭</figcaption></figure><p id="f738" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我向您展示将音频从麦克风通过浏览器流式传输到Dialogflow，然后通过扬声器输出的最佳实践。</p><p id="7c17" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">所有代码都可以在Github上找到:<a class="ae kg" href="https://github.com/dialogflow/selfservicekiosk-audio-streaming" rel="noopener ugc nofollow" target="_blank">https://Github . com/dialog flow/self service kiosk-audio-streaming</a></p><p id="c820" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">最终的解决方案已经部署了App Engine Flex:【http://selfservicedesk.appspot.com T4】</p><p id="8cb2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">构建演示应用程序需要以下工具:</p><ul class=""><li id="5c60" class="ki kj hi jf b jg kb jk kc jo kk js kl jw km ka kn ko kp kq bi translated">NodeJS</li><li id="9052" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated">Dialogflow客户端SDK</li><li id="5860" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated">STT Node.js客户端SDK</li><li id="2b2a" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated">TTS Node.js客户端SDK</li><li id="9929" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated"><a class="ae kg" href="https://www.npmjs.com/package/socket.io" rel="noopener ugc nofollow" target="_blank">socket . io</a>T12<a class="ae kg" href="https://www.npmjs.com/package/socket.io-stream" rel="noopener ugc nofollow" target="_blank">socket . io-Stream</a></li><li id="c14e" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated"><a class="ae kg" href="https://github.com/muaz-khan/RecordRTC" rel="noopener ugc nofollow" target="_blank"> RecordRTC </a></li><li id="270f" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated">AppEngine灵活环境(支持websockets和HTTPS)</li></ul><h1 id="7328" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">体系结构</h1><p id="7e0e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这是我一直使用的架构:</p><figure class="kx ky kz la fd lb er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lm"><img src="../Images/7203a2c062e3f4efbc6fae74b51bbb8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*84Gk-bnqshOaOjqE"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated">我用过的建筑。</figcaption></figure><ul class=""><li id="c5ec" class="ki kj hi jf b jg kb jk kc jo kk js kl jw km ka kn ko kp kq bi translated">客户端网站/应用程序。出于演示的目的，我将向您展示两个版本。一个简单的HTML页面，和一个完整的Angular web应用程序的例子，<a class="ae kg" href="https://github.com/dialogflow/selfservicekiosk-audio-streaming" rel="noopener ugc nofollow" target="_blank">比如自助服务kiosk演示</a>。它包含由<strong class="jf hj"> RecordRTC </strong>库包装的<strong class="jf hj"> getUserMedia() WebRTC </strong>调用，以记录来自浏览器麦克风的音频流。</li><li id="0086" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated">NodeJS服务器将提供静态内容(如HTML页面)并连接到GCP图书馆，如Dialogflow、STT和TTS。</li><li id="d674" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated">您也可以使用任何其他编程语言。所有GCP服务都有各种客户端SDK(比如Node.js、Java、Python、Go等)，还有Rest和GRPC库。</li><li id="2f34" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka kn ko kp kq bi translated">Dialogflow代理，包含意图、实体和FAQ知识库。</li></ul><p id="5d70" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">客户端应用程序通过websockets与后端服务器对话。这是构建聊天机器人或聊天应用程序时的常用方法，因为它们可以实时响应，无需刷新页面。</p><p id="1fd8" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我使用socket.io框架和socket.io流插件，因为它很容易使用，而且我需要利用双向流。</p><p id="9da8" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="kh">注:</em> </strong></p><p id="a153" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="kh">我在网上看到过一些解决方案，其中麦克风直接连接到Dialogflow，中间没有服务器。其余的调用直接在web客户端用JavaScript完成。我认为这是一种反模式。您可能会在客户端代码中公开您的服务帐户/私钥。任何熟悉Chrome开发工具的人都可以窃取你的密钥，并通过你的账户进行(付费)API调用。总是让服务器处理Google Cloud认证是一个更好的方法。这样，服务帐户就不会暴露给公众。</em></p><h1 id="1cd1" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">短话语与流</h1><p id="499d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">关于如何将语音集成到您的应用中，通常有两种方法。</p><ol class=""><li id="b710" class="ki kj hi jf b jg kb jk kc jo kk js kl jw km ka ln ko kp kq bi translated">简短话语/探测意图。这意味着您的最终用户按下录音按钮，说话，当他们按下停止，我们收集音频流返回结果。在您的代码中，这意味着一旦客户端web应用程序收集了完整的音频记录，它就将其发送到服务器，这样服务器就可以调用Dialogflow或语音到文本API。</li><li id="2de2" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka ln ko kp kq bi translated">长话语流/检测流中的意图。这意味着您的最终用户按下录制按钮，说话，将看到结果的飞行。当检测意图时，这可能意味着一旦你说得更多，它将检测到更好的匹配，或者它可以收集多个结果。在您的代码中，这意味着客户端开始创建一个双向流，并将数据块传输到服务器，这样服务器就可以通过事件侦听器对传入的数据进行调用，因此它是实时的。</li><li id="719c" class="ki kj hi jf b jg kr jk ks jo kt js ku jw kv ka ln ko kp kq bi translated">当存在意图匹配时，我们可以通过显示文本在屏幕上显示结果，或者我们可以通过将音频缓冲流回客户端来合成(读出)结果，这将通过WebRTC AudioBufferSourceNode(或音频播放器)播放。</li></ol><p id="1a67" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">请继续关注我的下一篇博客。在这篇博客中，我将首先构建一个客户端web应用程序，该应用程序使用HTML5麦克风和WebRTC，将音频字节流传输到Node.js后端。</p></div></div>    
</body>
</html>