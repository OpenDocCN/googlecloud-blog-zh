# åœ¨ Google Cloud Composer ä¸­åˆ›å»ºåŠ¨æ€åˆ›å»ºçš„ DAG å’Œ TroubleshootAirflow çš„ web æœåŠ¡å™¨

> åŸæ–‡ï¼š<https://medium.com/google-cloud/create-a-dynamically-created-dag-and-troubleshoot-airflows-webserver-in-google-cloud-composer-290af1e3eb1b?source=collection_archive---------0----------------------->

æœ‰ä¸€ç§æƒ…å†µï¼Œæˆ‘éœ€è¦ä» Google Cloud SQL è·å–æˆ‘çš„é…ç½®ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªåŠ¨æ€åˆ›å»ºçš„ DAGã€‚æˆ‘åšçš„ç¬¬ä¸€æ­¥æ˜¯å°è¯•åœ¨æˆ‘çš„ Composer é›†ç¾¤ä¸­åˆ›å»ºä¸€ä¸ª`clousql proxy`ã€‚è¿™æ˜¯å¯ä»¥åšåˆ°çš„ï¼Œå› ä¸ºä¸€èˆ¬æ¥è¯´ä½ æ‰€æœ‰çš„æ°”æµå…ƒæ•°æ®ä¹Ÿå­˜å‚¨åœ¨äº‘ SQL é‡Œé¢ï¼Œæ‰€ä»¥ä½ å¯ä»¥åŸºäºä½ çš„ Composer é›†ç¾¤ä¸­å¯ç”¨çš„`cloudsql proxy` Dockerimage åˆ›å»ºä½ è‡ªå·±çš„`cloudsql proxy`ã€‚å–å†³äºä½ çš„æ°”æµç‰ˆæœ¬ï¼Œä½ å¯ä»¥å¾—åˆ°ä»¥ä¸‹çš„`Pod`å’Œ`Service`ä¸ºä»£ç†ã€‚

```
kubectl get svc airflow-sqlproxy-service -o yamlkubectl get deploy airflow-sqlproxy -o yaml > airflow-sql-proxy-2.yaml # change the -instance in the deployment, don't forget to add your composer service account to IAM of the project where your Cloud SQL instance belongs to
```

å¯¹äºä¸´æ—¶å­˜å‚¨ï¼Œæ‚¨è¿˜å¯ä»¥åœ¨ Composer é›†ç¾¤ä¸­çœ‹åˆ°é»˜è®¤çš„ Redis æœåŠ¡ã€‚

```
airflow-redis-service.<NAMESPACE> # Depends on your Composer version
```

åˆ›å»ºåŠ¨æ€ç”Ÿæˆçš„ DAG æ–‡ä»¶:

```
import json
import re
from datetime import datetime, timedelta
import loggingfrom airflow import DAG
from airflow.models import Variable
from airflow.hooks.postgres_hook import PostgresHook
from airflow.operators.python_operator import PythonOperator
from airflow.hooks.base_hook import BaseHook
from airflow.contrib.operators.slack_webhook_operator import SlackWebhookOperator
from psycopg2.extras import RealDictCursor
import redislogger = logging.getLogger(__name__)class Variables(object):
    DATABASE_CONNECTION_ID = Variable.get('DB_CONNECTION_ID')
    ENVIRONMENT = Variable.get('ENVIRONMENT')
    SLACK_CONN_ID = Variable.get("SLACK_WEBHOOK_ID")
    MYAIRFLOW_DASHBOARD_URL = Variable.get("EXPAIRFLOW_DASHBOARD_URL")
    AIRFLOW_DASHBOARD_URL = Variable.get("AIRFLOW_DASHBOARD_URL")def standardize_naming(input):
    return '{}_{}'.format(input, Variables.ENVIRONMENT)class Constants(object):
    REDIS_KEY = standardize_naming("my-redis")
    REDIS_CONNECION_SERVICE = #your redis service
    ALPHANUM_UNDERSCORE_ONLY_REGEX = '[^0-9a-zA-Z_]+'
    STRPTIME_FORMAT = '%Y-%m-%d %X'
    MAX_ETL_DURATION_DAYS = 35
    PG_QUERY = # your config querydef remove_unsafe_character(input):
    return re.sub(Constants.ALPHANUM_UNDERSCORE_ONLY_REGEX, '_', input)def failed_task_slack_alert(context):
    slack_webhook_token = BaseHook.get_connection(Variables.SLACK_CONN_ID).passwordbase_log_url = context.get('task_instance').log_url
    updated_log_url = base_log_url.replace(Variables.AIRFLOW_DASHBOARD_URL,
                                           Variables.MYAIRFLOW_DASHBOARD_URL)
    slack_msg = """
            :red_circle: Task Failed. 
            *Task*: {task}  
            *Dag*: {dag} 
            *Execution Time*: {exec_date}  
            *Log Url*: {log_url} 
            """.format(
            task=context.get('task_instance').task_id,
            dag=context.get('task_instance').dag_id,
            ti=context.get('task_instance'),
            exec_date=context.get('execution_date'),
            log_url=updated_log_url,
    )
    failed_alert = SlackWebhookOperator(
            task_id='slack_test',
            http_conn_id=Variables.SLACK_CONN_ID,
            webhook_token=slack_webhook_token,
            message=slack_msg,
            username='airflow')
    return failed_alert.execute(context=context)default_args = {
    "owner": "someone",
    "depends_on_past": False,
    "start_date": datetime(2019, 1, 1),
    "email": [],
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "on_failure_callback": failed_task_slack_alert,
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
}def deduplicate_email(emails):
    tmp = set(emails)
    return list(tmp)def convertCamelToSnake(camel):
    s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', camel)
    s2 = re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()
    s3 = s2.replace("-", "_").replace(".", "_")
    while "__" in s3:
        s3 = s3.replace("__", "_")
    return s3"""
[https://stackoverflow.com/questions/47705060/running-bigquery-query-uncached-using-python-api](https://stackoverflow.com/questions/47705060/running-bigquery-query-uncached-using-python-api)
[https://github.com/tylertreat/BigQuery-Python/blob/master/bigquery/client.py](https://github.com/tylertreat/BigQuery-Python/blob/master/bigquery/client.py)
[https://stackoverflow.com/questions/47172150/google-cloud-bigquery-python-how-to-get-the-bytes-processed-by-query](https://stackoverflow.com/questions/47172150/google-cloud-bigquery-python-how-to-get-the-bytes-processed-by-query)
"""def create_dag(dag_id, config, default_args):
    def dummy(task_id):
        print(task_id)start_date = datetime.strptime(config['startdatestr'], '%Y-%m-%d')
    if config['enddatestr'] is not None:
        end_date = datetime.strptime(config['enddatestr'], '%Y-%m-%d')
    else:
        end_date = start_date + timedelta(days=Constants.MAX_ETL_DURATION_DAYS)if len(config['notificationemails']) > 0:
        default_args['email'].extend([str(email) for email in config['notificationemails']])default_args['email'] = deduplicate_email(default_args['email'])execution_period = 24dag = DAG(dag_id,
              default_args=default_args,
              schedule_interval=timedelta(hours=execution_period),
              start_date=start_date,
              end_date=end_date,
              catchup=False)logger.info("Exp Processing: Create DAG with ID {}".format(dag_id))with dag:
        """
        common var
        """start_date_safe = remove_unsafe_character(config['startdatestr'])
        cur_date = '{{ ds }}'
        cur_time = '{{ ts }}'
        cur_date_safe = '{{ execution_date.strftime("%Y_%m_%d") }}'
        version = str(config['version'])task_id = '{prefix}_{name}_{version}'.format(
                prefix=standardize_naming('my-dag'),
                version=version)task = PyhtonOperator(
                task_id=task_id,
                provide_context=True,
                python_callable=dummy,
                op_kwargs={'task_id': task_id}
            )return dag"""
handle datetime format when fetching from pg
so that the fetch result can be stored as json instead of plain string
"""def converter(o):
    if isinstance(o, datetime):
        return o.__str__()"""
Bahavior:
Create an exact DAG which in turn will create it's own file
[https://www.astronomer.io/guides/dynamically-generating-dags/](https://www.astronomer.io/guides/dynamically-generating-dags/)
"""redis_conn = redis.StrictRedis(host=Constants.REDIS_CONNECTION_SERVICE, port=6379,
                               db=0)
serialized_configs = redis_conn.get(Constants.REDIS_KEY)configs = []
if serialized_configs == None:
    pg_hook = PostgresHook(postgres_conn_id=Variables.DATABASE_CONNECTION_ID)
    pg_conn = pg_hook.get_conn()cursor = pg_conn.cursor(cursor_factory=RealDictCursor)
    cursor.execute(Constannts.PG_QUERY)res = cursor.fetchall()logger.info(
            "{res} fetched on {time}".format(res=json.dumps(res, default=converter),
                                             time=datetime.now()))
    # Set expiration to 1 minutes, to prevent to much query on the DB.
    redis_conn.setex(Constants.REDIS_KEY, timedelta(minutes=1),
                     json.dumps(res, default=converter))configs = res
else:
    configs = json.loads(serialized_configs)for config in configs:
    if config['hasetlstarted'] == True:
        id = re.sub(Constants.ALPHANUM_UNDERSCORE_ONLY_REGEX, '_',
                            config['id'])
        version = str(config['version'])
        dag_id = "{prefix}_{id}_{version}".format(
                prefix=standardize_naming('my_dag'),
                id=id,
                version=version)
        logger.info("Dag with ID : {} is created.".format(dag_id))
        globals()[dag_id] = create_dag(dag_id, config, default_args)
```

æ‚¨è¿˜å¯ä»¥åˆ›å»ºä¸€ä¸ª Cloudbuild æ–‡ä»¶æ¥å®ç° Github å’Œ Composer Airflow ç›®å½•ä¹‹é—´çš„åŒæ­¥:

```
steps:- name: 'google/cloud-sdk'
  entrypoint: 'bash'
  args:
    - '-c'
    - |
      gsutil rsync -R dags ${_BUCKET}
  id: 'deploy'substitutions:
  _BUCKET: gs://<your-composer-dags-gcs-id>/dags
```

æˆ‘å‘ç°äº†ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘çš„ DAG åœ¨ [Composer Webserver](https://stackoverflow.com/questions/51218314/dags-not-clickable-on-google-cloud-composer-webserver-but-working-fine-on-a-loc) ä¸­æ— æ³•ç‚¹å‡»ã€‚DAG ä¸å¯ç‚¹å‡»,â€œæœ€è¿‘çš„ä»»åŠ¡â€å’Œâ€œDAG è¿è¡Œâ€åˆ—å°†æ°¸è¿œåŠ è½½ã€‚æ¯ä¸ª DAG åç§°æ—è¾¹çš„â€œinfoâ€æ ‡è®°è¡¨ç¤º:

```
This DAG isn't available in the webserver DagBag object. It shows up in this list because the scheduler marked it as active in the metadata database.
```

![](img/23e0a460710c812d835df44075cd6596.png)

æ— æ³•åœ¨ Composer Webserver ä¸­å•å‡»åŠ¨æ€åˆ›å»ºçš„ DAGã€‚

è¿™æ˜¯å› ä¸º Webserver ä½¿ç”¨ App Engineï¼Œè€Œå…ƒæ•°æ®æœ¬èº«å­˜å‚¨åœ¨ Cloud SQL å’Œ GCS ä¸­ï¼Œä¸ç›´æ¥ä¸ Composer é›†ç¾¤å†…éƒ¨çš„æœ¬åœ° Redis é€šä¿¡ã€‚äº‹å®ä¸Šï¼Œè‡ªåŠ¨ç”Ÿæˆçš„ DAG æ˜¯ç”±ä¸€ä¸ªæ–‡ä»¶ç”Ÿæˆçš„ï¼Œå…¶ä¸­å…³äº DAG çš„æ‰€æœ‰ä¿¡æ¯ä»…å­˜åœ¨äº Composer é›†ç¾¤å†…çš„ Redis ä¸­ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ­¤ç±»é—®é¢˜ã€‚æ‚¨å¯ä»¥é€šè¿‡éƒ¨ç½²æ‚¨çš„è‡ªæˆ‘ç®¡ç†æ°”æµç½‘ç»œæœåŠ¡å™¨æ¥å¤„ç†æ­¤é—®é¢˜ï¼Œè¯¥æœåŠ¡å™¨å¯åœ¨[ä¸­æ‰¾åˆ°](https://cloud.google.com/composer/docs/how-to/managing/deploy-webserver)ã€‚ç„¶è€Œï¼Œå¦‚æœä½ æƒ³ä½¿ç”¨å…¬å…± API è®¿é—®ä½ çš„ç½‘ç»œæœåŠ¡å™¨ï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ªå…¥å£ã€‚

```
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubernetes.io/ingress.global-static-ip-name: <YOUR-STATIC-IP-THAT_ALREADY-BIND-WITH-AN-IP>
    name: airflow-webserver
    namespace: <YOUR-NAMESPACE>
  spec:
    backend:
      serviceName: airflow-webserver-service
      servicePort: 80
    tls:
    - secretName: <YOUr-SSL-SECRET>
```

ä½¿ç”¨ä½ æƒ³è¦çš„ä»»ä½•ä¸œè¥¿æ¥ä¿æŠ¤å®ƒï¼Œå¯¹æˆ‘æ¥è¯´ï¼Œæˆ‘ä½¿ç”¨çš„æ˜¯è°·æ­Œ IAPã€‚

è¿™æ˜¯æˆ‘èƒ½æƒ³åˆ°çš„ä¸€åˆ‡ã€‚å†è§ï¼ğŸ‘‹