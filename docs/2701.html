<html>
<head>
<title>Exporting data from Snowflake to GCS using PySpark on Dataproc Serverless</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Dataproc无服务器上使用PySpark将数据从雪花导出到GCS</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/exporting-data-from-snowflake-to-gcs-using-pyspark-on-dataproc-serverless-363d3bed551b?source=collection_archive---------7-----------------------#2022-12-28">https://medium.com/google-cloud/exporting-data-from-snowflake-to-gcs-using-pyspark-on-dataproc-serverless-363d3bed551b?source=collection_archive---------7-----------------------#2022-12-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/feb39e21f4b3edcaefcbfe1a8ac08af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*alX_Op84F7q9FvsESztYbQ.jpeg"/></div></figure><h1 id="096c" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">介绍</h1><p id="a61c" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">雪花是基于云的数据仓库解决方案，作为SaaS产品提供。它利用AWS、Google Cloud或Azure提供的基础设施来满足其存储和计算需求。</p><p id="852b" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">将数据从Snowflake导出到GCS的典型程序包括使用<code class="du kn ko kp kq b"><strong class="jm hj">COPY INTO</strong></code> <strong class="jm hj"> </strong>命令将数据直接移入GCS或通过创建外部阶段。尽管这是将数据导入GCS的最简单方法，但如果您需要按计划导出数据，或者需要导出的数据太大，这不是一种有利/推荐的方法。当您需要定期导出大量数据时，Dataproc Serverless将会派上用场。</p><p id="3b0e" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">这篇博文解释了如何使用Dataproc Serverless将数据从雪花导出到GCS。</p><p id="3f92" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">以下是一些需要将数据从雪花导出到GCS的用例:</p><ol class=""><li id="577d" class="kr ks hi jm b jn ki jr kj jv kt jz ku kd kv kh kw kx ky kz bi translated">您希望将您的DWH迁移到BigQuery，并希望GCS充当一个临时层。</li><li id="6339" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh kw kx ky kz bi translated">您希望按照规定的计划将cold数据归档到GCS。</li><li id="6575" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh kw kx ky kz bi translated">你想把你的数据用于机器学习活动(BQML，Vertex AI等。等等。)</li></ol><h1 id="e693" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">Dataproc无服务器有什么帮助？</h1><p id="7a36" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">嗯，Dataproc Serverless为您提供了Hadoop和Spark的功能，并且少了一个顾虑，即管理Dataproc集群配置和调优的开销。无论您的数据大小如何，Dataproc Serverless都会在托管计算基础设施上运行您的工作负载，根据需要自动缩放/缩减资源。<br/>与传统的Dataproc集群相比，它也更便宜，在传统的data proc集群中，为了节省成本，您需要担心在集群空闲时停止集群，并在需要时重启集群。在Dataproc Serverless的情况下，只对处理工作负载的时间收费。</p><h1 id="4c89" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">输入Dataproc无服务器模板..</h1><p id="b78b" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><a class="ae lf" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> Dataproc模板</a>用JAVA和Python为Spark工作负载的大多数常见用例提供现成的代码。这些可以根据最终用户的具体需求进一步定制。这些模板在根据repo中提供的指令运行时，将部署Dataproc无服务器作业来执行您的数据工作负载的处理。</p><p id="d742" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">在本帖</strong>中，我们将讨论如何使用PySpark <strong class="jm hj">雪花到GCS模板</strong>。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lg"><img src="../Images/abccec8b18795bba2a92f023ca00436a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZbRHK-moZGBBqBVTeaWOLQ.png"/></div></div></figure><h1 id="2d20" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">先决条件</h1><p id="ba0d" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">为了运行这些模板，我们需要:</p><ul class=""><li id="81cb" class="kr ks hi jm b jn ki jr kj jv kt jz ku kd kv kh lp kx ky kz bi translated">具有对象读取/创建权限的雪花帐户</li><li id="28f7" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh lp kx ky kz bi translated">具有时段创建权限并启用了Dataproc API的GCP帐户。</li><li id="bc21" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh lp kx ky kz bi translated">Google Cloud SDK已安装并通过验证。</li><li id="e012" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh lp kx ky kz bi translated">Python 3.7+已安装。</li><li id="2dfd" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh lp kx ky kz bi translated">这里提到的<a class="ae lf" href="https://github.com/GoogleCloudPlatform/dataproc-templates/tree/main/python/dataproc_templates/snowflake#jars-required" rel="noopener ugc nofollow" target="_blank">所需的罐子</a></li><li id="936a" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh lp kx ky kz bi translated">启用了专用Google访问的VPC子网。默认子网是合适的，只要启用了私有Google访问。您可以在此查看所有Dataproc无服务器网络需求<a class="ae lf" href="https://cloud.google.com/dataproc-serverless/docs/concepts/network" rel="noopener ugc nofollow" target="_blank">。</a></li></ul><pre class="lh li lj lk fd lq kq lr bn ls lt bi"><span id="11ab" class="lu in hi kq b be lv lw l lx ly"># Example updating default network to enable Private Google Access<br/>gcloud compute networks subnets update default — region=us-central1 \<br/>— enable-private-ip-google-access</span></pre></div><div class="ab cl lz ma gp mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hb hc hd he hf"><h1 id="e656" class="im in hi bd io ip mg ir is it mh iv iw ix mi iz ja jb mj jd je jf mk jh ji jj bi translated">使用</h1><ol class=""><li id="abc1" class="kr ks hi jm b jn jo jr js jv ml jz mm kd mn kh kw kx ky kz bi translated">创建一个GCS存储桶，用作Dataproc的暂存位置。这个桶将用于存储运行我们的无服务器集群所需的依赖关系。</li></ol><pre class="lh li lj lk fd lq kq lr bn ls lt bi"><span id="8a6a" class="lu in hi kq b be lv lw l lx ly">export STAGING_BUCKET=”dataproc-staging-bucket”<br/>gsutil mb gs://$STAGING_BUCKET</span></pre><p id="381f" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">2.克隆Dataproc模板库并导航到Python。模板的目录</p><pre class="lh li lj lk fd lq kq lr bn ls lt bi"><span id="b2d6" class="lu in hi kq b be lv lw l lx ly">git clone https://github.com/GoogleCloudPlatform/dataproc-templates.git<br/>cd dataproc-templates/python</span></pre><p id="2729" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">3.配置Dataproc无服务器作业</p><p id="acb0" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">为了将作业提交给Dataproc Serverless，我们将使用提供的bin/start.sh脚本。该脚本要求我们使用环境变量来配置Dataproc无服务器集群。</p><p id="a63d" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">强制配置包括:</p><ul class=""><li id="5643" class="kr ks hi jm b jn ki jr kj jv kt jz ku kd kv kh lp kx ky kz bi translated"><code class="du kn ko kp kq b">GCP_PROJECT</code>:无服务器运行Dataproc的GCP项目。</li><li id="276e" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh lp kx ky kz bi translated"><code class="du kn ko kp kq b">REGION</code>:运行Dataproc无服务器的区域。</li><li id="0ec3" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh lp kx ky kz bi translated"><code class="du kn ko kp kq b">GCS_STAGING_LOCATION</code>:一个GCS位置，Dataproc将在此存储登台资产。应该在我们之前创建的桶内。</li><li id="a958" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh lp kx ky kz bi translated"><code class="du kn ko kp kq b">SUBNET</code>:运行Dataproc无服务器的子网。</li></ul><pre class="lh li lj lk fd lq kq lr bn ls lt bi"><span id="8034" class="lu in hi kq b be lv lw l lx ly"># Project ID to run the Dataproc Serverless Job<br/>export GCP_PROJECT=&lt;project_id&gt;# GCP region where the job should be submitted<br/>export REGION=&lt;region&gt;# The staging location for Dataproc<br/>export GCS_STAGING_LOCATION=gs://$STAGING_BUCKET/staging<br/>export SUBNET=&lt;subnet&gt;</span></pre><p id="1428" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">在我们的例子中，到GCS的雪花需要先决条件中提到的ARs在类路径中可用。您可以将JAR文件存储在一个桶中，我们将使用<code class="du kn ko kp kq b">JARS</code>环境变量来添加它。</p><pre class="lh li lj lk fd lq kq lr bn ls lt bi"><span id="d1fa" class="lu in hi kq b be lv lw l lx ly"># Path to the Snowflake connector JAR file<br/>export JARS=&lt;comma-seperated-gcs-bucket-location-containing-jar-file&gt;</span></pre><p id="db96" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">4.运行Dataproc模板shell脚本，它将读取上述变量，创建一个Python包，并将作业提交给Dataproc Serverless。</p><pre class="lh li lj lk fd lq kq lr bn ls lt bi"><span id="23e5" class="lu in hi kq b be lv lw l lx ly">bin/start.sh \<br/>-- --template=SNOWFLAKETOGCS \<br/>--snowflake.to.gcs.sf.url=&lt;snowflake-account-url&gt; \<br/>--snowflake.to.gcs.sf.user=&lt;snowflake-user&gt; \<br/>--snowflake.to.gcs.sf.password=&lt;snowflake-user-password&gt; \<br/>--snowflake.to.gcs.sf.database=&lt;snowflake-database&gt; \<br/>--snowflake.to.gcs.sf.schema=&lt;snowflake-schema&gt; \<br/>--snowflake.to.gcs.sf.warehouse=&lt;snowflake-warehouse&gt; \<br/>--snowflake.to.gcs.sf.query=&lt;snowflake-select-query&gt; \<br/>--snowflake.to.gcs.output.location="gs://bucket" \<br/>--snowflake.to.gcs.output.format=&lt;csv|avro|orc|json|parquet&gt; \<br/>--snowflake.to.gcs.output.mode=&lt;Overwrite|ErrorIfExists|Append|Ignore&gt; \<br/>--snowflake.to.gcs.partition.column=&lt;gcs-output-partitionby-columnname&gt; \<br/>--snowflake.gcs.sf.autopushdown=&lt;on|off&gt;</span></pre><p id="7658" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">注</strong>:</p><ul class=""><li id="89a5" class="kr ks hi jm b jn ki jr kj jv kt jz ku kd kv kh lp kx ky kz bi translated">确保“python”可执行文件在您的路径中</li><li id="ab43" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh lp kx ky kz bi translated">提交作业将要求您启用Dataproc API，如果还没有启用的话。</li><li id="aee0" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh lp kx ky kz bi translated">强制参数:雪花. to.gcs.sf.url，雪花. to.gcs.sf.user，雪花. to.gcs.sf.password，雪花. to.gcs.sf.database，雪花. to.gcs.output.location</li></ul><p id="e6df" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">5.您可能已经注意到，在提交模板时需要填写一些参数。这些参数在<a class="ae lf" href="https://github.com/GoogleCloudPlatform/dataproc-templates/tree/main/python/dataproc_templates/snowflake#1-snowflake-to-gcs" rel="noopener ugc nofollow" target="_blank">模板文档</a>中描述。</p><p id="545e" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">6.监控Spark批处理作业</p><p id="381c" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">提交工作后，我们将能够在<a class="ae lf" href="https://console.cloud.google.com/dataproc/batches" rel="noopener ugc nofollow" target="_blank">data roc批处理界面</a>中看到。在那里，我们可以查看作业的度量和日志。</p><h1 id="1a99" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">预定处决</h1><p id="cef6" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">您也可以选择设置作业的计划执行，而不是通过start.sh脚本提交作业。当您希望在白天收到新数据时定期从雪花移动到GCS时，此设置非常有用。您可以使用云计划程序来计划Dataproc模板的执行。云计划程序是一项GCP服务，提供cron作业计划程序的功能。</p><p id="a526" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">由于雪花是一个OLAP数据库，CDC的变化将主要包括批量更新。雪花表有一个维护上次更新时间戳的附加时间戳列。您需要修改查询参数<code class="du kn ko kp kq b">snowflake.to.gcs.sf.query</code>的值，以包括时间戳过滤器，这样您的计划作业就可以读取最新的数据来进行增量加载。</p></div><div class="ab cl lz ma gp mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hb hc hd he hf"><h1 id="0c66" class="im in hi bd io ip mg ir is it mh iv iw ix mi iz ja jb mj jd je jf mk jh ji jj bi translated">关键词</h1><p id="5fca" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><code class="du kn ko kp kq b"><strong class="jm hj">Autopushdown</strong></code></p><ul class=""><li id="e44e" class="kr ks hi jm b jn ki jr kj jv kt jz ku kd kv kh lp kx ky kz bi translated">Spark-雪花连接器提供的一种功能，通过允许将大型复杂的Spark逻辑计划转换并推送到雪花，而不是在Spark中进行处理，从而引入了高级优化功能以提高性能。这意味着，雪花将通过利用其性能效率来完成大部分繁重的工作。</li></ul><h1 id="c4a0" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">参考</h1><ul class=""><li id="392d" class="kr ks hi jm b jn jo jr js jv ml jz mm kd mn kh lp kx ky kz bi translated"><a class="ae lf" href="https://cloud.google.com/dataproc-serverless/docs/overview" rel="noopener ugc nofollow" target="_blank">data roc serv less</a></li><li id="beaf" class="kr ks hi jm b jn la jr lb jv lc jz ld kd le kh lp kx ky kz bi translated"><a class="ae lf" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank">data roc模板库</a></li></ul></div></div>    
</body>
</html>