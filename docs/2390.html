<html>
<head>
<title>Exporting data from Redshift to GCS — using GCP Dataproc Serverless and PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将数据从Redshift导出到GCS —使用GCP Dataproc Serverless和PySpark</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/exporting-data-from-redshift-to-gcs-using-gcp-dataproc-serverless-and-pyspark-9ab78de11405?source=collection_archive---------2-----------------------#2022-09-28">https://medium.com/google-cloud/exporting-data-from-redshift-to-gcs-using-gcp-dataproc-serverless-and-pyspark-9ab78de11405?source=collection_archive---------2-----------------------#2022-09-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/42ae6e89ed4ec2cf720e7dfee3e5605f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*6-ZZhdeol08u2M9Lqsd1wg.png"/></div></figure><p id="fad1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" href="https://cloud.google.com/dataproc-serverless/docs" rel="noopener ugc nofollow" target="_blank"> Dataproc无服务器</a>允许您运行Spark batch工作负载，而不需要您提供和管理自己的集群。该服务将在托管计算基础架构上运行工作负载，根据需要自动扩展资源。指定工作负载参数，然后将工作负载提交给Dataproc无服务器服务。</p><p id="c95e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> Dataproc模板</a>使用Java和Python为在Dataproc无服务器上运行的常见用例提供解决方案，让我们能够定制工作负载并轻松运行。</p><p id="b47a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当前多云世界中的一个这样的用例是将数据从Redshift导出到Google云存储(GCS)。在这篇博客中，我们将讨论如何使用Dataproc Serverless处理从redshift到GCS的数据导出。</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div class="er es jl"><img src="../Images/802fa04e8c7009864d8acc4a9c9827fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*BSAiKnSNDMoNHKm7gXIBbA.png"/></div></figure><h1 id="fdc3" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">主要优势</h1><ul class=""><li id="594f" class="ko kp hi io b ip kq it kr ix ks jb kt jf ku jj kv kw kx ky bi translated">使用Dataproc Serverless运行Spark batch工作负载，而无需提供和管理您自己的集群。</li><li id="7c95" class="ko kp hi io b ip kz it la ix lb jb lc jf ld jj kv kw kx ky bi translated"><a class="ae jk" href="https://github.com/GoogleCloudPlatform/dataproc-templates/tree/main/python/dataproc_templates/redshift#redshift-to-gcs" rel="noopener ugc nofollow" target="_blank">红移到GCS </a>模板是开源的，完全可定制，随时可用于所需的工作。</li><li id="9fb7" class="ko kp hi io b ip kz it la ix lb jb lc jf ld jj kv kw kx ky bi translated">您可以将Redshift中的数据以拼花、AVRO、CSV和JSON格式导入GCS。</li></ul><h1 id="9532" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">先决条件</strong></h1><p id="e21a" class="pw-post-body-paragraph im in hi io b ip kq ir is it kr iv iw ix le iz ja jb lf jd je jf lg jh ji jj hb bi translated">为了运行这些模板，我们需要:</p><ul class=""><li id="fc77" class="ko kp hi io b ip iq it iu ix lh jb li jf lj jj kv kw kx ky bi translated">Google Cloud SDK已安装并通过验证</li><li id="bd8e" class="ko kp hi io b ip kz it la ix lb jb lc jf ld jj kv kw kx ky bi translated">Python 3.7以上版本已安装</li><li id="8ed4" class="ko kp hi io b ip kz it la ix lb jb lc jf ld jj kv kw kx ky bi translated">GCP自动气象站红移和Dataproc无服务器之间的连接。</li><li id="ea2f" class="ko kp hi io b ip kz it la ix lb jb lc jf ld jj kv kw kx ky bi translated">用于访问临时S3位置和红移IAM角色的AWS密钥和访问密钥。</li><li id="934f" class="ko kp hi io b ip kz it la ix lb jb lc jf ld jj kv kw kx ky bi translated">这里所说的<a class="ae jk" href="https://github.com/GoogleCloudPlatform/dataproc-templates/tree/main/python/dataproc_templates/redshift#required-jar-files" rel="noopener ugc nofollow" target="_blank">所需的罐子</a>。</li></ul><h1 id="4c04" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">使用</h1><ol class=""><li id="a016" class="ko kp hi io b ip kq it kr ix ks jb kt jf ku jj lk kw kx ky bi translated">创建一个GCS存储桶，用作Dataproc的暂存位置。这个桶将用于存储运行我们的无服务器集群所需的依赖关系。</li></ol><pre class="jm jn jo jp fd ll lm ln lo aw lp bi"><span id="c886" class="lq jr hi lm b fi lr ls l lt lu">export STAGING_BUCKET=”dataproc-staging-bucket”<br/>gsutil mb gs://$STAGING_BUCKET</span></pre><p id="3a41" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.克隆Dataproc模板库并导航到Python。模板的目录</p><pre class="jm jn jo jp fd ll lm ln lo aw lp bi"><span id="4ecb" class="lq jr hi lm b fi lr ls l lt lu">git clone <a class="ae jk" href="https://github.com/GoogleCloudPlatform/dataproc-templates.git" rel="noopener ugc nofollow" target="_blank">https://github.com/GoogleCloudPlatform/dataproc-templates.git</a><br/>cd dataproc-templates/python</span></pre><p id="e8ee" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">3.配置Dataproc无服务器作业</p><p id="1e3d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了将作业提交给Dataproc Serverless，我们将使用提供的bin/start.sh脚本。该脚本要求我们使用环境变量来配置Dataproc无服务器集群。</p><p id="e82d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">强制性配置包括:</p><ul class=""><li id="1f4e" class="ko kp hi io b ip iq it iu ix lh jb li jf lj jj kv kw kx ky bi translated"><code class="du lv lw lx lm b">GCP_PROJECT</code>:无服务器运行Dataproc的GCP项目。</li><li id="6d51" class="ko kp hi io b ip kz it la ix lb jb lc jf ld jj kv kw kx ky bi translated"><code class="du lv lw lx lm b">REGION</code>:运行Dataproc无服务器的区域。</li><li id="5446" class="ko kp hi io b ip kz it la ix lb jb lc jf ld jj kv kw kx ky bi translated"><code class="du lv lw lx lm b">GCS_STAGING_LOCATION</code>:一个GCS位置，Dataproc将在此存储登台资产。应该在我们之前创建的桶内。</li><li id="4575" class="ko kp hi io b ip kz it la ix lb jb lc jf ld jj kv kw kx ky bi translated"><code class="du lv lw lx lm b">SUBNET</code>:运行Dataproc无服务器的子网。</li></ul><pre class="jm jn jo jp fd ll lm ln lo aw lp bi"><span id="a011" class="lq jr hi lm b fi lr ls l lt lu"># Project ID to run the Dataproc Serverless Job<br/>export GCP_PROJECT=&lt;project_id&gt;# GCP region where the job should be submitted<br/>export REGION=&lt;region&gt;# The staging location for Dataproc<br/>export GCS_STAGING_LOCATION=gs://$STAGING_BUCKET/staging<br/>export SUBNET=&lt;subnet&gt; </span></pre><p id="e415" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在我们的例子中，红移到GCS需要先决条件中提到的<a class="ae jk" href="https://cloud.google.com/dataproc-serverless/docs/guides/bigquery-connector-spark-example" rel="noopener ugc nofollow" target="_blank">J</a>ar在类路径中可用。您可以将JAR文件存储在bucket上，我们将使用<code class="du lv lw lx lm b">JARS</code>环境变量添加它。</p><p id="af82" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了以AVRO文件格式导出红移数据，我们还需要已经包含在<code class="du lv lw lx lm b">bin/start.sh</code>中的spark-avro.jar</p><pre class="jm jn jo jp fd ll lm ln lo aw lp bi"><span id="a1d6" class="lq jr hi lm b fi lr ls l lt lu"># Path to the Spark Redshift JAR file<br/>export JARS=<!-- -->&lt;comma-seperated-gcs-bucket-location-containing-jar-file&gt;</span></pre><p id="894e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">4.执行红移到GCS Dataproc模板</p><p id="f0df" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">配置作业后，我们就可以触发它了。我们将运行<code class="du lv lw lx lm b">bin/start.sh</code>脚本，指定我们想要运行的模板和执行的参数值。</p><pre class="jm jn jo jp fd ll lm ln lo aw lp bi"><span id="a17f" class="lq jr hi lm b fi lr ls l lt lu">./bin/start.sh \<br/>-- --template=REDSHIFTTOGCS \<br/>--redshifttogcs.input.url="jdbc:redshift://[Redshift Endpoint]:[PORT]/&lt;dbname&gt;?user=&lt;username&gt;&amp;password=&lt;password&gt;" \<br/>--redshifttogcs.s3.tempdir="s3a://bucket-name/temp" \<br/>--redshifttogcs.input.table="table-name" \<br/>--redshifttogcs.iam.rolearn="arn:aws:iam::xxxxxx:role/Redshift-S3-Role" \<br/>--redshifttogcs.s3.accesskey="xxxxxxxx" \<br/>--redshifttogcs.s3.secretkey="xxxxxxxx" \<br/>--redshifttogcs.output.location="gs://bucket" \<br/>--redshifttogcs.output.mode=&lt;optional-write-mode&gt; \<br/>--redshifttogcs.output.format=&lt;output-write-format&gt; \<br/>--redshifttogcs.output.partitioncolumn=&lt;optional-output-partition-column-name&gt;</span></pre><p id="4f8e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">注意</strong>:提交作业会要求您启用Dataproc API，如果还没有启用的话。</p><p id="229a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">5.监控Spark批处理作业</p><p id="84ee" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">提交作业后，我们将能够在<a class="ae jk" href="https://console.cloud.google.com/dataproc/batches" rel="noopener ugc nofollow" target="_blank"> Dataproc Batches UI </a>中看到。从那里，我们可以查看作业的指标和日志。</p><h1 id="a0a7" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">参考</h1><ul class=""><li id="e07b" class="ko kp hi io b ip kq it kr ix ks jb kt jf ku jj kv kw kx ky bi translated"><a class="ae jk" href="https://cloud.google.com/dataproc-serverless/docs/overview" rel="noopener ugc nofollow" target="_blank"> Dataproc无服务</a></li><li id="652a" class="ko kp hi io b ip kz it la ix lb jb lc jf ld jj kv kw kx ky bi translated"><a class="ae jk" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> Dataproc模板库</a></li></ul></div></div>    
</body>
</html>