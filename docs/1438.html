<html>
<head>
<title>Apache Spark BigQuery Connector — Optimization tips &amp; example Jupyter Notebooks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark BigQuery连接器—优化技巧和Jupyter笔记本示例</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/apache-spark-bigquery-connector-optimization-tips-example-jupyter-notebooks-f17fd8476309?source=collection_archive---------0-----------------------#2020-05-21">https://medium.com/google-cloud/apache-spark-bigquery-connector-optimization-tips-example-jupyter-notebooks-f17fd8476309?source=collection_archive---------0-----------------------#2020-05-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="eb10" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">了解如何将BigQuery存储API与Apache Spark on Cloud Dataproc结合使用</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/df0d968d7e5d65f204367aed1a5f6149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TClXUkxA6gLadoUFcDn0jw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">BigQuery存储到BigQuery计算</figcaption></figure><p id="1600" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">Google <a class="ae kj" href="https://cloud.google.com/bigquery" rel="noopener ugc nofollow" target="_blank"> BigQuery </a>是Google Cloud完全托管的数据仓库，刚刚满10岁(<a class="ae kj" href="https://www.youtube.com/watch?v=O4_q2fQ1sJw" rel="noopener ugc nofollow" target="_blank">生日快乐BigQuery！！！</a>)。它的一个关键特性是它将计算和存储分开，最近这导致了<a class="ae kj" href="https://cloud.google.com/bigquery/docs/reference/storage" rel="noopener ugc nofollow" target="_blank"> BigQuery存储API </a>的开发，它允许你从其他平台大规模读取数据，如Apache Spark，在那里数据将被处理，而不需要首先将数据导出到谷歌云存储作为中间步骤。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kk"><img src="../Images/0cd811b45364911a11c156332485c804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jntSkef5VwRD6fRrci7k6w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">BigQuery存储API连接到Apache Spark、Apache Beam、Presto、TensorFlow和Pandas</figcaption></figure><p id="b838" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这种与其他平台集成的一些例子是Apache Spark(这将是本文的重点)、<a class="ae kj" href="https://prestosql.io/docs/current/connector/bigquery.html" rel="noopener ugc nofollow" target="_blank"> Presto </a>、<a class="ae kj" href="https://beam.apache.org/documentation/io/built-in/google-bigquery/#storage-api" rel="noopener ugc nofollow" target="_blank"> Apache Beam </a>、<a class="ae kj" href="https://www.tensorflow.org/io/tutorials/bigquery" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>和<a class="ae kj" href="https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas" rel="noopener ugc nofollow" target="_blank"> Pandas </a>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kl"><img src="../Images/d07ca987b8aeb6123465ad96d4e93e9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rR5yYEKCVSjfymjMdpnRmg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Apache Spark可以从BigQuery存储API并行读取多个数据流</figcaption></figure><p id="591d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">BigQuery存储API允许并行读取数据，这使得它非常适合Apache Spark这样的并行处理平台。</p><p id="bb96" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">使用构建在BigQuery存储API和BigQuery API之上的<a class="ae kj" href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector" rel="noopener ugc nofollow" target="_blank"> Apache Spark BigQuery连接器</a>，您现在可以将BigQuery视为从Apache Spark读取和写入数据的另一个来源。</p><h1 id="e231" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">阿帕奇，火花和JupyterLab</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es le"><img src="../Images/24077a4eac787800c3445671e8f65127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KnyIHUmxrCvrk-3p5URYlA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">云Dataproc、Apache Spark、Apache Spark BigQuery连接器和Jupyter笔记本如何连接</figcaption></figure><p id="69a4" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">Jupyter笔记本是开始学习如何使用<a class="ae kj" href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector" rel="noopener ugc nofollow" target="_blank"> Apache Spark BigQuery连接器</a>的好方法。</p><p id="c8de" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">你可以阅读这篇关于在Cloud Dataproc 上使用Apache Spark和Jupyter笔记本的文章，以进行设置，然后继续阅读如何使用连接器、如何优化工作的技巧，并查看GitHub上现有的Jupyter笔记本示例。</p><h1 id="8c97" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">设置Apache Spark BigQuery存储连接器</h1><p id="1867" class="pw-post-body-paragraph jn jo hi jp b jq lf ij js jt lg im jv jw lh jy jz ka li kc kd ke lj kg kh ki hb bi translated">一旦您的笔记本开始运行，您只需要在创建Spark会话时包含Apache Spark BigQuery存储连接器包。</p><pre class="iy iz ja jb fd lk ll lm ln aw lo bi"><span id="3eef" class="lp kn hi ll b fi lq lr l ls lt">from pyspark.sql import SparkSession<br/>spark = SparkSession.builder \<br/>  .appName('Optimize BigQuery Storage') \<br/>  .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.11:0.15.1-beta') \<br/>  .getOrCreate()</span></pre><p id="ce2a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">根据您运行的Scala版本，需要将工件名称更改为以下名称之一:</p><ul class=""><li id="3893" class="lu lv hi jp b jq jr jt ju jw lw ka lx ke ly ki lz ma mb mc bi translated"><strong class="jp hj">Scala 2.11:</strong>T0】</li><li id="e667" class="lu lv hi jp b jq mg jt mh jw mi ka mj ke mk ki lz ma mb mc bi translated"><strong class="jp hj">Scala 2.12:</strong>T1】</li></ul><p id="dd6b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">您可以查看连接器的<a class="ae kj" href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector/blob/master/CHANGES.md" rel="noopener ugc nofollow" target="_blank">发行说明，以检查包的最新版本。</a></p><h2 id="da65" class="lp kn hi bd ko ml mm mn ks mo mp mq kw jw mr ms ky ka mt mu la ke mv mw lc mx bi translated">正在读取BigQuery数据</h2><p id="5681" class="pw-post-body-paragraph jn jo hi jp b jq lf ij js jt lg im jv jw lh jy jz ka li kc kd ke lj kg kh ki hb bi translated">接下来，您只需将读取格式设置为<code class="du md me mf ll b">"bigquery"</code>，就可以将BigQuery表加载到Spark作业中了。这些例子将利用由马克·科恩创建的维基百科页面浏览量公共数据集。</p><pre class="iy iz ja jb fd lk ll lm ln aw lo bi"><span id="f2c4" class="lp kn hi ll b fi lq lr l ls lt">table = "bigquery-public-data.wikipedia.pageviews_2019"</span><span id="2c74" class="lp kn hi ll b fi my lr l ls lt">df = spark.read \<br/>  .format("bigquery") \<br/>  .option("table", table) \<br/>  .load()</span></pre><p id="c748" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">然而，由于Wikipedia页面视图表非常大，有2TB，您应该过滤Apache Spark作业实际需要的数据。</p><p id="dde6" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">由于<code class="du md me mf ll b">.load()</code>没有触发Spark作业，数据现在实际上不会被读取。当一个动作被调用时，数据将被读取，这就是我们稍后使用的<code class="du md me mf ll b">.show()</code>方法。</p><h1 id="bc43" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">过滤数据以优化Apache Spark作业</h1><p id="5044" class="pw-post-body-paragraph jn jo hi jp b jq lf ij js jt lg im jv jw lh jy jz ka li kc kd ke lj kg kh ki hb bi translated">为了在使用Apache Spark BigQuery存储连接器时优化Apache Spark作业的性能，这里有一些步骤向您展示如何只读取作业所需的数据。</p><h2 id="fa62" class="lp kn hi bd ko ml mm mn ks mo mp mq kw jw mr ms ky ka mt mu la ke mv mw lc mx bi translated">BigQuery分区筛选</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mz"><img src="../Images/8b51482c8bab05aae787eacc45faa9db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ng-p7jZlNy3doGMVPKmd4A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">API在读取器之间重新平衡记录，直到它们全部完成</figcaption></figure><p id="4369" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><a class="ae kj" href="https://cloud.google.com/bigquery/docs/partitioned-tables" rel="noopener ugc nofollow" target="_blank"> BigQuery表可以按照日期或整数进行分区</a>，方式与Hive、Parquet和ORC类似。wikipedia公共数据集使用日期分区，因此您可以将过滤器选项设置为仅读取7天的数据，而不是所有365天。</p><pre class="iy iz ja jb fd lk ll lm ln aw lo bi"><span id="0494" class="lp kn hi ll b fi lq lr l ls lt">table = "bigquery-public-data.wikipedia.pageviews_2019"</span><span id="aad0" class="lp kn hi ll b fi my lr l ls lt">df_wiki_pageviews = spark.read \<br/>  .format("bigquery") \<br/>  .option("table", table) \<br/>  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08'") \<br/>  .load()</span></pre><h2 id="f2ce" class="lp kn hi bd ko ml mm mn ks mo mp mq kw jw mr ms ky ka mt mu la ke mv mw lc mx bi translated">BigQuery列存储筛选</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es na"><img src="../Images/632c0bcc023cf20670eb05e4920295f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5TQFdyHEyuX2zhun01-VSg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Apache Spark只读取所需的列</figcaption></figure><p id="396b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">BigQuery使用类似于Apache Parquet和ORC的列存储。因此，您可以通过使用filter选项选择某些列来读取Spark作业所需的列。</p><pre class="iy iz ja jb fd lk ll lm ln aw lo bi"><span id="5de7" class="lp kn hi ll b fi lq lr l ls lt">table = "bigquery-public-data.wikipedia.pageviews_2019"</span><span id="d569" class="lp kn hi ll b fi my lr l ls lt">df_wiki_pageviews = spark.read \<br/>  .format("bigquery") \<br/>  .option("table", table) \<br/>  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08'") \<br/>  .load()</span><span id="6530" class="lp kn hi ll b fi my lr l ls lt">df_wiki_en = df_wiki_pageviews \<br/>  .select("title", "wiki", "views")</span><span id="f079" class="lp kn hi ll b fi my lr l ls lt">df_wiki_en.printSchema()</span></pre><h2 id="07a6" class="lp kn hi bd ko ml mm mn ks mo mp mq kw jw mr ms ky ka mt mu la ke mv mw lc mx bi translated">BigQuery行筛选</h2><p id="f1a0" class="pw-post-body-paragraph jn jo hi jp b jq lf ij js jt lg im jv jw lh jy jz ka li kc kd ke lj kg kh ki hb bi translated">BigQuery存储API支持过滤器的谓词下推，这意味着如果您稍后在Apache Spark作业的where语句中设置过滤器，它将尝试将过滤器推送到BigQuery。因此，下面的两个作业会产生相同的结果</p><p id="26fe" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">使用过滤器和地点</strong></p><pre class="iy iz ja jb fd lk ll lm ln aw lo bi"><span id="da43" class="lp kn hi ll b fi lq lr l ls lt">table = "bigquery-public-data.wikipedia.pageviews_2019"</span><span id="3467" class="lp kn hi ll b fi my lr l ls lt">df_wiki_pageviews = spark.read \<br/>  .format("bigquery") \<br/>  .option("table", table) \<br/>  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08'") \<br/>  .load()</span><span id="faee" class="lp kn hi ll b fi my lr l ls lt">df_wiki_en = df_wiki_pageviews \<br/>  .select("title", "wiki", "views") \<br/>  .where("views &gt; 10 AND wiki in ('en', 'en.m')")</span><span id="bcdf" class="lp kn hi ll b fi my lr l ls lt">df_wiki_en.show()</span></pre><p id="61c7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">仅使用过滤器</strong></p><pre class="iy iz ja jb fd lk ll lm ln aw lo bi"><span id="028a" class="lp kn hi ll b fi lq lr l ls lt">table = "bigquery-public-data.wikipedia.pageviews_2019"</span><span id="fa59" class="lp kn hi ll b fi my lr l ls lt">df_wiki_pageviews = spark.read \<br/>  .format("bigquery") \<br/>  .option("table", table) \<br/>  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08' AND views &gt; 10 AND wiki in ('en', 'en.m')") \<br/>  .load()</span><span id="e107" class="lp kn hi ll b fi my lr l ls lt">df_wiki_en = df_wiki_pageviews \<br/>  .select("title", "wiki", "views")</span><span id="4f03" class="lp kn hi ll b fi my lr l ls lt">df_wiki_en.show()</span></pre><h2 id="25a6" class="lp kn hi bd ko ml mm mn ks mo mp mq kw jw mr ms ky ka mt mu la ke mv mw lc mx bi translated">在Spark UI中查看工作绩效</h2><p id="e80e" class="pw-post-body-paragraph jn jo hi jp b jq lf ij js jt lg im jv jw lh jy jz ka li kc kd ke lj kg kh ki hb bi translated">当您使用来自BigQuery的数据运行作业时，您会希望查看Spark作业的性能。以此聚合作业为例:</p><pre class="iy iz ja jb fd lk ll lm ln aw lo bi"><span id="0a3d" class="lp kn hi ll b fi lq lr l ls lt">import pyspark.sql.functions as F</span><span id="1c1f" class="lp kn hi ll b fi my lr l ls lt">table = "bigquery-public-data.wikipedia.pageviews_2019"</span><span id="21f0" class="lp kn hi ll b fi my lr l ls lt">df_wiki_pageviews = spark.read \<br/>  .format("bigquery") \<br/>  .option("table", table) \<br/>  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08'") \<br/>  .load()</span><span id="db0f" class="lp kn hi ll b fi my lr l ls lt">df_wiki_en = df_wiki_pageviews \<br/>  .select("title", "wiki", "views") \<br/>  .where("views &gt; 10 AND wiki in ('en', 'en.m')")</span><span id="b6a4" class="lp kn hi ll b fi my lr l ls lt">df_wiki_en_totals = df_wiki_en \<br/>.groupBy("title") \<br/>.agg(F.sum('views').alias('total_views'))</span><span id="e222" class="lp kn hi ll b fi my lr l ls lt">df_wiki_en_totals.orderBy('total_views', ascending=False).show()</span></pre><p id="39d9" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">使用<a class="ae kj" href="https://cloud.google.com/dataproc/docs/concepts/accessing/dataproc-gateways" rel="noopener ugc nofollow" target="_blank">组件网关</a>特性，您可以轻松地访问Spark UI来查看任务是如何执行的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nb"><img src="../Images/a963f8fd6eae7050d76e37b254b10bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5H-X2L2SRpeEowqhfOYtQA.png"/></div></div></figure><p id="450a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">此处作业的这一阶段显示，创建了188个任务来从BigQuery存储中并行读取数据。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nc"><img src="../Images/5fdaa804ca1fab682e253e4529238dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2uM3GNQIhDt7h6kK_iHW0A.png"/></div></div></figure><h2 id="6eeb" class="lp kn hi bd ko ml mm mn ks mo mp mq kw jw mr ms ky ka mt mu la ke mv mw lc mx bi translated">设置最大并行度</h2><p id="b491" class="pw-post-body-paragraph jn jo hi jp b jq lf ij js jt lg im jv jw lh jy jz ka li kc kd ke lj kg kh ki hb bi translated">正如我们所看到的，其中一些任务是空的，如188个已完成任务的摘要指标所示。<strong class="jp hj"> </strong>我们可以通过设置read API 中的<code class="du md me mf ll b">maxParallelism</code> <a class="ae kj" href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector#properties" rel="noopener ugc nofollow" target="_blank">属性来匹配集群大小和设置<strong class="jp hj">，从而减少从BigQuery存储中读取的任务数量。</strong>对于大多数用例，尝试每400MB读取一个分区的默认设置应该足够了。</a></p><p id="ce53" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果您有一个总共有8个执行器核心的集群，您可以考虑将属性设置为<code class="du md me mf ll b">maxParallelism</code>属性，如下所示:</p><ul class=""><li id="75d0" class="lu lv hi jp b jq jr jt ju jw lw ka lx ke ly ki lz ma mb mc bi translated">8个总执行器核心* 7个分区天数= 56</li></ul><pre class="iy iz ja jb fd lk ll lm ln aw lo bi"><span id="0831" class="lp kn hi ll b fi lq lr l ls lt">table = "bigquery-public-data.wikipedia.pageviews_2019"</span><span id="89e3" class="lp kn hi ll b fi my lr l ls lt">df_wiki_pageviews = spark.read \<br/>  .format("bigquery") \<br/>  .option("table", table) \<br/>  .option("maxParallelism", 56) \<br/>  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08'") \<br/>  .load()</span></pre><p id="75ca" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">您应该用您的数据大小、BigQuery格式(分区与非分区)、过滤器选项和集群配置来测试这一点。</p><p id="ab3e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">目前，BigQuery存储API允许的最大并行度是1000。如果将<code class="du md me mf ll b">maxParallelism</code>属性值设置为大于1000，那么在一个会话中仍然只有1000个任务从1000多个流中读取数据。</p><h2 id="b1b4" class="lp kn hi bd ko ml mm mn ks mo mp mq kw jw mr ms ky ka mt mu la ke mv mw lc mx bi translated">在内存中缓存数据</h2><p id="37df" class="pw-post-body-paragraph jn jo hi jp b jq lf ij js jt lg im jv jw lh jy jz ka li kc kd ke lj kg kh ki hb bi translated">有些情况下，您可能希望将数据放在内存中，而不是每次都从BigQuery存储中读取数据来提高性能。</p><p id="85be" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">上面查找每个标题的总页面浏览量的作业将从BigQuery读取数据，并将过滤器推送到BigQuery。然后将在Apache Spark中计算聚合。</p><p id="aaf9" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">您可以修改作业，使其包含表中过滤数据的缓存，并进一步过滤wiki列上的数据，这些数据将由Apache Spark应用到内存中。</p><pre class="iy iz ja jb fd lk ll lm ln aw lo bi"><span id="08e6" class="lp kn hi ll b fi lq lr l ls lt">import pyspark.sql.functions as F</span><span id="fac2" class="lp kn hi ll b fi my lr l ls lt">table = "bigquery-public-data.wikipedia.pageviews_2019"</span><span id="240b" class="lp kn hi ll b fi my lr l ls lt">df_wiki_pageviews = spark.read \<br/>  .format("bigquery") \<br/>  .option("table", table) \<br/>  .option("filter", "datehour &gt;= '2019-01-01' AND datehour &lt; '2019-01-08'") \<br/>  .load()</span><span id="b1ca" class="lp kn hi ll b fi my lr l ls lt">df_wiki_all = df_wiki_pageviews \<br/>  .select("title", "wiki", "views") \<br/>  .where("views &gt; 10")</span><span id="bc0f" class="lp kn hi ll b fi my lr l ls lt">df_wiki_all.cache()</span><span id="e64a" class="lp kn hi ll b fi my lr l ls lt">df_wiki_en = df_wiki_all \<br/>  .where("wiki in ('en', 'en.m')")</span><span id="c16c" class="lp kn hi ll b fi my lr l ls lt">df_wiki_en_totals = df_wiki_en \<br/>.groupBy("title") \<br/>.agg(F.sum('views').alias('total_views'))</span><span id="3967" class="lp kn hi ll b fi my lr l ls lt">df_wiki_en_totals.orderBy('total_views', ascending=False).show()</span></pre><p id="d4bd" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果查看作业详细信息，您会看到该表作为作业的一部分被缓存。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nd"><img src="../Images/89e86515947e53a2fa413a5e43ad0418.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q5QRdn54-OC1GPdphLZI-A.png"/></div></div></figure><p id="8507" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">然后，您可以使用缓存过滤另一种wiki语言，而不是再次从BigQuery存储中读取数据。</p><pre class="iy iz ja jb fd lk ll lm ln aw lo bi"><span id="e3b2" class="lp kn hi ll b fi lq lr l ls lt">df_wiki_de = df_wiki_all \<br/>  .where("wiki in ('de', 'de.m')")</span><span id="148c" class="lp kn hi ll b fi my lr l ls lt">df_wiki_de_totals = df_wiki_de \<br/>.groupBy("title") \<br/>.agg(F.sum('views').alias('total_views'))</span><span id="0abf" class="lp kn hi ll b fi my lr l ls lt">df_wiki_de_totals.orderBy('total_views', ascending=False).show()</span></pre><p id="7340" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下一个作业的不同之处在于，现在从内存中读取表，因此运行速度更快。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ne"><img src="../Images/7d75c150f8f23b9626c41d8ce6761554.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wzmaejs-uRCQ0Rhg_sbubA.png"/></div></div></figure><p id="98ac" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">您可以通过运行以下命令来删除缓存</p><pre class="iy iz ja jb fd lk ll lm ln aw lo bi"><span id="693f" class="lp kn hi ll b fi lq lr l ls lt">df_wiki_all.unpersist()</span></pre><h1 id="719b" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">Jupyter笔记本示例</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nb"><img src="../Images/e6404be8302577125d2056a807c57f0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FlRqq9yDOt5kOaPbURFGAg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">BigQuery存储和Spark数据帧— Python Jupyter笔记本</figcaption></figure><p id="e04c" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">示例<a class="ae kj" href="https://github.com/GoogleCloudDataproc/cloud-dataproc/tree/master/notebooks" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本现已在官方Google Cloud data proc Github repo</a>上发布，介绍Apache Spark BigQuery存储连接器如何与Spark DataFrames、Spark SQL和Spark MLlib一起读写数据。</p><blockquote class="nf ng nh"><p id="e252" class="jn jo ni jp b jq jr ij js jt ju im jv nj jx jy jz nk kb kc kd nl kf kg kh ki hb bi translated"><strong class="jp hj">注意:这些笔记本被设计为与Python 3内核(不是PySpark内核)一起工作，因为这允许你创建你的Spark会话并包括Apache Spark BigQuery连接器</strong></p></blockquote><p id="172f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这些笔记本利用<code class="du md me mf ll b">spark.sql.repl.eagerEval</code>输出每一步数据帧的结果，而不需要使用<code class="du md me mf ll b">df.show()</code>，并且还改进了输出的格式。</p><ul class=""><li id="322d" class="lu lv hi jp b jq jr jt ju jw lw ka lx ke ly ki lz ma mb mc bi translated"><a class="ae kj" href="https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/master/notebooks/python/1.1.%20BigQuery%20Storage%20%26%20Spark%20DataFrames%20-%20Python.ipynb" rel="noopener ugc nofollow" target="_blank"> BigQuery存储&amp; Spark DataFrames笔记本</a></li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nm nn l"/></div></figure><ul class=""><li id="3371" class="lu lv hi jp b jq jr jt ju jw lw ka lx ke ly ki lz ma mb mc bi translated"><a class="ae kj" href="https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/master/notebooks/python/1.2.%20BigQuery%20Storage%20%26%20Spark%20SQL%20-%20Python.ipynb" rel="noopener ugc nofollow" target="_blank"> BigQuery存储&amp; Spark SQL </a></li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nm nn l"/></div></figure><ul class=""><li id="f9f9" class="lu lv hi jp b jq jr jt ju jw lw ka lx ke ly ki lz ma mb mc bi translated"><a class="ae kj" href="https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/master/notebooks/python/1.3.%20BigQuery%20Storage%20%26%20Spark%20MLlib%20-%20Python.ipynb" rel="noopener ugc nofollow" target="_blank"> BigQuery存储&amp; Spark MLlib </a></li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nm nn l"/></div></figure><h1 id="20f2" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">下一步是什么</h1><ul class=""><li id="e16f" class="lu lv hi jp b jq lf jt lg jw no ka np ke nq ki lz ma mb mc bi translated">如果您对其他Apache Spark笔记本示例有想法，请通过<a class="ae kj" rel="noopener" href="/@tfayyaz"> Medium (@tfayyaz) </a>或<a class="ae kj" href="http://twitter.com/tfayyaz" rel="noopener ugc nofollow" target="_blank"> Twitter (tfayyaz) </a>联系我们。</li><li id="c395" class="lu lv hi jp b jq mg jt mh jw mi ka mj ke mk ki lz ma mb mc bi translated">在评论中或者在<a class="ae kj" href="https://stackoverflow.com/questions/tagged/google-cloud-dataproc" rel="noopener ugc nofollow" target="_blank"> google-cloud-dataproc </a>标签下的Stackoverflow上提问。</li><li id="fbb0" class="lu lv hi jp b jq mg jt mh jw mi ka mj ke mk ki lz ma mb mc bi translated">请在GitHub上留下关于<a class="ae kj" href="https://github.com/GoogleCloudDataproc/spark-bigquery-connector" rel="noopener ugc nofollow" target="_blank"> Apache Spark BigQuery连接器</a>的任何反馈或问题</li></ul></div></div>    
</body>
</html>