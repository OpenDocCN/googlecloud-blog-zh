<html>
<head>
<title>Moving Data with Apache Sqoop in Google Cloud Dataproc</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Google Cloud Dataproc中使用Apache Sqoop移动数据</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/moving-data-with-apache-sqoop-in-google-cloud-dataproc-4056b8fa2600?source=collection_archive---------0-----------------------#2018-06-01">https://medium.com/google-cloud/moving-data-with-apache-sqoop-in-google-cloud-dataproc-4056b8fa2600?source=collection_archive---------0-----------------------#2018-06-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/434b648d1035f5cbc50f8a70dcc83d3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*sd3yiSAX6WVnroTeBEN6ag.jpeg"/></div></figure><p id="957a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">有关系数据库吗？—想用<a class="ae jk" href="https://sqoop.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Sqoop </a>把这个数据库导出到<a class="ae jk" href="https://cloud.google.com/storage/" rel="noopener ugc nofollow" target="_blank">云存储</a>、<a class="ae jk" href="https://cloud.google.com/bigquery/" rel="noopener ugc nofollow" target="_blank"> BigQuery </a>，或者<a class="ae jk" href="https://hive.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Hive </a>？—想通过<a class="ae jk" href="https://cloud.google.com/dataproc/" rel="noopener ugc nofollow" target="_blank"> Cloud Dataproc </a>来完成这一切，这样您就只需为您使用的东西付费了？</p><p id="099a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">希望你回答<strong class="io hj">是</strong>因为这就是这篇文章的内容！</p><p id="8a58" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Cloud Dataproc非常棒，因为它可以快速创建一个Hadoop集群，然后您可以使用它来运行您的Hadoop作业(特别是本文中的Sqoop作业)，然后一旦您的作业完成，您就可以立即删除集群。这是利用Dataproc短暂的按使用付费模型削减成本的一个很好的例子，因为现在您可以快速创建/删除hadoop集群，再也不会让集群闲置了！</p><p id="db2b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里是关于使用Sqoop的快速独家新闻(看我在那里做了什么😏)移动数据:</p><ul class=""><li id="77e0" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated">Sqoop将数据从关系数据库系统或大型机导入HDFS (Hadoop分布式文件系统)。</li><li id="0056" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">在Dataproc Hadoop集群上运行Sqoop可以让您访问内置的云存储连接器，该连接器允许您使用云存储gs:// file前缀，而不是Hadoop hdfs:// file前缀。</li><li id="0525" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">前面两点意味着你可以使用Sqoop将数据直接导入云存储，完全跳过HDFS！</li><li id="bc34" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">一旦你的数据在云存储中，你可以简单地使用Cloud SDK <a class="ae jk" href="https://cloud.google.com/bigquery/docs/bq-command-line-tool" rel="noopener ugc nofollow" target="_blank"> bq命令行工具</a>将数据<a class="ae jk" href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#bigquery-import-gcs-file-cli" rel="noopener ugc nofollow" target="_blank">加载到BigQuery中。<strong class="io hj">或者，您可以通过将</strong><em class="jz">Hive . metastore . warehouse . dir</em><strong class="io hj">指向一个</strong></a><a class="ae jk" href="https://cloud.google.com/storage/docs/creating-buckets" rel="noopener ugc nofollow" target="_blank"><strong class="io hj">GCS bucket</strong></a><strong class="io hj">，让Sqoop将数据直接导入到您的Dataproc集群的Hive仓库中，该仓库可以基于云存储，而不是HDFS。</strong></li></ul><p id="5dd2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">您可以使用两种不同的方法向集群提交Dataproc作业:</p><h2 id="6456" class="ka kb hi bd kc kd ke kf kg kh ki kj kk ix kl km kn jb ko kp kq jf kr ks kt ku bi translated"><strong class="ak">方法一。)手动Dataproc任务提交</strong></h2><ul class=""><li id="1900" class="jl jm hi io b ip kv it kw ix kx jb ky jf kz jj jq jr js jt bi translated"><a class="ae jk" href="https://cloud.google.com/dataproc/docs/guides/create-cluster" rel="noopener ugc nofollow" target="_blank">创建Dataproc集群</a></li><li id="8795" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><a class="ae jk" href="https://cloud.google.com/dataproc/docs/guides/submit-job" rel="noopener ugc nofollow" target="_blank">提交Dataproc作业</a></li><li id="48c2" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><a class="ae jk" href="https://cloud.google.com/dataproc/docs/guides/manage-cluster#deleting_a_cluster" rel="noopener ugc nofollow" target="_blank">当作业完成时删除Dataproc集群</a></li></ul><h2 id="dcb9" class="ka kb hi bd kc kd ke kf kg kh ki kj kk ix kl km kn jb ko kp kq jf kr ks kt ku bi translated"><strong class="ak">方法二。)使用工作流模板自动提交Dataproc任务</strong></h2><ul class=""><li id="1009" class="jl jm hi io b ip kv it kw ix kx jb ky jf kz jj jq jr js jt bi translated">创建一个<a class="ae jk" href="https://cloud.google.com/dataproc/docs/concepts/workflows/overview" rel="noopener ugc nofollow" target="_blank">工作流模板</a>，该模板自动执行之前的3个手动步骤(创建、提交、删除)。</li></ul><p id="f193" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用手动作业提交方法更容易排除作业错误，因为您可以控制何时删除集群。一旦您准备好在生产中运行作业，使用工作流模板的自动化方法是理想的，因为它负责集群创建、作业提交和删除。这两种方法实际上非常类似于设置，下面会有更详细的介绍。</p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h1 id="c780" class="lh kb hi bd kc li lj lk kg ll lm ln kk lo lp lq kn lr ls lt kq lu lv lw kt lx bi translated"><strong class="ak">等等！在你阅读之前… </strong></h1><p id="17ce" class="pw-post-body-paragraph im in hi io b ip kv ir is it kw iv iw ix ly iz ja jb lz jd je jf ma jh ji jj hb bi translated">下面的例子演示了使用Sqoop连接到MySQL数据库。您可以通过在线<a class="ae jk" href="https://cloud.google.com/sql/docs/mysql/quickstart" rel="noopener ugc nofollow" target="_blank">quick start for Cloud SQL for MySQL</a>在GCP快速轻松地创建自己的测试MySQL数据库。如果您完成了快速入门，不要忘记使用<a class="ae jk" href="https://github.com/GoogleCloudPlatform/dataproc-initialization-actions/tree/master/cloud-sql-proxy" rel="noopener ugc nofollow" target="_blank">云SQL代理初始化操作</a>创建Dataproc集群，以便您可以轻松连接到MySQL数据库。最后，通过在完成后清理数据库来避免产生费用。</p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h1 id="3015" class="lh kb hi bd kc li lj lk kg ll lm ln kk lo lp lq kn lr ls lt kq lu lv lw kt lx bi translated">手动提交Dataproc作业的3个步骤</h1><p id="7775" class="pw-post-body-paragraph im in hi io b ip kv ir is it kw iv iw ix ly iz ja jb lz jd je jf ma jh ji jj hb bi translated"><strong class="io hj"> 1。创建一个Dataproc集群</strong></p><p id="d368" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">gcloud工具的<a class="ae jk" href="https://cloud.google.com/dataproc/docs/guides/create-cluster" rel="noopener ugc nofollow" target="_blank"> Dataproc集群创建命令</a>将默认创建一个主节点虚拟机(虚拟机)和两个工作节点虚拟机。所有3个节点虚拟机都是n1型标准4机器。为简单起见，下面的示例命令使用了最少的必要标志。<strong class="io hj">注意:使用最后一个</strong> <code class="du mb mc md me b">--properties</code> <strong class="io hj">标志是为了用云存储位置</strong> <code class="du mb mc md me b">gs://&lt;GCS_BUCKET&gt;/hive-warehouse</code> <strong class="io hj">覆盖默认的集群上的Hive仓库目录(hdfs:///user/hive/warehouse)。通过将这个Hive仓库目录</strong> <code class="du mb mc md me b">hive.metastore.warehouse.dir</code> <strong class="io hj">指向云存储，可以持久化所有Hive数据(即使删除了Dataproc集群)。</strong></p><ul class=""><li id="1526" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><em class="jz">如果您正在连接到本地托管的MySQL数据库，因此不需要</em> <a class="ae jk" href="https://cloud.google.com/sql/docs/mysql/sql-proxy" rel="noopener ugc nofollow" target="_blank"> <em class="jz">云SQL代理</em> </a> <em class="jz">，请使用下面的命令创建您的集群。确保将集群分配给在您的GCP环境和本地环境之间共享的</em> <a class="ae jk" href="https://cloud.google.com/vpc/" rel="noopener ugc nofollow" target="_blank"> <em class="jz"> VPC网络</em> </a> <em class="jz">。</em></li></ul><pre class="mf mg mh mi fd mj me mk ml aw mm bi"><span id="971a" class="ka kb hi me b fi mn mo l mp mq">gcloud dataproc clusters create &lt;<strong class="me hj">CLUSTER_NAME</strong>&gt; --zone=&lt;<strong class="me hj">ZONE</strong>&gt; --network=&lt;<strong class="me hj">VPC_NETWORK</strong>&gt; --properties=hive:hive.metastore.warehouse.dir=gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/hive-warehouse</span></pre><ul class=""><li id="724a" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><em class="jz">如果您使用</em> <a class="ae jk" href="https://cloud.google.com/sql/docs/mysql/sql-proxy" rel="noopener ugc nofollow" target="_blank"> <em class="jz">云SQL代理</em> </a> <em class="jz">连接到云SQL-MySQL数据库，请使用下面的命令创建您的集群。</em> <strong class="io hj"> <em class="jz">注意:由于默认的MySQL端口3306已经被Dataproc的默认Hive元数据MySQL服务器占用，所以需要为&lt;端口&gt;选择一个3306以外的值(如3307)。</em> </strong></li></ul><pre class="mf mg mh mi fd mj me mk ml aw mm bi"><span id="358b" class="ka kb hi me b fi mn mo l mp mq">gcloud dataproc clusters create &lt;<strong class="me hj">CLUSTER_NAME</strong>&gt; --zone=&lt;<strong class="me hj">ZONE</strong>&gt; --scopes=default,sql-admin --initialization-actions=gs://dataproc-initialization-actions/cloud-sql-proxy/cloud-sql-proxy.sh --properties=hive:hive.metastore.warehouse.dir=gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/hive-warehouse --metadata=enable-cloud-sql-hive-metastore=false --metadata=additional-cloud-sql-instances=&lt;<strong class="me hj">PROJECT_ID</strong>&gt;:&lt;<strong class="me hj">REGION</strong>&gt;:&lt;<a class="ae jk" href="https://cloud.google.com/sql/docs/mysql/instance-info" rel="noopener ugc nofollow" target="_blank"><strong class="me hj">SQL_INSTANCE_NAME</strong></a>&gt;=tcp:&lt;<strong class="me hj">PORT</strong>&gt; </span></pre><p id="2caf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 2。提交一个运行</strong> <a class="ae jk" href="https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_literal_sqoop_import_literal" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj"> Sqoop导入工具</strong> </a>的dataproc hadoop作业</p><ul class=""><li id="a544" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><em class="jz">如果您希望Sqoop将您的数据库表作为Avro文件导入云存储，请提交下面的作业命令，然后您将使用bq工具</em> <a class="ae jk" href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#bigquery-import-gcs-file-cli" rel="noopener ugc nofollow" target="_blank"> <em class="jz">加载到BigQuery </em> </a> <em class="jz">中。</em></li></ul><pre class="mf mg mh mi fd mj me mk ml aw mm bi"><span id="63b6" class="ka kb hi me b fi mn mo l mp mq">gcloud dataproc jobs submit hadoop --cluster=&lt;<strong class="me hj">CLUSTER_NAME</strong>&gt; --class=org.apache.sqoop.Sqoop --jars=gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/sqoop-1.4.7-hadoop260.jar,gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/avro-tools-1.8.2.jar,file:///usr/share/java/mysql-connector-java-5.1.42.jar -- import -Dmapreduce.job.user.classpath.first=true --connect=jdbc:mysql://&lt;<strong class="me hj">HOSTNAME</strong>&gt;:&lt;<strong class="me hj">PORT</strong>&gt;/&lt;<strong class="me hj">DATABASE_NAME</strong>&gt; --username=&lt;<strong class="me hj">DATABASE_USERNAME</strong>&gt; --password-file=gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/passwordFile.txt --target-dir=gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/mysql_output --table=&lt;<strong class="me hj">TABLE</strong>&gt; --as-avrodatafile</span></pre><ul class=""><li id="2bc3" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><em class="jz">如果您想让Sqoop将您的数据库表导入到一个Hive表中，提交下面的job命令。</em> <strong class="io hj"> <em class="jz">注意:下面的命令和上面的一样，只是在</em> </strong> <code class="du mb mc md me b">— -jars=</code> <strong class="io hj"> <em class="jz">参数中多加了一个jar </em> </strong> <code class="du mb mc md me b">file:///usr/lib/hive/lib/hive-exec.jar</code> <strong class="io hj"> <em class="jz">，用</em> </strong> <code class="du mb mc md me b">--hive-import</code>替换了  <code class="du mb mc md me b">--as-avrodatafile</code> <strong class="io hj"> <em class="jz">，并去掉了<code class="du mb mc md me b">--target-dir</code>标志。</em></strong></li></ul><pre class="mf mg mh mi fd mj me mk ml aw mm bi"><span id="8198" class="ka kb hi me b fi mn mo l mp mq">gcloud dataproc jobs submit hadoop --cluster=&lt;<strong class="me hj">CLUSTER_NAME</strong>&gt; --class=org.apache.sqoop.Sqoop --jars=gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/sqoop-1.4.7-hadoop260.jar,gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/avro-tools-1.8.2.jar,file:///usr/share/java/mysql-connector-java-5.1.42.jar,file:///usr/lib/hive/lib/hive-exec.jar -- import -Dmapreduce.job.user.classpath.first=true --connect=jdbc:mysql://&lt;<strong class="me hj">HOSTNAME</strong>&gt;:&lt;<strong class="me hj">PORT</strong>&gt;/&lt;<strong class="me hj">DATABASE_NAME</strong>&gt; --username=&lt;<strong class="me hj">DATABASE_USERNAME</strong>&gt; --password-file=gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/passwordFile.txt --table=&lt;<strong class="me hj">TABLE</strong>&gt; --hive-import</span></pre><p id="b3c9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 3。一旦Dataproc任务完成，删除集群</strong></p><pre class="mf mg mh mi fd mj me mk ml aw mm bi"><span id="8bdb" class="ka kb hi me b fi mn mo l mp mq">gcloud dataproc clusters delete &lt;<strong class="me hj">CLUSTER_NAME</strong>&gt;</span></pre></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h1 id="a1c8" class="lh kb hi bd kc li lj lk kg ll lm ln kk lo lp lq kn lr ls lt kq lu lv lw kt lx bi translated">仔细看看步骤2</h1><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/99212497900f4f1356d44fd03aa47e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*4JZ7IhpOBzsTm35Fbdlqwg.jpeg"/></div><figcaption class="ms mt et er es mu mv bd b be z dx translated">近距离观察</figcaption></figure><p id="8c02" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">gcloud dataproc jobs submit hadoop</code>←该命令的第一部分是调用<a class="ae jk" href="https://cloud.google.com/sdk/gcloud/reference" rel="noopener ugc nofollow" target="_blank"> gcloud工具</a>的地方。gcloud工具有很多功能，所以更具体地说，您调用的是<a class="ae jk" href="https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit/hadoop" rel="noopener ugc nofollow" target="_blank"> hadoop </a>作业提交命令。该命令嵌套在gcloud工具中的几层命令组下:<a class="ae jk" href="https://cloud.google.com/sdk/gcloud/reference/beta/dataproc" rel="noopener ugc nofollow" target="_blank"> dataproc组</a>-&gt;-<a class="ae jk" href="https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs" rel="noopener ugc nofollow" target="_blank">作业组</a>-&gt;-<a class="ae jk" href="https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit" rel="noopener ugc nofollow" target="_blank">提交组</a> - &gt; <a class="ae jk" href="https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit/hadoop" rel="noopener ugc nofollow" target="_blank"> hadoop命令</a>。</p><p id="e33c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">--cluster</code> ←这个gcloud标志指定将作业提交到哪个dataproc集群。</p><p id="fe0a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">--class</code> ←这个gcloud标志指定了您希望Dataproc作业运行的主类(org.apache.sqoop.Sqoop)。在幕后，Dataproc调用Hadoop工具并运行这个主类，就像<a class="ae jk" href="https://github.com/apache/sqoop/blob/11c83f68386add243762929ecf7f6f25a99efbf4/bin/sqoop#L101" rel="noopener ugc nofollow" target="_blank"> Sqoop命令行工具</a>一样。<strong class="io hj">注意:确保在</strong> <code class="du mb mc md me b">--jars</code> <strong class="io hj">标志中包含主类的jar文件的路径(例如</strong> <code class="du mb mc md me b">--jars=gs://&lt;GCS_BUCKET&gt;/sqoop-1.4.7-hadoop260.jar</code> <strong class="io hj">)。</strong></p><p id="96bb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">--jars</code>←这个gcloud标志指定了一个逗号分隔的jar文件列表<strong class="io hj">路径</strong>(路径应该指向GCS或Dataproc集群中的文件)。这些jar文件被提供给Dataproc集群中的MapReduce和driver类路径。要在Dataproc中运行Sqoop，您需要提供以下jar文件:</p><ul class=""><li id="89bc" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated">最新的Sqoop版本(<a class="ae jk" href="http://central.maven.org/maven2/org/apache/sqoop/sqoop/1.4.7/sqoop-1.4.7-hadoop260.jar" rel="noopener ugc nofollow" target="_blank">Sqoop-1 . 4 . 7-Hadoop 260 . jar</a>)(<a class="ae jk" href="https://mvnrepository.com/artifact/org.apache.sqoop/sqoop" rel="noopener ugc nofollow" target="_blank">Maven站点</a>查看最新版本)</li><li id="8391" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">JDBC驱动程序需要连接到您的数据库。对于MySQL驱动程序，只需将文件路径添加到存在于Dataproc集群中的MySQL驱动程序(file:///usr/share/Java/MySQL-connector-Java-5 . 1 . 42 . jar)。如果您连接到另一个数据库，例如DB2，那么包括该数据库所必需的DB2 JDBC驱动程序。</li><li id="821b" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">以指定的输出格式序列化数据所需的任何jar文件。在本例中，您将导入的MySQL数据存储为Avro文件，因此您包括了Avro jar(<a class="ae jk" href="https://repo1.maven.org/maven2/org/apache/avro/avro-tools/1.8.2/avro-tools-1.8.2.jar" rel="noopener ugc nofollow" target="_blank">Avro-tools-1 . 8 . 2 . jar</a>)(<a class="ae jk" href="https://mvnrepository.com/artifact/org.apache.avro/avro-tools" rel="noopener ugc nofollow" target="_blank">Maven站点</a>以检查最新版本)。<strong class="io hj">注意:Sqoop工具本身要求Avro版本至少为1.8.0才能正常运行，因此即使您不打算以Avro格式存储数据，Avro jar也是必要的。</strong></li><li id="630f" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">(可选)已经存在于Dataproc中的配置单元驱动程序(file:///usr/lib/Hive/lib/Hive-exec . jar)。<strong class="io hj">注意:只有当您使用Sqoop将数据直接导入Dataproc的Hive仓库时，才需要这个Hive jar。</strong></li></ul><p id="8d48" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">-- import</code>←单个<code class="du mb mc md me b">--</code> gcloud参数将左边的gcloud特定标志和右边的hadoop作业参数分开。既然您将Sqoop主类传递给了hadoop工具(通过<code class="du mb mc md me b">--class</code>标志)，那么您还必须指定您希望主类运行的Sqoop工具(在本例中为<code class="du mb mc md me b">import</code>)。在<code class="du mb mc md me b">import</code>之后，您为<a class="ae jk" href="https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_literal_sqoop_import_literal" rel="noopener ugc nofollow" target="_blank"> Sqoop导入工具</a>指定所有参数。</p><p id="42d8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">-Dmapreduce.job.user.classpath.first=true</code>←这个Hadoop参数对于指示Hadoop优先使用用户提供的jar(通过<code class="du mb mc md me b">--jars</code>标志传递)而不是集群中包含的默认jar是必要的。Sqoop需要Avro的1.8.0版本，而Avro的本地Dataproc版本在撰写本文时是1.7.7。将这个参数设置为<code class="du mb mc md me b">true</code>将会给Sqoop你在<code class="du mb mc md me b">--jars</code>列表中传递的Avro jar的正确版本。</p><p id="a036" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">-—connect=</code>←这个Sqoop参数指定了JDBC连接字符串:<em class="jz"> jdbc:mysql:// &lt;主机名&gt; : &lt;端口&gt; / &lt;数据库名&gt;。</em> <strong class="io hj">注意:如果您使用云SQL代理连接到您的数据库，请确保主机名设置为“localhost ”,并且您使用的端口与您创建Dataproc集群</strong>时使用的代理端口相同(例如，确保集群创建参数<code class="du mb mc md me b">--metadata=additional-cloud-sql-instances=&lt;PROJECT_ID&gt;:&lt;REGION&gt;:&lt;<a class="ae jk" href="https://cloud.google.com/sql/docs/mysql/instance-info" rel="noopener ugc nofollow" target="_blank">SQL_INSTANCE_NAME</a>&gt;=tcp:&lt;<strong class="io hj">PORT</strong>&gt;</code>和您的Sqoop连接参数<code class="du mb mc md me b">--connect=jdbc:mysql://&lt;HOSTNAME&gt;:&lt;<strong class="io hj">PORT</strong>&gt;/&lt;DATABASE_NAME&gt;</code>共享同一个端口)。</p><p id="18bc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">--username=</code>←这个Sqoop参数指定在向关系数据库进行身份验证时使用的用户名。</p><p id="09c6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">--password-file=</code>←这个Sqoop参数指定了GCS (Google Cloud Storage)中的一个文件的路径，该文件包含您的数据库的密码。<strong class="io hj">警告:不要使用</strong> <code class="du mb mc md me b">--password</code> <strong class="io hj">选项而不是</strong> <code class="du mb mc md me b">--password-file</code> <strong class="io hj">选项，因为这将使您的密码暴露在日志中。</strong></p><p id="83a0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">--target-dir=</code>←传统上，当在Hadoop中运行Sqoop时，您会将这个Sqoop参数设置为HDFS (hdfs://)目录路径，但是由于您使用的是Dataproc，您可以利用内置的GCS连接器，将其设置为GCS (gs://)目录路径。这意味着Sqoop会将导入的数据存储在您指定的GCS位置，完全跳过HDFS。</p><p id="4baa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">--table=</code>←这个Sqoop参数指定了要导入的数据库表。</p><p id="9200" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">--as-avrodatafile</code>←指定此Sqoop标志，以Avro文件格式存储所有导入的数据。Avro格式的优点是既能以二进制格式压缩数据，又能将表模式存储在同一个文件中。</p><p id="0f64" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du mb mc md me b">--hive-import</code> ←指定这个Sqoop标志，将所有导入的数据存储到一个Hive表中。<strong class="io hj">注意:当您使用</strong> <code class="du mb mc md me b">--hive-import</code> <strong class="io hj">标志时，请确保您的Dataproc集群是使用</strong> <code class="du mb mc md me b">--properties=hive:hive.metastore.warehouse.dir=gs://&lt;GCS_BUCKET&gt;/hive-warehouse</code> <strong class="io hj">标志创建的，这样您就可以在GCS中持久化您的配置单元数据。</strong></p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h1 id="13f9" class="lh kb hi bd kc li lj lk kg ll lm ln kk lo lp lq kn lr ls lt kq lu lv lw kt lx bi translated">使用工作流模板自动提交Dataproc任务的4个步骤</h1><p id="7af2" class="pw-post-body-paragraph im in hi io b ip kv ir is it kw iv iw ix ly iz ja jb lz jd je jf ma jh ji jj hb bi translated"><strong class="io hj"> 1。创建工作流程</strong></p><ul class=""><li id="28ae" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><em class="jz">创建工作流模板本身不会创建云Dataproc集群，也不会创建和提交作业。模板只是一组指令。只有当工作流模板被</em> <strong class="io hj"> <em class="jz">实例化</em> </strong> <em class="jz">时，集群和作业才会被创建和执行(参见步骤4)。</em></li></ul><pre class="mf mg mh mi fd mj me mk ml aw mm bi"><span id="f1df" class="ka kb hi me b fi mn mo l mp mq">gcloud beta dataproc workflow-templates create &lt;<strong class="me hj">TEMPLATE_ID</strong>&gt;</span></pre><p id="12c4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 2。添加工作流集群创建属性</strong></p><ul class=""><li id="92e4" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><em class="jz">为了简单起见，许多集群属性被故意省略了</em></li></ul><pre class="mf mg mh mi fd mj me mk ml aw mm bi"><span id="c55d" class="ka kb hi me b fi mn mo l mp mq">gcloud beta dataproc workflow-templates set-managed-cluster &lt;<strong class="me hj">TEMPLATE_ID</strong>&gt; --zone=&lt;<strong class="me hj">ZONE</strong>&gt; --cluster-name=&lt;<strong class="me hj">CLUSTER_NAME</strong>&gt;</span></pre><p id="7ed4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 3。将作业添加到您的工作流中，以便在集群上运行</strong></p><ul class=""><li id="80af" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><em class="jz">如果您希望Sqoop将您的数据库表作为Avro文件导入到云存储中，请将下面的作业添加到您的工作流中，然后您将使用bq工具</em> <a class="ae jk" href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#bigquery-import-gcs-file-cli" rel="noopener ugc nofollow" target="_blank"> <em class="jz">加载到BigQuery </em> </a> <em class="jz">中。</em></li></ul><pre class="mf mg mh mi fd mj me mk ml aw mm bi"><span id="1954" class="ka kb hi me b fi mn mo l mp mq">gcloud beta dataproc workflow-templates add-job hadoop --step-id=&lt;<strong class="me hj">STEP_ID</strong>&gt; --workflow-template=&lt;<strong class="me hj">TEMPLATE_ID</strong>&gt; --class=org.apache.sqoop.Sqoop --jars=gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/sqoop-1.4.7-hadoop260.jar,gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/avro-tools-1.8.2.jar,file:///usr/share/java/mysql-connector-java-5.1.42.jar -- import -Dmapreduce.job.user.classpath.first=true --connect=jdbc:mysql://&lt;<strong class="me hj">HOSTNAME</strong>&gt;:&lt;<strong class="me hj">PORT</strong>&gt;/&lt;<strong class="me hj">DATABASE_NAME</strong>&gt; --username=sqoop --password-file=gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/passwordFile.txt --target-dir=gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/mysql_output --table=&lt;<strong class="me hj">TABLE</strong>&gt; --as-avrodatafile</span></pre><ul class=""><li id="764a" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><em class="jz">如果您希望Sqoop将您的数据库表导入到配置单元表中，请将下面的作业添加到您的工作流中。</em> <strong class="io hj"> <em class="jz">注意:下面的命令在</em> </strong> <code class="du mb mc md me b">— -jars=</code> <strong class="io hj"> <em class="jz">参数中增加了一个jar </em> </strong> <code class="du mb mc md me b">file:///usr/lib/hive/lib/hive-exec.jar</code> <strong class="io hj"> <em class="jz">并把</em> </strong> <code class="du mb mc md me b">--as-avrodatafile</code> <strong class="io hj"> <em class="jz">替换为</em> </strong> <code class="du mb mc md me b">--hive-import</code></li></ul><pre class="mf mg mh mi fd mj me mk ml aw mm bi"><span id="3d11" class="ka kb hi me b fi mn mo l mp mq">gcloud beta dataproc workflow-templates add-job hadoop --step-id=&lt;<strong class="me hj">STEP_ID</strong>&gt; --workflow-template=&lt;<strong class="me hj">TEMPLATE_ID</strong>&gt; --class=org.apache.sqoop.Sqoop --jars=gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/sqoop-1.4.7-hadoop260.jar,gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/avro-tools-1.8.2.jar,file:///usr/share/java/mysql-connector-java-5.1.42.jar,file:///usr/lib/hive/lib/hive-exec.jar -- import -Dmapreduce.job.user.classpath.first=true --connect=jdbc:mysql://&lt;<strong class="me hj">HOSTNAME</strong>&gt;:&lt;<strong class="me hj">PORT</strong>&gt;/&lt;<strong class="me hj">DATABASE_NAME</strong>&gt; --username=&lt;<strong class="me hj">DATABASE_USERNAME</strong>&gt; --password-file=gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/passwordFile.txt --table=&lt;<strong class="me hj">TABLE</strong>&gt; --hive-import</span></pre><p id="da58" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 4。实例化/运行工作流</strong></p><ul class=""><li id="336f" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><em class="jz">支持模板的多个(同时)实例化。</em></li></ul><pre class="mf mg mh mi fd mj me mk ml aw mm bi"><span id="7cfa" class="ka kb hi me b fi mn mo l mp mq">gcloud beta dataproc workflow-templates instantiate &lt;<strong class="me hj">TEMPLATE_ID</strong>&gt;</span></pre></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h1 id="3af5" class="lh kb hi bd kc li lj lk kg ll lm ln kk lo lp lq kn lr ls lt kq lu lv lw kt lx bi translated">将您的Sqoop-Import数据加载到BigQuery中</h1><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/fa5c033c1ace48d3ee39de60e96a93c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*g28KzVazHM0vNf5Et0S-dg.png"/></div></figure><p id="a73e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果您使用Sqoop将数据库表导入云存储，您可以使用<a class="ae jk" href="https://cloud.google.com/bigquery/docs/bq-command-line-tool" rel="noopener ugc nofollow" target="_blank"> bq命令行工具</a>简单地<a class="ae jk" href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#bigquery-import-gcs-file-cli" rel="noopener ugc nofollow" target="_blank">将其加载到BigQuery </a>:</p><pre class="mf mg mh mi fd mj me mk ml aw mm bi"><span id="be68" class="ka kb hi me b fi mn mo l mp mq">bq load --source_format=AVRO &lt;<strong class="me hj">YOUR_DATASET</strong>&gt;.&lt;<strong class="me hj">YOUR_TABLE</strong>&gt; gs://&lt;<strong class="me hj">GCS_BUCKET</strong>&gt;/mysql_output/*.avro</span></pre></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h1 id="f4b0" class="lh kb hi bd kc li lj lk kg ll lm ln kk lo lp lq kn lr ls lt kq lu lv lw kt lx bi translated">使用Dataproc配置单元作业查询配置单元表</h1><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/527512cbec5210217f66bb2d0f190a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*55-nhaKWnnOgvPdS36w5Kg.png"/></div></figure><p id="7857" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果您使用Sqoop将您的数据库表导入到Dataproc中的Hive，那么您可以通过向Dataproc集群提交一个<a class="ae jk" href="https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit/hive" rel="noopener ugc nofollow" target="_blank"> Hive作业</a>,在您的Hive仓库上运行SQL <a class="ae jk" href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-SQLOperations" rel="noopener ugc nofollow" target="_blank">查询</a>:</p><pre class="mf mg mh mi fd mj me mk ml aw mm bi"><span id="3c69" class="ka kb hi me b fi mn mo l mp mq">gcloud dataproc jobs submit hive --cluster=&lt;<strong class="me hj">CLUSTER_NAME</strong>&gt; -e="SELECT * FROM &lt;<strong class="me hj">TABLE</strong>&gt;"</span></pre><p id="0f31" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">注意:确保在Dataproc集群上运行这些配置单元作业，该集群的默认配置单元仓库目录指向包含您的Sqoop导出数据的同一个GCS bucket(例如，您的集群应该使用</strong> <code class="du mb mc md me b">--properties=hive:hive.metastore.warehouse.dir=gs://&lt;<strong class="io hj">GCS_BUCKET</strong>&gt;/hive-warehouse</code> <strong class="io hj">创建)。</strong></p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h2 id="0cc7" class="ka kb hi bd kc kd ke kf kg kh ki kj kk ix kl km kn jb ko kp kq jf kr ks kt ku bi translated">希望这份指南能为你提供许多有价值的数据！</h2></div></div>    
</body>
</html>