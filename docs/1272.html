<html>
<head>
<title>Streaming pipelines with Scala and Kafka on Google Cloud Platform</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Google云平台上Scala和Kafka的流媒体管道</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/streaming-pipelines-with-scala-and-kafka-on-google-cloud-platform-ef39c41fc1d3?source=collection_archive---------1-----------------------#2020-01-27">https://medium.com/google-cloud/streaming-pipelines-with-scala-and-kafka-on-google-cloud-platform-ef39c41fc1d3?source=collection_archive---------1-----------------------#2020-01-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/0b13a14fae7c438554066c3828730a9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*zcFBMC-sJ0OkImbUIZMcBg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">我们将在这篇博文中推出的管道</figcaption></figure><p id="c070" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">谷歌云平台(GCP)提供了许多运行流媒体分析管道的选项。GCP的基石消息队列处理系统是<a class="ae jo" href="https://cloud.google.com/pubsub/" rel="noopener ugc nofollow" target="_blank">云发布/订阅</a>。然而，<a class="ae jo" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank">卡夫卡</a>可能是Pub/Sub最受欢迎的替代品，关于如何与其他GCP产品集成，目前还没有太多信息。</p><p id="32b1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于流处理，<a class="ae jo" href="https://cloud.google.com/dataflow/" rel="noopener ugc nofollow" target="_blank">数据流</a>是最先进的系统之一。为了在数据流上运行管道，我们必须使用<a class="ae jo" href="https://beam.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Beam </a>，这是一个开源的SDK，可以在许多不同的系统上运行，如<a class="ae jo" href="https://flink.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Flink </a>、<a class="ae jo" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>和<a class="ae jo" href="https://beam.apache.org/documentation/runners/capability-matrix/" rel="noopener ugc nofollow" target="_blank">许多其他的</a>，而不仅仅是数据流。然而，在我写这篇文章的时候，<a class="ae jo" href="https://beam.apache.org/documentation/sdks/java/" rel="noopener ugc nofollow" target="_blank"> Beam SDK只有Java、Python、Go和SQL </a>版本，我深深地喜欢Scala和函数式编程。</p><p id="4928" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当编写数据处理系统时，我认为函数式编程范式非常适合流处理的概念。对于像我一样喜欢Scala的人来说，幸运的是，<a class="ae jo" href="https://spotify.github.io/scio/" rel="noopener ugc nofollow" target="_blank"> Spotify开发了Scio，这是一个用于Apache Beam的Scala API</a>。尽管基于光束顶部，因此<a class="ae jo" href="https://spotify.github.io/scio/Runners.html" rel="noopener ugc nofollow" target="_blank">可用于任何光束运行器</a>，Scio更关注数据流；这很有意义，因为其他典型处理系统(Spark、Flink)的原生API已经在Scala中了。事实上，<a class="ae jo" href="https://spotify.github.io/scio/Scio,-Scalding-and-Spark.html" rel="noopener ugc nofollow" target="_blank"> Scio API的灵感来自Spark，一些想法来自滚烫的</a>。</p><p id="f389" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它还附带了许多额外的好东西。我特别喜欢<a class="ae jo" href="https://spotify.github.io/scio/io/Type-Safe-BigQuery.html#using-type-safe-bigquery" rel="noopener ugc nofollow" target="_blank">BigQuery的类型安全API</a>，它允许将任何BigQuery表映射到Scala中的case类，自动从BigQuery推断模式——也就是说，<em class="jp">您不必为您的case类编写任何字段来表示您在big query(！).</em></p><h1 id="7d2c" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">这个例子</h1><p id="fbb0" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">在这篇文章中，我们将使用来自谷歌云专业服务团队Github repo的这个例子:<a class="ae jo" href="https://github.com/GoogleCloudPlatform/professional-services/tree/master/examples/dataflow-scala-kafka2avro" rel="noopener ugc nofollow" target="_blank">https://Github . com/Google Cloud platform/Professional-Services/tree/master/examples/data flow-Scala-Kafka 2 avro</a></p><p id="e814" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在示例中，我们将设置两条数据流管道:一条将对象写入Kafka，另一条从相同的Kafka主题中读取这些对象。管道是用Scala编写的，使用Scio。</p><p id="3063" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里我假设你已经准备好了一个谷歌云平台项目。如果你以前从未使用过GCP，你可以使用免费的GCP信用点数。</p><h1 id="3781" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">设置Kafka服务器</h1><p id="f871" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">我们需要做的第一件事是建立一个Kafka服务器。通常，您会将Kafka作为一个集群来运行，但是对于这个示例，使用一个标准虚拟机(VM)就足够了。</p><p id="01e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要在虚拟机中部署Kafka，<a class="ae jo" rel="noopener" href="/@iht/setting-up-a-small-kafka-server-on-google-cloud-platform-for-testing-purposes-9958a47ea8b9">请参见上一篇文章</a>。对于您的虚拟机，您必须选择一个区域，尝试选择离您最近的<a class="ae jo" href="https://cloud.google.com/dataflow/docs/concepts/regional-endpoints" rel="noopener ugc nofollow" target="_blank">区域，该区域也有一个数据流端点</a>。</p><p id="3379" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我的例子中，我的区域是<code class="du kt ku kv kw b">europe-west1</code>，我将在区域<code class="du kt ku kv kw b">europe-west1-d</code>中部署我的Kafka VM。当您部署了Kafka VM后，请回到这里继续学习本教程。</p><h1 id="95a5" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">设置虚拟机来编译和触发管道</h1><p id="321a" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">虽然我们可以使用云外壳来尝试编译管道，但是云外壳并不意味着进行繁重的计算，它更像是一个方便的外壳而不是VM来支持我们的计算。</p><p id="355f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，为了编译管道代码，并在数据流中触发它们，我们将创建一个小型虚拟机。如果你在之前的帖子中仍然有<code class="du kt ku kv kw b">test-vm</code>在运行，请删除它，因为我们需要添加一个额外的选项。</p><p id="45d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们创建一个<code class="du kt ku kv kw b">build-vm</code>，并确保它与我们的小型Kafka服务器在同一个区域中。我们需要确保这个虚拟机的范围允许它访问所有Google Cloud APIs(否则它将无法触发数据流管道):</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="30ab" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">$ export ZONE=europe-west1-d</strong><br/><strong class="kw hj">$ gcloud compute instances create build-vm --zone=$ZONE                 --machine-type=g1-small                                             --scopes=https://www.googleapis.com/auth/cloud-platform<br/></strong>Created [<a class="ae jo" href="https://www.googleapis.com/compute/v1/projects/ihr-kafka-dataflow/zones/europe-west1-d/instances/test-vm" rel="noopener ugc nofollow" target="_blank">https://www.googleapis.com/compute</a>...<br/>NAME     ZONE            MACHINE_TYPE  PREEMPTIBLE  INTERNAL_IP ...<br/>build-vm europe-west1-d  g1-small                   10.132.0.4  ....</span></pre><h1 id="4b6e" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">克隆谷歌云专业服务回购</h1><p id="067f" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">我们示例的代码在<a class="ae jo" href="http://github.com/GoogleCloudPlatform/professional-services/" rel="noopener ugc nofollow" target="_blank">谷歌云平台专业服务Github repo </a>中。在这个报告中，我们包括了你的工作(和我们的工作)的所有类型的例子和工具！)在谷歌云平台上。</p><p id="e139" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这种情况下，我们要用这个例子:<a class="ae jo" href="https://github.com/GoogleCloudPlatform/professional-services/tree/master/examples/dataflow-scala-kafka2avro" rel="noopener ugc nofollow" target="_blank">https://github . com/Google cloud platform/professional-services/tree/master/examples/data flow-Scala-Kafka 2 avro</a></p><p id="8ee1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在该目录中，您会发现两条用Scala编写的数据流管道。</p><p id="a269" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中一个管道是<em class="jp">生成器</em>，它将向Kafka填充一些消息。消息实际上是序列化的对象(case类的实例)，它们被转换成base64编码的字符串。所以我们不仅传递字符串消息，我们还通过Kafka主题传递整个对象！</p><p id="b90f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一个管道是<em class="jp">消费者</em>，它将监听相同Kafka主题中的更新，将恢复对象，将它们转换为<a class="ae jo" href="https://avro.apache.org/" rel="noopener ugc nofollow" target="_blank"> Avro格式</a>，并将对象复制到Google云存储中。因此，从某种意义上说，这个<em class="jp">消费者</em>管道正在对发送到该主题的所有对象进行备份。</p><p id="f228" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们从克隆Github repo开始，并切换到示例的目录。首先，我们需要ssh到<code class="du kt ku kv kw b">build-vm</code>并安装一些缺失的依赖项:</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="08e8" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">$ gcloud compute ssh build-vm --zone=$ZONE</strong><br/><strong class="kw hj">build-vm$ sudo apt install -y git apt-transport-https kafkacat default-jdk<br/></strong>[...]</span><span id="f106" class="lf jr hi kw b fi lk lh l li lj"><strong class="kw hj">build-vm</strong>$ <strong class="kw hj">git clone </strong><a class="ae jo" href="https://github.com/GoogleCloudPlatform/professional-services.git" rel="noopener ugc nofollow" target="_blank"><strong class="kw hj">https://github.com/GoogleCloudPlatform/professional-services.git</strong></a><br/>Cloning into 'professional-services'...<br/>remote: Enumerating objects: 48766, done.<br/>remote: Total 48766 (delta 0), reused 0 (delta 0), pack-reused 48766<br/>Receiving objects: 100% (48766/48766), 189.70 MiB | 25.66 MiB/s, done.<br/>Resolving deltas: 100% (14245/14245), done.<br/></span></pre><h1 id="2091" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">配置文件</h1><p id="5958" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">传递给管道的选项通过配置文件设置。这个配置文件与编译后的代码一起打包在生成的JAR包中。</p><p id="dcda" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中一个选项需要在<a class="ae jo" href="https://cloud.google.com/storage/" rel="noopener ugc nofollow" target="_blank"> Google Cloud Storage (GCS) </a>中有一个存储桶，Avro文件将存储在这里。所以我们先创建一个桶。我们将在与Kafka服务器相同的区域中创建一个区域存储桶，以避免跨区域从数据流或Kafka向GCS发送数据。选择上面使用过的相同区域(在我的例子中是<code class="du kt ku kv kw b">europe-west1</code>)。在所有GCP项目的所有可能时段中，时段名称必须是唯一的。所以我通常选择我的项目id或项目名称(有时项目名称可能不是唯一的)。在本例中，这是我创建的bucket(您可以在构建VM或云Shell中运行该命令):</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="4374" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">$ gsutil mb -l europe-west1 gs://ihr-kafka-dataflow<br/></strong>Creating gs://ihr-kafka-dataflow/...</span></pre><p id="f1e6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">配置文件位于我们示例目录的<code class="du kt ku kv kw b">src/main/resources/application.conf</code>中。回购协议中该文件的内容是:</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="40a4" class="lf jr hi kw b fi lg lh l li lj">broker = "YOUR_IP_HERE:PORT"<br/>dest-bucket = "BUCKET_NAME_NOT_STARTING_WITH_GS://"<br/>dest-path = "PATH_INSIDE_BUCKET"<br/>kafka-topic = "TOPIC_NAME"<br/>num-demo-objects = 500  # number of messages to be generated...</span></pre><p id="2f24" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代理IP是<code class="du kt ku kv kw b">kafka-vm</code>的内部地址，在前面的章节中创建(如果你按照前一篇文章中给出的说明，你的Kafka服务器的名称是<code class="du kt ku kv kw b">kafka-vm</code>)，端口是9092。您可以使用(从构建VM或云Shell运行)找出服务器的内部IP地址:</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="e73d" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">$ gcloud compute instances list</strong><br/>NAME      ZONE   MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  ...<br/>kafka-vm  eu...  n1-standard-1               <strong class="kw hj"><em class="jp">10.132.0.3</em></strong></span></pre><p id="cf25" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将把文件复制到一个名为<code class="du kt ku kv kw b">kafka2avro</code>的子目录中，主题名将被命名为<code class="du kt ku kv kw b">avro_objs</code>。</p><p id="5a99" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，这是我的配置文件的内容:</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="3e9f" class="lf jr hi kw b fi lg lh l li lj">broker = "10.132.0.3:9092"<br/>dest-bucket = "ihr-kafka-dataflow"<br/>dest-path = "kafka2avro"<br/>kafka-topic = "avro_objs"<br/>num-demo-objects = 500</span></pre><p id="89ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要编辑文件并设置管道选项，请切换到repo中包含源代码的目录，并使用编辑器nano编辑文件:</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="e357" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">build-vm$ cd<br/>build-vm$ cd professional-services/examples/dataflow-scala-kafka2avro/<br/>build-vm$ nano src/main/resources/application.conf</strong></span></pre><p id="41c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">编辑完文件后，按Ctrl + X，回复“Y”(表示是)，然后按enter编写文件并退出编辑器。现在，您可以编译代码并生成一个包了:</p><h1 id="46c2" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">生成一个JAR包</h1><p id="de71" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">为了启动数据流管道，我们需要编译代码并生成一个Java包。此外，配置文件包含在生成的包中，因此对配置的任何更改都需要重新编译包。</p><p id="d519" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">作为一个Scala项目，我们将要使用的构建工具是<a class="ae jo" href="https://www.scala-sbt.org/" rel="noopener ugc nofollow" target="_blank"> sbt (Scala构建工具</a>)。让我们将它安装在我们的构建虚拟机中。或者，如果您在本地安装了sbt，您可以跳过这些步骤。我们正在安装sbt，以便能够生成一个JAR包并启动数据流管道。</p><p id="8aa2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">按照文档中的说明，让我们在构建vm中执行一些命令。</p><p id="aac9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先将sbt的存储库添加到<code class="du kt ku kv kw b">build-vm:</code>中的repos列表中</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="059f" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">echo "deb https://dl.bintray.com/sbt/debian /" | sudo tee -a /etc/apt/sources.list.d/sbt.list</strong></span></pre><p id="fb97" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">导入密钥以检查包签名:</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="f3fc" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&amp;search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | sudo apt-key add</strong></span></pre><p id="5b27" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在更新你的包列表</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="fe7d" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">sudo apt update</strong></span></pre><p id="585e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们安装sbt</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="7c1d" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">sudo apt install -y sbt</strong></span></pre><p id="4432" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦安装完成，我们就可以调用<code class="du kt ku kv kw b">sbt</code>来编译和生成这个包:</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="423d" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">build-vm$ cd<br/>build-vm$ cd professional-services/examples/dataflow-scala-kafka2avro/<br/>build-vm$ sbt<br/></strong>[..]<br/><strong class="kw hj">sbt:kafka2avro&gt; </strong>compile<br/>[..]<br/><strong class="kw hj">sbt:kafka2avro&gt; </strong>pack<br/>[..]<br/><strong class="kw hj">sbt:kafka2avro&gt; </strong>exit<br/>[..]</span></pre><p id="56ab" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">至此，我们已经生成了一个启动管道的包。</p><p id="cab5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们使用了一个小的VM来编译代码，我们也将使用它来启动管道。为了避免为此运行虚拟机，你可能会想使用像<a class="ae jo" href="https://cloud.google.com/cloud-build/" rel="noopener ugc nofollow" target="_blank">云构建</a>这样的服务。事实上，在这个管道的repo中有<a class="ae jo" href="https://github.com/GoogleCloudPlatform/professional-services/tree/master/examples/dataflow-scala-kafka2avro#continuous-integration" rel="noopener ugc nofollow" target="_blank">一个云构建配置文件。但是我们不会在这篇文章中使用它。</a></p><h1 id="72fb" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">启动消费者渠道</h1><p id="a3cc" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">消费者管道将只读取在消费者管道启动后发布到Kafka主题的消息。出于这个原因，我们将在生产者管道之前启动它。</p><p id="f3fa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要启动管道，我们需要为包和临时文件设置一个暂存位置，并且我们需要设置一个本地类路径来找到所有必需的依赖项。</p><p id="1bae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，我们需要<a class="ae jo" href="https://console.cloud.google.com/flows/enableapi?apiid=dataflow,compute_component,logging,storage_component,storage_api,bigquery,pubsub,datastore.googleapis.com,cloudresourcemanager.googleapis.com&amp;_ga=2.211629653.1598145337.1580125273-1377503026.1579767663&amp;_gac=1.216945124.1579767775.Cj0KCQiApaXxBRDNARIsAGFdaB9GpS9FghvK2rsvuTSbtQjrkKDj8Mq06I1LylP4A_ppkNK79U4CoyEaAlkUEALw_wcB" rel="noopener ugc nofollow" target="_blank">确保Dataflow API在我们的项目</a>中启用(使用该链接在您的项目中启用必要的API)。</p><p id="b791" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">至于凭证，我们将依靠<code class="du kt ku kv kw b">gcloud</code> CLI实用程序来处理认证。<a class="ae jo" href="https://cloud.google.com/dataflow/docs/quickstarts/quickstart-java-maven#before-you-begin" rel="noopener ugc nofollow" target="_blank">您还可以为您的数据流管道创建一个服务帐户，并使用这些凭证进行身份验证</a>。</p><p id="d76e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所有需要的库都存储在目录<code class="du kt ku kv kw b">target/pack/lib/</code>中，暂存位置将是我们bucket中的一个目录(例如，我将使用<code class="du kt ku kv kw b">gs://ihr-kafka-dataflow-stg</code>)。</p><p id="8597" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，还有最后一个要求:我们需要连接到Kafka服务器来启动管道。提交过程在启动管道之前从Kafka获取一些元数据。因为我们在与Kafka服务器相同的区域中运行虚拟机，所以这不会成为问题。但是，如果您从不同的机器或服务运行管道，您应该考虑添加防火墙规则和正确的权限。</p><p id="9f8b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在与上一节相同的目录中，现在让我们触发消费者管道:</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="7ff7" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">build-vm$ export STAGING_LOCATION=gs://ihr-kafka-dataflow/stg<br/>build-vm$ export PROJECT_ID=ihr-kafka-dataflow<br/>build-vm$ export </strong><strong class="kw hj">CLASSPATH="target/pack/lib/*"<br/></strong><strong class="kw hj">build-vm</strong><strong class="kw hj">$ export REGION=europe-west1</strong><strong class="kw hj"> <br/>build-vm</strong><strong class="kw hj">$ java com.google.cloud.pso.kafka2avro.Kafka2Avro                   --exec.mainClass=com.google.cloud.pso.kafka2avro.Kafka2Avro          --project=$PROJECT_ID --stagingLocation=$STAGING_LOCATION           --runner=DataflowRunner --region=$REGION</strong></span></pre><p id="167f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦管道运行，只要Kafka主题开始接收消息，它就会开始在Google云存储中产生Avro文件。</p><p id="213d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">启动管道后，我们可以在<a class="ae jo" href="https://console.cloud.google.com/dataflow/" rel="noopener ugc nofollow" target="_blank">数据流界面</a>的作业页面中检查管道是否正在运行。我们应该会看到如下图表:</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/6e001d003564ae3fa648414e3bd8f00a.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/0*PXKFiBVVKCZmbsb5"/></div><figcaption class="im in et er es io ip bd b be z dx translated">消费者管道，全天候运行，因为它是一个流管道</figcaption></figure><h1 id="a654" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">启动生产者管道</h1><p id="7a27" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">除非我们在Kafka中发布消息，否则消费者管道不会开始处理消息。所以我们也启动它吧。我们使用与前面管道中相同的环境变量:</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="81fe" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">build-vm$ </strong><strong class="kw hj">java com.google.cloud.pso.kafka2avro.Object2Kafka                   --exec.mainClass=com.google.cloud.pso.kafka2avro.Object2Kafka          --project=$PROJECT_ID --stagingLocation=$STAGING_LOCATION           --runner=DataflowRunner --region=$REGION</strong></span></pre><p id="362e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">启动管道后，我们可以在<a class="ae jo" href="https://console.cloud.google.com/dataflow/" rel="noopener ugc nofollow" target="_blank">数据流UI </a>的作业页面中检查管道是否正在运行:</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/44af31d60251456cdd0e515bfecbc2b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/0*_82F75bGwmX0TkQ9"/></div><figcaption class="im in et er es io ip bd b be z dx translated">管道刚刚完成向卡夫卡发布对象</figcaption></figure><p id="ecaa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了检查它实际上已经向Kafka主题发布了500个对象(在上面的配置中:<code class="du kt ku kv kw b">avro_objs</code>)，让我们用Kafka的命令行客户端<code class="du kt ku kv kw b">kafkacat</code>连接check:</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="3e86" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">build-vm:~$ export BROKER=10.132.0.3 <br/>build-vm:~$ export TOPIC=avro_objs<br/>build-vm:~$</strong> <strong class="kw hj">kafkacat -b $BROKER:9092 -t $TOPIC -e | wc -l</strong><br/>% Auto-selecting Consumer mode (use -P or -C to override)<br/>% Reached end of topic avro_objs [0] at offset 500: exiting<br/>500</span></pre><p id="c6d0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">消费者管道应该已经开始处理消息，并且应该已经产生了一个Avro文件，将500个对象分组为一个列表。我们可以检查输出位置:</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="7f54" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">$ export DEST_BUCKET="ihr-kafka-dataflow"</strong><br/><strong class="kw hj">$ export DEST_PATH="kafka2avro"</strong><br/><strong class="kw hj">$ gsutil ls -hl gs://${DEST_BUCKET}/${DEST_PATH}</strong><br/> 22.18 KiB  2020-01-27T16:09:35Z  gs://ihr...0930.avro<br/>TOTAL: 1 objects, 22713 bytes (22.18 KiB)</span></pre><p id="f549" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以两条管道都工作正常！</p><h1 id="659d" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">清理</h1><p id="aba7" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">请注意，消费者管道是一个流管道，<strong class="is hj">因此它将一直运行(并产生额外的成本)，直到您停止它</strong>。要停止管道，请执行下列操作:列出正在运行的作业，记下作业id，然后取消作业(如果使用不同的作业，请更改区域):</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="64f0" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">$ export REGION=europe-west1<br/>$ gcloud dataflow jobs list --status=active --region=$REGION<br/></strong>JOB_ID                                    NAME       ...                        <br/>2020-01-27_08_32_00-13108354405313287331  kafka2avro-...<br/><strong class="kw hj">$ gcloud dataflow jobs cancel 2020-01-27_08_32_00-13108354405313287331 --region=$REGION<br/></strong>Cancelled job [2020-01-27_08_32_00-13108354405313287331]</span></pre><p id="bfd6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">生产者管道是一个批处理管道，所以它一创建500个对象就结束了。</p><p id="fe40" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您可能还希望停止和/或删除构建虚拟机。如果你让它运行，即使你不使用它，它也会产生费用。从云Shell运行这个程序:(<strong class="is hj">注意:不可逆删除</strong>)</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="0094" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">gcloud compute instances delete build-vm — zone=$ZONE</strong></span></pre><p id="e7ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">同样，记住<code class="du kt ku kv kw b">kafka-vm</code>仍然在运行。您可能想要停止和/或删除它。</p><p id="8c16" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另外，请记住，您已经创建了一个GCS bucket(在云Shell中使用<code class="du kt ku kv kw b">gsutil ls</code>检查其名称)来删除它，在云Shell中运行:(<strong class="is hj">注意:不可逆删除</strong>)</p><pre class="kx ky kz la fd lb kw lc ld aw le bi"><span id="560c" class="lf jr hi kw b fi lg lh l li lj"><strong class="kw hj">gsutil -m rm -rf gs://[BUCKET_NAME]</strong></span></pre><h1 id="58fe" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">序列化任何种类的自定义对象</h1><p id="f0cb" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">管道通过序列化和反序列化对象来工作，这些对象在Kafka主题中被编码为base64字符串，并且在代码中由Scala case类表示:</p><figure class="kx ky kz la fd ij"><div class="bz dy l di"><div class="ln lo l"/></div><figcaption class="im in et er es io ip bd b be z dx translated">示例中使用的演示类型</figcaption></figure><p id="ff30" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代码可以处理任何Scala case类。唯一的限制是这个类在编译时必须是已知的。在管道的开始，类型被定义为<code class="du kt ku kv kw b">T</code> <a class="ae jo" href="https://github.com/GoogleCloudPlatform/professional-services/blob/master/examples/dataflow-scala-kafka2avro/src/main/scala/com/google/cloud/pso/kafka2avro/Kafka2Avro.scala#L47" rel="noopener ugc nofollow" target="_blank">。您需要提供两个函数来使</a><a class="ae jo" href="https://github.com/GoogleCloudPlatform/professional-services/blob/master/examples/dataflow-scala-kafka2avro/src/main/scala/com/google/cloud/pso/kafka2avro/utils/Kafka2AvroUtils.scala#L56-L65" rel="noopener ugc nofollow" target="_blank">序列化</a>:</p><figure class="kx ky kz la fd ij"><div class="bz dy l di"><div class="ln lo l"/></div><figcaption class="im in et er es io ip bd b be z dx translated">用于序列化管道对象的示例类型不可知函数</figcaption></figure><p id="24cf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">并且<a class="ae jo" href="https://github.com/GoogleCloudPlatform/professional-services/blob/master/examples/dataflow-scala-kafka2avro/src/main/scala/com/google/cloud/pso/kafka2avro/utils/Kafka2AvroUtils.scala#L38-L45" rel="noopener ugc nofollow" target="_blank">反序列化</a>对象:</p><figure class="kx ky kz la fd ij"><div class="bz dy l di"><div class="ln lo l"/></div><figcaption class="im in et er es io ip bd b be z dx translated">用于反序列化管道对象的示例类型不可知函数</figcaption></figure><p id="6134" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请注意，在这种情况下，这两个函数都是类型不可知的，因此您可以将它们应用于任何case类或任何其他类型，只要它是可序列化的。</p><h1 id="0930" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">使用消费者管道读取旧消息</h1><p id="86b6" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">我们首先启动了消费者管道，否则它不会接收到消息。这是因为在创建消费者管道之前，消息是带时间戳发送的。</p><p id="178c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们如何解决这个问题？如何确保我们阅读了主题中包含的所有未确认消息？</p><p id="965c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为此，我们需要知道比我们要读取的消息的时间戳早一个时刻的时间戳，<a class="ae jo" href="https://github.com/GoogleCloudPlatform/professional-services/blob/master/examples/dataflow-scala-kafka2avro/src/main/scala/com/google/cloud/pso/kafka2avro/Kafka2Avro.scala#L116-L121" rel="noopener ugc nofollow" target="_blank">，并在</a> <code class="du kt ku kv kw b"><a class="ae jo" href="https://github.com/GoogleCloudPlatform/professional-services/blob/master/examples/dataflow-scala-kafka2avro/src/main/scala/com/google/cloud/pso/kafka2avro/Kafka2Avro.scala#L116-L121" rel="noopener ugc nofollow" target="_blank">KafkaIO.Read</a></code>中设置 <code class="du kt ku kv kw b"><a class="ae jo" href="https://github.com/GoogleCloudPlatform/professional-services/blob/master/examples/dataflow-scala-kafka2avro/src/main/scala/com/google/cloud/pso/kafka2avro/Kafka2Avro.scala#L116-L121" rel="noopener ugc nofollow" target="_blank">withStartReadTime</a></code> <a class="ae jo" href="https://github.com/GoogleCloudPlatform/professional-services/blob/master/examples/dataflow-scala-kafka2avro/src/main/scala/com/google/cloud/pso/kafka2avro/Kafka2Avro.scala#L116-L121" rel="noopener ugc nofollow" target="_blank">属性(Github repo中的原始代码没有指定任何开始读取时间):</a></p><figure class="kx ky kz la fd ij"><div class="bz dy l di"><div class="ln lo l"/></div></figure><p id="08e3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这种情况下，我们正在读取所有早于纪元1的消息，即早于1970年1月1日之后的一秒钟的消息。</p><h1 id="a99e" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">包裹</h1><p id="24c5" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">在这篇文章中，我们看到了如何在Google云平台上使用Scio和Dataflow，将Scala和Kafka用于流数据处理管道。</p><ul class=""><li id="bcde" class="lp lq hi is b it iu ix iy jb lr jf ls jj lt jn lu lv lw lx bi translated">您可以使用Scala创建流和批处理管道，在数据流上运行，并与许多服务(GCP本地或开源，如Kafka)进行交互</li><li id="cfb1" class="lp lq hi is b it ly ix lz jb ma jf mb jj mc jn lu lv lw lx bi translated">我们展示了一个序列化和反序列化对象的例子，没有进一步的修改，但是您可以对您的数据应用任何转换，只需修改代码和几个函数。</li><li id="9191" class="lp lq hi is b it ly ix lz jb ma jf mb jj mc jn lu lv lw lx bi translated">如果您有一个需要从Kafka主题中恢复所有消息的消费者管道，我们已经看到了如何读取任何消息，不管它有多旧，只要它仍然存储在Kafka主题中。</li><li id="51b7" class="lp lq hi is b it ly ix lz jb ma jf mb jj mc jn lu lv lw lx bi translated">你可以使用GCP的免费版本来学习这个教程。</li></ul></div></div>    
</body>
</html>