<html>
<head>
<title>Accelerating Question Answering Applications with MobileBERT and TFLite</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用MobileBERT和TFLite加速问答应用</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/accelerating-question-answering-applications-with-mobilebert-and-tflite-1f8d301ddbf7?source=collection_archive---------6-----------------------#2021-11-22">https://medium.com/google-cloud/accelerating-question-answering-applications-with-mobilebert-and-tflite-1f8d301ddbf7?source=collection_archive---------6-----------------------#2021-11-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/14a665f8125412987ef2faa5cc8e5083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7FEUsM5bV50jdaff7CNvdw.png"/></div></div></figure><p id="8060" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随着预训练、自我监督模型的使用，自然语言处理的世界一直在快速发展。然而，最先进的模型，如BERT，往往有大量的参数，因此，遭受高延迟。对于许多资源有限的应用程序，这些模型的计算需求可能使它们的使用不可行。</p><p id="8782" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇文章中，我想分享我们部署BERT系列问答模型的轨迹，以解决<a class="ae jo" href="https://www.weg.net/" rel="noopener ugc nofollow" target="_blank"> WEG </a>的一个特定业务问题——在电机询价期间自动检测技术偏差。这些报价通常附有必须满足的技术要求，当有特定摘录需要评论/澄清时，汇编报价提案的分析师应强调这些注意点或偏差。</p><p id="0ffc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所提出的策略中最大的瓶颈是所需的等待时间，假设要处理一个单一的技术规范，需要查询成百上千的文本需求以获得适当的偏差。因此，我们将把重点放在我们如何解决这个问题并设法加速我们的问答应用程序上。</p><p id="ef44" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了防止得出结论，让我们先从底线开始:我们的最终实现依赖于一个转换成TFLite格式的<a class="ae jo" href="https://arxiv.org/abs/2004.02984" rel="noopener ugc nofollow" target="_blank"> MobileBERT </a>模型，以便与我们最初的BERT语言模型相比，显著地减少我们模型的二进制文件大小，这最终使我们能够部署我们提出的策略。</p><p id="413a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们仔细看看我们的应用程序。</p><h1 id="7825" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">应用概述</h1><p id="177e" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">在收到一个或多个电动机的报价请求时，WEG公司的分析师需要仔细阅读技术规范，以便提出报价建议。这包括寻找这些规范中列出的可能与WEG提供的有所不同的要求，或者可能需要额外的评论/澄清的要求。</p><p id="be69" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，让我们假设其中一个规格说明如下:</p><blockquote class="ks kt ku"><p id="9141" class="iq ir kv is b it iu iv iw ix iy iz ja kw jc jd je kx jg jh ji ky jk jl jm jn hb bi translated">所有尺寸的机器外壳都应该配备一个螺纹黄铜排泄塞。</p></blockquote><p id="a62d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，对于该特定的电机系列，WEG公司可能必须澄清该特定要求的以下偏差:</p><blockquote class="ks kt ku"><p id="36c0" class="iq ir kv is b it iu iv iw ix iy iz ja kw jc jd je kx jg jh ji ky jk jl jm jn hb bi translated">我们可以供应橡胶和不锈钢排水管。</p></blockquote><p id="f423" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于规范的大部分句子，不需要额外的注释，但是对于分析师来说，拥有某种“偏差检测器”是非常宝贵的，这种应用程序能够通读规范，以便检测这些注意点，同时提出可能的偏差。</p><p id="8629" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们决定将这个问题构建成一个问答问题，其中规范的句子或者需求是我们的输入问题，偏差是我们的答案。所有实验的问答模型都已经在<a class="ae jo" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank"> SQuAD1.1 </a>问答数据集上进行了预训练，并随后在WEG的特定数据集上进行了微调。该数据集是使用分析师在过去3年中生成的简报集合构建的。简报是概括的文档，展示了关于相关报价请求的最基本的信息，包括我们感兴趣的需求-偏差对。</p><p id="f792" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以下是我们应用程序的概述:</p><figure class="la lb lc ld fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kz"><img src="../Images/3a9fdb31cb3129350ce59258aafc450a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eYN2wK_d-okuqGZ0uBxl5w.png"/></div></div></figure><p id="600a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">起点是您希望对其要求提出偏差建议的规范。技术规格可以以不同的格式接收，但大多数是PDF文件。因此，通过PDF解析过程提取句子，以便形成需求列表。</p><p id="4542" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，每个需求由一个问答管道单独处理，以寻找潜在的偏差。为了做到这一点，我们首先从过去几年分析师制作的简报中创建一个偏差数据库。被训练来为每个需求搜索整个数据库的问题回答模型将花费非常长的时间，因此应用初步的文档检索阶段，以便将候选者缩小到30个可能偏差的列表。这是通过<a class="ae jo" href="https://en.wikipedia.org/wiki/Okapi_BM25" rel="noopener ugc nofollow" target="_blank"> Okapi BM25 </a>算法完成的，这是一种流行的词袋排名功能，用于检索与给定查询最相关的文档。</p><p id="0419" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们终于有了我们需要的东西来填充我们的问答模型:查询(需求)和我们的上下文(最相关的潜在偏差)。根据给定的上下文，模型输出一个可能答案的列表，以及每个答案的置信度值。如果得分高于预定义的阈值，则从得分最高的答案中选择适当的偏差。否则，不建议偏离当前要求。</p><h1 id="280b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">延迟问题</h1><p id="032d" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">一个技术规范由数百个句子组成。如果对Q&amp;A模型的每个请求都有几十秒的延迟，那么我们手中的应用程序就无法使用。这就是为什么我们决定将问答模型与应用程序的其余部分分离，并将其作为容器化的应用程序托管在<a class="ae jo" href="https://cloud.google.com/run" rel="noopener ugc nofollow" target="_blank"> Google Cloud Run </a>中。这样，我们就不必担心为计算要求最高的应用程序进程管理基础设施和资源供应。</p><p id="111a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一个应用程序的迭代使用了BERT大模型的Pytorch实现。初始实验的延迟结果很快向我们表明，当前的解决方案行不通，这促使我们寻找替代方案。自然的途径是搜索BERT的压缩版本，如<a class="ae jo" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank"> DistilBERT </a>，它利用知识提炼的过程来减少整个模型的大小。最后，在最后一次迭代中，我们使用了<a class="ae jo" href="https://openreview.net/forum?id=SJxjVaNKwB" rel="noopener ugc nofollow" target="_blank"> MobileBERT </a>，这是BERT的另一个压缩版本，专门为移动和资源受限的环境而设计。此外，我们从Pytorch切换到Tensorflow实现，并继续将TF模型转换为<a class="ae jo" href="https://www.tensorflow.org/lite" rel="noopener ugc nofollow" target="_blank"> TFLite </a>。TFLite最初设计用于在移动和嵌入式设备上运行Tensorflow模型。尽管目前情况并非如此，但延迟和模型二进制文件大小方面的优势证明了使用它的合理性。</p><p id="ec37" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下图展示了我们的每一项变革所带来的改进。在这个简单的实验中，跟踪了样本规范处理过程中每个请求的延迟值(以毫秒为单位)。实际上，每个实现都要处理三次规范:BERT/Pytorch、DistilBERT/Pytorch和MobileBERT/TFLite。</p><figure class="la lb lc ld fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/3a3cac2a0e94616e4870eddfe30a98e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hRgnQw3YK77zUYa5Bgg6Qw.png"/></div></div></figure><p id="1cb9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于所创建的3个服务中的每一个，在云运行时都使用了相同的设置:4个vcpus，8GiB的分配内存，每个容器的最大请求数为80，最小实例数为0，最大实例数为5。MobileBERT模型最初使用TensorFlow实现进行训练，并使用<a class="ae jo" href="https://www.tensorflow.org/lite/tutorials/model_maker_question_answer" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite模型生成器</a>转换为TFLite，而模型在<a class="ae jo" href="https://github.com/tensorflow/tflite-support" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite支持</a>的帮助下部署在C++实现上。在TFLite模型转换过程中，该过程被设置为针对延迟进行优化。</p><p id="b2ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">简单地通过观察上图，我们可以看到延迟的明显减少。处理单个规格的平均持续时间如下:</p><ul class=""><li id="0954" class="lf lg hi is b it iu ix iy jb lh jf li jj lj jn lk ll lm ln bi translated">BERT/py torch—<strong class="is hj">1545</strong><strong class="is hj">秒</strong></li><li id="4e71" class="lf lg hi is b it lo ix lp jb lq jf lr jj ls jn lk ll lm ln bi translated">蒸馏器/Pytorch — <strong class="is hj"> 350 </strong> <strong class="is hj">秒</strong></li><li id="2ffc" class="lf lg hi is b it lo ix lp jb lq jf lr jj ls jn lk ll lm ln bi translated">mobile Bert/TF lite—<strong class="is hj">200</strong><strong class="is hj">秒</strong></li></ul><p id="1978" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">就百分比而言，与最初的实施相比，我们的最终解决方案在处理时间上减少了87%。</p><p id="bb39" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">至于模型的二进制文件大小，结果也非常令人印象深刻:</p><ul class=""><li id="42e2" class="lf lg hi is b it iu ix iy jb lh jf li jj lj jn lk ll lm ln bi translated">BERT/Pytorch — <strong class="is hj"> 1.2GB </strong></li><li id="889a" class="lf lg hi is b it lo ix lp jb lq jf lr jj ls jn lk ll lm ln bi translated">蒸馏器/Pytorch — <strong class="is hj"> 253MB </strong></li><li id="9571" class="lf lg hi is b it lo ix lp jb lq jf lr jj ls jn lk ll lm ln bi translated">MobileBERT/TFLite — <strong class="is hj"> 25MB </strong></li></ul><p id="e1b4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在最好/最坏的情况之间，这是一个巨大的98%的缩减。</p><h2 id="19db" class="lt jq hi bd jr lu lv lw jv lx ly lz jz jb ma mb kd jf mc md kh jj me mf kl mg bi translated">结论</h2><p id="2a9b" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">使用最先进的NLP模型可以为许多不同的应用产生令人印象深刻的结果。然而，当延迟是一个重要问题时，它的数亿个参数可能会阻止它的使用。在所示的应用程序中，我们感兴趣的是自动检测电动机报价请求中的偏差。对于这个用例，需要数百或数千个问题来处理一个技术规范。</p><p id="b8f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于最初用BERT的大模型进行的实验，这个限制似乎阻止了所提出的策略。然而，通过进一步研究压缩我们的语言模型的方式，我们能够结合两种特定的方法来实现一个可行的解决方案。第一个是将模型从BERT改为MobileBERT模型的简化版本，以加速我们的推断。最后，我们将TensorFlow模型转换为TFLite格式，以便在性能损失最小的情况下进一步压缩MobileBERT。</p><p id="bfa5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">至于性能指标，还需要在模型之间进行适当的比较，即使最初的评估没有表明BERT和MobileBERT之间的性能下降，这将与<a class="ae jo" href="https://arxiv.org/abs/2004.02984" rel="noopener ugc nofollow" target="_blank"> MobileBERT在SQuAD数据集</a>上的结果一致。尽管需要每个模型的性能指标才能全面了解情况，但观察到的延迟和模型大小的减少使我们能够部署建议的策略来解决我们的业务问题。</p><p id="1f54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您有任何反馈或问题，请随时联系我们！</p></div></div>    
</body>
</html>