<html>
<head>
<title>Serverless Spark ETL Pipeline Orchestrated by Airflow on GCP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">由GCP气流编排的无服务器Spark ETL管道</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/serverless-spark-etl-pipeline-orchestrated-by-airflow-on-gcp-199efbf9a9f3?source=collection_archive---------0-----------------------#2022-06-25">https://medium.com/google-cloud/serverless-spark-etl-pipeline-orchestrated-by-airflow-on-gcp-199efbf9a9f3?source=collection_archive---------0-----------------------#2022-06-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1d8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大数据Spark工程师平均仅花费40%在实际数据或ml管道开发活动上。他们的大部分时间通常用于管理集群或优化spark应用程序变量。</p><p id="8230" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为这种整体集群设置的替代方案，是基于云的无服务器、无操作平台，如基于GCP的BigQuery(基于SQL)或Data Fusion(无代码、基于UI)。虽然工程师真的很喜欢这些无服务器分析解决方案，但他们也更喜欢将管道放在pyspark或scala中，以便在需要时方便移植。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/63cd39a3c1ef634c18398bceb1802aaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-YVyyOXLsSVq6-i3"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">火花工程师期望从他们的新Q(火花季度主)一个巨大的火花集群</figcaption></figure><p id="adf5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为一种中庸之道，将spark工程师从非分析活动中解放出来，同时能够保留或继续编写Spark原生应用，这就是GCP的<strong class="ih hj"> <em class="jt">无服务器Spark </em> </strong>解决方案。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ju"><img src="../Images/32394c90e0374cf7e9c1b5305602a0b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*46QSxyiFORz4fkPjialevw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">q用无服务器Spark让Spark工程师大吃一惊！！我们真的不喜欢新世界中的集群:-)</figcaption></figure><p id="611e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是业界首款无需任何手动基础架构配置的自动扩展无服务器Spark。它启用于，</p><p id="28e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">&gt; Vertex AI笔记本，供数据和ML工程师发布Jupyter笔记本和编写spark代码(现已正式发布)</p><p id="191b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">&gt; BigQuery，数据分析师可以编写和执行pyspark代码以及BigQuery SQL(私有预览)</p><p id="c6eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">&gt;可以使用Airflow或Cloud Composer(现已正式发布)将它们编排为完整的数据和ML管道</p><p id="1ed9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我将讨论如何在GCP上以完全无服务器的模式执行Spark ETL管道。</p><p id="ab85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先让我们在无服务器模式下运行一个简单的Spark Pi应用程序。</p><p id="f1f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">导航到GCP的Dataproc控制台，向下滚动到无服务器批处理并点击‘创建’，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jv"><img src="../Images/cd0811f3e0bbe58cdcc4e0364f06038f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WQ4QG9QwvbVt3UOMPiVc7w.png"/></div></div></figure><p id="3aeb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如下所示输入批次详细信息，您可以将其他选项保留为默认选项，然后单击提交。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jw"><img src="../Images/554c2be43d064941a8404a619cc34fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EGsbsz6c7xjeBrz5UCF6Zw.png"/></div></div></figure><p id="e5f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">批处理作业将在30秒内启动并从挂起状态变为运行状态，<em class="jt">(设置一个Dataproc集群大约需要90秒，而设置一个Hadoop集群则是一个未知数)</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jx"><img src="../Images/f9441ad11ed7a590b4e72ab86a0f8d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ek-no5nzx6QC3V0g5lk4sA.png"/></div></div></figure><p id="c0ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark作业的输出如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jy"><img src="../Images/30b716076ea5f93c20404bd58acf208c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UwyJurdQarrXKkDQ61XJUg.png"/></div></div></figure><p id="860b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在“监控”部分可以看到执行者的状态</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jz"><img src="../Images/23f28f4367bee626a2708e68ca19b076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sLBXlzmlFfJr9jlx7ajumA.png"/></div></div></figure><p id="dd46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如现在可以看到的，没有要创建或管理的集群，所有计算都是自动创建和删除的。</p><p id="08f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们已经看到了无服务器Spark的“Hello World”示例，现在让我们继续在由Cloud Composer(托管Apache气流)编排的无服务器Spark上构建ETL管道。</p><p id="1933" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ETL管道的解决方案架构如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ka"><img src="../Images/556addb460ed96081074595e45c0e476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l7QBCx89BI7ePmyzyxIyzA.png"/></div></div></figure><p id="1328" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个管道，我们可以考虑一个简单的聚合示例。可以在GCS存储桶中写入和存储一个文本文件，该文件只有很少的记录，其中包含人员的姓名和年龄。pyspark代码应该计算每个人的平均年龄，并将数据写入BigQuery表。</p><p id="c873" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Cloud Composer中的Dag(GCP的managed Apache Airflow)将以无服务器模式在Dataproc上启动批处理操作符。dag将查找每个人的平均年龄，并将结果存储在BigQuery数据集中。</p><p id="99d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GCS存储桶中的原始数据:</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="c165" class="kg kh hi kc b fi ki kj l kk kl">name,age<br/>Brooke,20<br/>Denny,31<br/>Jules,30<br/>TD,35<br/>Brooke,25<br/>TD,45<br/>Jules,69</span></pre><p id="7718" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">云作曲家DAG:</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="9fd9" class="kg kh hi kc b fi ki kj l kk kl">import os<br/>from airflow.models import Variable<br/>from datetime import datetime<br/>from airflow import models<br/>from airflow.providers.google.cloud.operators.dataproc import (DataprocCreateBatchOperator,DataprocGetBatchOperator)<br/>from datetime import datetime<br/>from airflow.utils.dates import days_ago<br/>import string<br/>import random # define the random module<br/>S = 10  # number of characters in the string.<br/># call random.choices() string module to find the string in Uppercase + numeric data.<br/>ran = ''.join(random.choices(string.digits, k = S))<br/>project_id = models.Variable.get("project_id")<br/>region = models.Variable.get("region")<br/>subnet=models.Variable.get("subnet")<br/>phs_server=Variable.get("phs") ## create a Spark Persistent history server<br/>code_bucket=Variable.get("code_bucket")<br/>bq_dataset=Variable.get("bq_dataset")</span><span id="f8fc" class="kg kh hi kc b fi km kj l kk kl">name=&lt;&lt;your username&gt;&gt;</span><span id="20ab" class="kg kh hi kc b fi km kj l kk kl">dag_name= "serverless_spark_etl"<br/>service_account_id= &lt;&lt;enter your service account&gt;&gt;</span><span id="de2b" class="kg kh hi kc b fi km kj l kk kl">avg_age_script= "gs://"+code_bucket+"/00-scripts/avg_age.py"</span><span id="c6b3" class="kg kh hi kc b fi km kj l kk kl">BATCH_ID = "avg-age-"+str(ran)</span><span id="9072" class="kg kh hi kc b fi km kj l kk kl">BATCH_CONFIG1 = {<br/>    "pyspark_batch": {<br/>        "main_python_file_uri": avg_age_script,<br/>        "args": [<br/>          project_id,<br/>          bq_dataset,<br/>          code_bucket,<br/>          name<br/>        ],<br/>        "jar_file_uris": [<br/>      "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar"<br/>    ]<br/>    },<br/>    "environment_config":{<br/>        "execution_config":{<br/>              "service_account": service_account_id,<br/>            "subnetwork_uri": subnet<br/>            },<br/>        "peripherals_config": {<br/>            "spark_history_server_config": {<br/>                "dataproc_cluster": f"projects/{project_id}/regions/{region}/clusters/{phs_server}"<br/>                }<br/>            },<br/>        },<br/>}<br/>with models.DAG(<br/>    dag_name,<br/>    schedule_interval=None,<br/>    start_date = days_ago(2),<br/>    catchup=False,<br/>) as dag_serverless_batch:<br/>    # [START how_to_cloud_dataproc_create_batch_operator]<br/>    create_serverless_batch1 = DataprocCreateBatchOperator(<br/>        task_id="Avg_Age",<br/>        project_id=project_id,<br/>        region=region,<br/>        batch=BATCH_CONFIG1,<br/>        batch_id=BATCH_ID,<br/>    )</span><span id="a2d6" class="kg kh hi kc b fi km kj l kk kl">create_serverless_batch1</span></pre><p id="6745" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Pyspark代码来计算作者的平均年龄，</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="15c0" class="kg kh hi kc b fi ki kj l kk kl">from pyspark.sql import SparkSession<br/>from pyspark.sql.functions import avg</span><span id="d17c" class="kg kh hi kc b fi km kj l kk kl">project_name =&lt;&lt;enter your project id&gt;&gt;<br/>dataset_name='serverless_spark'<br/># Create a DataFrame using SparkSession<br/>spark = SparkSession.builder.appName("ETL").config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar').getOrCreate()</span><span id="0f2d" class="kg kh hi kc b fi km kj l kk kl">input_data="gs://"+&lt;&lt;enter your bucket name&gt;&gt;+"01-datasets/Authors.csv"</span><span id="2e89" class="kg kh hi kc b fi km kj l kk kl">#Reading the Input Data<br/>data_df = spark.read.format("csv").option("header", True).option("inferschema",True).load(input_data)<br/>data_df.printSchema()<br/># Group the same names together, aggregate their ages, and compute an average<br/>avg_df = data_df.groupBy("name").agg(avg("age"))<br/># Show the results of the final execution<br/>avg_df.show()<br/># extract columns to create country table<br/>avg_table = avg_df.selectExpr("name").dropDuplicates()<br/>avg_table.write.format('bigquery') .mode("overwrite").option('table', project_name+':'+dataset_name+'._avgage') .save()</span></pre><p id="85a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将dag文件放在GCS中气流桶的dags文件夹下后，dag serverless_spark_etl应该会显示在气流UI中，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kn"><img src="../Images/555473795da6af3a7d94705280a95687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vzj5P1Glf_O-_x5NzPT_Kw.png"/></div></div></figure><p id="ae46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后您可以触发dag，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ko"><img src="../Images/b7a9f76a3719948b6ff48da8fa8041be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0-UvK-jiyacruJN6v_tPGQ.png"/></div></div></figure><p id="206b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从Composer触发无服务器批处理操作符，并在Dataproc中启动无服务器批处理执行，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kp"><img src="../Images/faaaa1b827448e3502dce13ec80d2cec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4wKZwNtp2eR7xO1HC6mbNA.png"/></div></div></figure><p id="cd32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">火花执行历史:</p><p id="39e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以从永久历史服务器访问Spark历史服务器，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kq"><img src="../Images/eef979a560f920c34c995acd28ad2c77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPbdzkM4hAxG_Iux8zdN7w.png"/></div></div></figure><p id="3cde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BigQuery中的数据输出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kr"><img src="../Images/91f63790cbfdeb0397ba049b7d88b01e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n1Zy685FiOmqd9KiqfLK_w.png"/></div></div></figure><p id="50f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计费:对于所有无服务器Spark运行的成本，您可以在计费报告中跟踪SKU，如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ks"><img src="../Images/51e82fa7491c1b3f756f625a360e071f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*snCxGVx1M4DePUBxg8gcSg.png"/></div></div></figure><p id="9649" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下一篇文章中，我将讨论如何启动无服务器Spark Jupyter笔记本，以及如何通过Cloud Composer构建和编排ML管道。</p></div></div>    
</body>
</html>