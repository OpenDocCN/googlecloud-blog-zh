<html>
<head>
<title>Create a Dynamically Created DAG and TroubleshootAirflow’s Webserver in Google Cloud Composer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Google Cloud Composer中创建动态创建的DAG和TroubleshootAirflow的web服务器</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/create-a-dynamically-created-dag-and-troubleshoot-airflows-webserver-in-google-cloud-composer-290af1e3eb1b?source=collection_archive---------0-----------------------#2019-12-24">https://medium.com/google-cloud/create-a-dynamically-created-dag-and-troubleshoot-airflows-webserver-in-google-cloud-composer-290af1e3eb1b?source=collection_archive---------0-----------------------#2019-12-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6ffd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有一种情况，我需要从Google Cloud SQL获取我的配置，并创建一个动态创建的DAG。我做的第一步是尝试在我的Composer集群中创建一个<code class="du jd je jf jg b">clousql proxy</code>。这是可以做到的，因为一般来说你所有的气流元数据也存储在云SQL里面，所以你可以基于你的Composer集群中可用的<code class="du jd je jf jg b">cloudsql proxy</code> Dockerimage创建你自己的<code class="du jd je jf jg b">cloudsql proxy</code>。取决于你的气流版本，你可以得到以下的<code class="du jd je jf jg b">Pod</code>和<code class="du jd je jf jg b">Service</code>为代理。</p><pre class="jh ji jj jk fd jl jg jm jn aw jo bi"><span id="534e" class="jp jq hi jg b fi jr js l jt ju">kubectl get svc airflow-sqlproxy-service -o yaml</span><span id="764a" class="jp jq hi jg b fi jv js l jt ju">kubectl get deploy airflow-sqlproxy -o yaml &gt; airflow-sql-proxy-2.yaml # change the -instance in the deployment, don't forget to add your composer service account to IAM of the project where your Cloud SQL instance belongs to</span></pre><p id="121e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于临时存储，您还可以在Composer集群中看到默认的Redis服务。</p><pre class="jh ji jj jk fd jl jg jm jn aw jo bi"><span id="5a29" class="jp jq hi jg b fi jr js l jt ju">airflow-redis-service.&lt;NAMESPACE&gt; # Depends on your Composer version</span></pre><p id="6208" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建动态生成的DAG文件:</p><pre class="jh ji jj jk fd jl jg jm jn aw jo bi"><span id="29ca" class="jp jq hi jg b fi jr js l jt ju">import json<br/>import re<br/>from datetime import datetime, timedelta<br/>import logging</span><span id="529b" class="jp jq hi jg b fi jv js l jt ju">from airflow import DAG<br/>from airflow.models import Variable<br/>from airflow.hooks.postgres_hook import PostgresHook<br/>from airflow.operators.python_operator import PythonOperator<br/>from airflow.hooks.base_hook import BaseHook<br/>from airflow.contrib.operators.slack_webhook_operator import SlackWebhookOperator<br/>from psycopg2.extras import RealDictCursor<br/>import redis</span><span id="1d2b" class="jp jq hi jg b fi jv js l jt ju">logger = logging.getLogger(__name__)</span><span id="cdb1" class="jp jq hi jg b fi jv js l jt ju">class Variables(object):<br/>    DATABASE_CONNECTION_ID = Variable.get('DB_CONNECTION_ID')<br/>    ENVIRONMENT = Variable.get('ENVIRONMENT')<br/>    SLACK_CONN_ID = Variable.get("SLACK_WEBHOOK_ID")<br/>    MYAIRFLOW_DASHBOARD_URL = Variable.get("EXPAIRFLOW_DASHBOARD_URL")<br/>    AIRFLOW_DASHBOARD_URL = Variable.get("AIRFLOW_DASHBOARD_URL")</span><span id="809a" class="jp jq hi jg b fi jv js l jt ju">def standardize_naming(input):<br/>    return '{}_{}'.format(input, Variables.ENVIRONMENT)</span><span id="35a3" class="jp jq hi jg b fi jv js l jt ju">class Constants(object):<br/>    REDIS_KEY = standardize_naming("my-redis")<br/>    REDIS_CONNECION_SERVICE = #your redis service<br/>    ALPHANUM_UNDERSCORE_ONLY_REGEX = '[^0-9a-zA-Z_]+'<br/>    STRPTIME_FORMAT = '%Y-%m-%d %X'<br/>    MAX_ETL_DURATION_DAYS = 35<br/>    PG_QUERY = # your config query</span><span id="d75c" class="jp jq hi jg b fi jv js l jt ju">def remove_unsafe_character(input):<br/>    return re.sub(Constants.ALPHANUM_UNDERSCORE_ONLY_REGEX, '_', input)</span><span id="997b" class="jp jq hi jg b fi jv js l jt ju">def failed_task_slack_alert(context):<br/>    slack_webhook_token = BaseHook.get_connection(Variables.SLACK_CONN_ID).password</span><span id="6787" class="jp jq hi jg b fi jv js l jt ju">base_log_url = context.get('task_instance').log_url<br/>    updated_log_url = base_log_url.replace(Variables.AIRFLOW_DASHBOARD_URL,<br/>                                           Variables.MYAIRFLOW_DASHBOARD_URL)<br/>    slack_msg = """<br/>            :red_circle: Task Failed. <br/>            *Task*: {task}  <br/>            *Dag*: {dag} <br/>            *Execution Time*: {exec_date}  <br/>            *Log Url*: {log_url} <br/>            """.format(<br/>            task=context.get('task_instance').task_id,<br/>            dag=context.get('task_instance').dag_id,<br/>            ti=context.get('task_instance'),<br/>            exec_date=context.get('execution_date'),<br/>            log_url=updated_log_url,<br/>    )<br/>    failed_alert = SlackWebhookOperator(<br/>            task_id='slack_test',<br/>            http_conn_id=Variables.SLACK_CONN_ID,<br/>            webhook_token=slack_webhook_token,<br/>            message=slack_msg,<br/>            username='airflow')<br/>    return failed_alert.execute(context=context)</span><span id="ab29" class="jp jq hi jg b fi jv js l jt ju">default_args = {<br/>    "owner": "someone",<br/>    "depends_on_past": False,<br/>    "start_date": datetime(2019, 1, 1),<br/>    "email": [],<br/>    "email_on_failure": True,<br/>    "email_on_retry": False,<br/>    "retries": 1,<br/>    "retry_delay": timedelta(minutes=5),<br/>    "on_failure_callback": failed_task_slack_alert,<br/>    # 'queue': 'bash_queue',<br/>    # 'pool': 'backfill',<br/>    # 'priority_weight': 10,<br/>    # 'end_date': datetime(2016, 1, 1),<br/>}</span><span id="5e90" class="jp jq hi jg b fi jv js l jt ju">def deduplicate_email(emails):<br/>    tmp = set(emails)<br/>    return list(tmp)</span><span id="997f" class="jp jq hi jg b fi jv js l jt ju">def convertCamelToSnake(camel):<br/>    s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', camel)<br/>    s2 = re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()<br/>    s3 = s2.replace("-", "_").replace(".", "_")<br/>    while "__" in s3:<br/>        s3 = s3.replace("__", "_")<br/>    return s3</span><span id="d2f3" class="jp jq hi jg b fi jv js l jt ju">"""<br/><a class="ae jw" href="https://stackoverflow.com/questions/47705060/running-bigquery-query-uncached-using-python-api" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/questions/47705060/running-bigquery-query-uncached-using-python-api</a><br/><a class="ae jw" href="https://github.com/tylertreat/BigQuery-Python/blob/master/bigquery/client.py" rel="noopener ugc nofollow" target="_blank">https://github.com/tylertreat/BigQuery-Python/blob/master/bigquery/client.py</a><br/><a class="ae jw" href="https://stackoverflow.com/questions/47172150/google-cloud-bigquery-python-how-to-get-the-bytes-processed-by-query" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/questions/47172150/google-cloud-bigquery-python-how-to-get-the-bytes-processed-by-query</a><br/>"""</span><span id="fe1c" class="jp jq hi jg b fi jv js l jt ju">def create_dag(dag_id, config, default_args):<br/>    def dummy(task_id):<br/>        print(task_id)</span><span id="b38e" class="jp jq hi jg b fi jv js l jt ju">start_date = datetime.strptime(config['startdatestr'], '%Y-%m-%d')<br/>    if config['enddatestr'] is not None:<br/>        end_date = datetime.strptime(config['enddatestr'], '%Y-%m-%d')<br/>    else:<br/>        end_date = start_date + timedelta(days=Constants.MAX_ETL_DURATION_DAYS)</span><span id="1cf5" class="jp jq hi jg b fi jv js l jt ju">if len(config['notificationemails']) &gt; 0:<br/>        default_args['email'].extend([str(email) for email in config['notificationemails']])</span><span id="5226" class="jp jq hi jg b fi jv js l jt ju">default_args['email'] = deduplicate_email(default_args['email'])</span><span id="8d23" class="jp jq hi jg b fi jv js l jt ju">execution_period = 24</span><span id="ab61" class="jp jq hi jg b fi jv js l jt ju">dag = DAG(dag_id,<br/>              default_args=default_args,<br/>              schedule_interval=timedelta(hours=execution_period),<br/>              start_date=start_date,<br/>              end_date=end_date,<br/>              catchup=False)</span><span id="243d" class="jp jq hi jg b fi jv js l jt ju">logger.info("Exp Processing: Create DAG with ID {}".format(dag_id))</span><span id="5ca4" class="jp jq hi jg b fi jv js l jt ju">with dag:<br/>        """<br/>        common var<br/>        """</span><span id="58c0" class="jp jq hi jg b fi jv js l jt ju">start_date_safe = remove_unsafe_character(config['startdatestr'])<br/>        cur_date = '{{ ds }}'<br/>        cur_time = '{{ ts }}'<br/>        cur_date_safe = '{{ execution_date.strftime("%Y_%m_%d") }}'<br/>        version = str(config['version'])</span><span id="4825" class="jp jq hi jg b fi jv js l jt ju">task_id = '{prefix}_{name}_{version}'.format(<br/>                prefix=standardize_naming('my-dag'),<br/>                version=version)</span><span id="e2c2" class="jp jq hi jg b fi jv js l jt ju">task = PyhtonOperator(<br/>                task_id=task_id,<br/>                provide_context=True,<br/>                python_callable=dummy,<br/>                op_kwargs={'task_id': task_id}<br/>            )</span><span id="1f9e" class="jp jq hi jg b fi jv js l jt ju">return dag</span><span id="6133" class="jp jq hi jg b fi jv js l jt ju">"""<br/>handle datetime format when fetching from pg<br/>so that the fetch result can be stored as json instead of plain string<br/>"""</span><span id="60b3" class="jp jq hi jg b fi jv js l jt ju">def converter(o):<br/>    if isinstance(o, datetime):<br/>        return o.__str__()</span><span id="e0ca" class="jp jq hi jg b fi jv js l jt ju">"""<br/>Bahavior:<br/>Create an exact DAG which in turn will create it's own file<br/><a class="ae jw" href="https://www.astronomer.io/guides/dynamically-generating-dags/" rel="noopener ugc nofollow" target="_blank">https://www.astronomer.io/guides/dynamically-generating-dags/</a><br/>"""</span><span id="e287" class="jp jq hi jg b fi jv js l jt ju">redis_conn = redis.StrictRedis(host=Constants.REDIS_CONNECTION_SERVICE, port=6379,<br/>                               db=0)<br/>serialized_configs = redis_conn.get(Constants.REDIS_KEY)</span><span id="9aa5" class="jp jq hi jg b fi jv js l jt ju">configs = []<br/>if serialized_configs == None:<br/>    pg_hook = PostgresHook(postgres_conn_id=Variables.DATABASE_CONNECTION_ID)<br/>    pg_conn = pg_hook.get_conn()</span><span id="0efe" class="jp jq hi jg b fi jv js l jt ju">cursor = pg_conn.cursor(cursor_factory=RealDictCursor)<br/>    cursor.execute(Constannts.PG_QUERY)</span><span id="3a94" class="jp jq hi jg b fi jv js l jt ju">res = cursor.fetchall()</span><span id="aab3" class="jp jq hi jg b fi jv js l jt ju">logger.info(<br/>            "{res} fetched on {time}".format(res=json.dumps(res, default=converter),<br/>                                             time=datetime.now()))<br/>    # Set expiration to 1 minutes, to prevent to much query on the DB.<br/>    redis_conn.setex(Constants.REDIS_KEY, timedelta(minutes=1),<br/>                     json.dumps(res, default=converter))</span><span id="313d" class="jp jq hi jg b fi jv js l jt ju">configs = res<br/>else:<br/>    configs = json.loads(serialized_configs)</span><span id="4947" class="jp jq hi jg b fi jv js l jt ju">for config in configs:<br/>    if config['hasetlstarted'] == True:<br/>        id = re.sub(Constants.ALPHANUM_UNDERSCORE_ONLY_REGEX, '_',<br/>                            config['id'])<br/>        version = str(config['version'])<br/>        dag_id = "{prefix}_{id}_{version}".format(<br/>                prefix=standardize_naming('my_dag'),<br/>                id=id,<br/>                version=version)<br/>        logger.info("Dag with ID : {} is created.".format(dag_id))<br/>        globals()[dag_id] = create_dag(dag_id, config, default_args)</span></pre><p id="6090" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您还可以创建一个Cloudbuild文件来实现Github和Composer Airflow目录之间的同步:</p><pre class="jh ji jj jk fd jl jg jm jn aw jo bi"><span id="163b" class="jp jq hi jg b fi jr js l jt ju">steps:</span><span id="50dd" class="jp jq hi jg b fi jv js l jt ju">- name: 'google/cloud-sdk'<br/>  entrypoint: 'bash'<br/>  args:<br/>    - '-c'<br/>    - |<br/>      gsutil rsync -R dags ${_BUCKET}<br/>  id: 'deploy'</span><span id="34bd" class="jp jq hi jg b fi jv js l jt ju">substitutions:<br/>  _BUCKET: gs://&lt;your-composer-dags-gcs-id&gt;/dags</span></pre></div><div class="ab cl jx jy gp jz" role="separator"><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc"/></div><div class="hb hc hd he hf"><p id="b48c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我发现了一个问题，我的DAG在<a class="ae jw" href="https://stackoverflow.com/questions/51218314/dags-not-clickable-on-google-cloud-composer-webserver-but-working-fine-on-a-loc" rel="noopener ugc nofollow" target="_blank"> Composer Webserver </a>中无法点击。DAG不可点击,“最近的任务”和“DAG运行”列将永远加载。每个DAG名称旁边的“info”标记表示:</p><pre class="jh ji jj jk fd jl jg jm jn aw jo bi"><span id="fad2" class="jp jq hi jg b fi jr js l jt ju">This DAG isn't available in the webserver DagBag object. It shows up in this list because the scheduler marked it as active in the metadata database.</span></pre><figure class="jh ji jj jk fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es ke"><img src="../Images/23e0a460710c812d835df44075cd6596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nNiBLmeHnb0h9GDzgjmHow.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">无法在Composer Webserver中单击动态创建的DAG。</figcaption></figure><p id="a529" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是因为Webserver使用App Engine，而元数据本身存储在Cloud SQL和GCS中，不直接与Composer集群内部的本地Redis通信。事实上，自动生成的DAG是由一个文件生成的，其中关于DAG的所有信息仅存在于Composer集群内的Redis中，这可能会导致此类问题。您可以通过部署您的自我管理气流网络服务器来处理此问题，该服务器可在<a class="ae jw" href="https://cloud.google.com/composer/docs/how-to/managing/deploy-webserver" rel="noopener ugc nofollow" target="_blank">中找到</a>。然而，如果你想使用公共API访问你的网络服务器，你需要创建一个入口。</p><pre class="jh ji jj jk fd jl jg jm jn aw jo bi"><span id="2859" class="jp jq hi jg b fi jr js l jt ju">apiVersion: v1<br/>items:<br/>- apiVersion: extensions/v1beta1<br/>  kind: Ingress<br/>  metadata:<br/>    annotations:<br/>      kubernetes.io/ingress.global-static-ip-name: &lt;YOUR-STATIC-IP-THAT_ALREADY-BIND-WITH-AN-IP&gt;<br/>    name: airflow-webserver<br/>    namespace: &lt;YOUR-NAMESPACE&gt;<br/>  spec:<br/>    backend:<br/>      serviceName: airflow-webserver-service<br/>      servicePort: 80<br/>    tls:<br/>    - secretName: &lt;YOUr-SSL-SECRET&gt;</span></pre><p id="3a65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用你想要的任何东西来保护它，对我来说，我使用的是谷歌IAP。</p><p id="6592" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我能想到的一切。再见！👋</p></div></div>    
</body>
</html>