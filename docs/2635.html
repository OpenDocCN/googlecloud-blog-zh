<html>
<head>
<title>Dataplex — Data Processing using Custom Pyspark/Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Dataplex —使用定制Pyspark/Spark进行数据处理</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/dataplex-data-processing-using-custom-pyspark-spark-1cb73753fa03?source=collection_archive---------1-----------------------#2022-12-15">https://medium.com/google-cloud/dataplex-data-processing-using-custom-pyspark-spark-1cb73753fa03?source=collection_archive---------1-----------------------#2022-12-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/cbe2ddadc422fb8b2a4411365a13b78c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZWPmigBWcnTIDPObD08_7Q.png"/></div></div></figure><h2 id="5f8e" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">简介:</strong></h2><p id="58b3" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi kj translated"><span class="l kk kl km bm kn ko kp kq kr di"> D </span>它还集中了安全性和治理，实现了数据的分布式所有权和全局控制。Dataplex为存储在云存储和大查询中的数据提供自动元数据发现。除此之外，用户可以安全地访问数据，并通过BigQuery作为外部表进行查询。用户可以运行data quality和custom PySpark，后者运行无服务器批处理任务。Dataplex还提供了完全管理的、无服务器的Spark环境，可以简单地访问笔记本和SparkSQL查询。</p><p id="6894" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated"><strong class="jq hj">在这篇文章中，我们将关注两个主要方面:</strong></p><ul class=""><li id="533a" class="kx ky hi jq b jr ks jv kt jb kz jf la jj lb ki lc ld le lf bi translated">设置数据管理—湖、区域、资产。</li><li id="8e9e" class="kx ky hi jq b jr lg jv lh jb li jf lj jj lk ki lc ld le lf bi translated">使用自定义PySpark任务设置和运行转换。</li></ul><p id="4f5a" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">将详细介绍每个领域以及运行它所需的步骤。</p><h2 id="00f3" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">先决条件:</strong></h2><p id="07c7" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">要访问Dataplex UI并运行定制的PySpark任务，用户应该对<a class="ae ll" href="https://cloud.google.com/iam/docs/understanding-roles#cloud-dataplex-roles" rel="noopener ugc nofollow" target="_blank"> Cloud IAM </a>中授予的角色或服务帐户拥有适当的访问权限。如果数据转换任务需要读取或更新附加到lake的Dataproc Metastore实例，那么服务帐户也需要<a class="ae ll" href="https://cloud.google.com/dataproc-metastore/docs/iam-roles#predefined-roles" rel="noopener ugc nofollow" target="_blank"> Dataproc Metastore查看器或编辑器</a>角色。</p><h2 id="c131" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">设置数据管理—湖泊、区域、资产:</strong></h2><blockquote class="lm ln lo"><p id="bdb6" class="jo jp lp jq b jr ks jt ju jv kt jx jy lq ku ka kb lr kv kd ke ls kw kg kh ki hb bi translated"><strong class="jq hj">步骤1:创建数据湖:</strong></p></blockquote><p id="e66e" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">从导航菜单&gt;分析&gt;选择Dataplex &gt;管理湖泊-&gt;选择管理-&gt;创建</p><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/038904ca456fc0adff43ec7c869f8488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v_OHwPPEjMvUeKFs"/></div></div></figure><blockquote class="lm ln lo"><p id="5abf" class="jo jp lp jq b jr ks jt ju jv kt jx jy lq ku ka kb lr kv kd ke ls kw kg kh ki hb bi translated"><strong class="jq hj">第二步:定义数据区:</strong></p></blockquote><p id="7596" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">这是建立数据湖的重要一步。区域限制了您可以存储的数据类型。数据湖中有两种类型的区域:</p><ul class=""><li id="27e2" class="kx ky hi jq b jr ks jv kt jb kz jf la jj lb ki lc ld le lf bi translated"><strong class="jq hj">原始区域:</strong>原始格式的数据，没有经过严格的类型检查。</li><li id="4fce" class="kx ky hi jq b jr lg jv lh jb li jf lj jj lk ki lc ld le lf bi translated"><strong class="jq hj">精选区域:</strong>经过清理、格式化并准备好进行分析的数据。在Parquet、Avro、Orc文件或BigQuery表中，数据是柱状的、配置单元分区的。例如，对数据进行类型检查，以禁止使用CSV文件，因为它们在SQL访问中表现不佳。</li></ul><p id="c7b3" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">我们将在<strong class="jq hj">湖</strong> (Dataplex-demo)下创建<strong class="jq hj"> Raw (RAW_ZONE) </strong>和<strong class="jq hj">Curated(Curated _ ZONE)</strong><strong class="jq hj">ZONE</strong>。对于每个创建的区域，Dataplex自动创建一个BigQuery数据集。</p><p id="6ecd" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">单击新创建的数据湖&gt;单击“Zones”选项卡下的“Add Zone”。</p><p id="03d5" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">按照相同的步骤创建所需的分区(原始/管理的)。</p><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/30abed6617a60b01221f09ab2ace05dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gVioF35z5HYgbsoG"/></div></div></figure><blockquote class="lm ln lo"><p id="8991" class="jo jp lp jq b jr ks jt ju jv kt jx jy lq ku ka kb lr kv kd ke ls kw kg kh ki hb bi translated"><strong class="jq hj">第三步:添加资产:</strong></p></blockquote><p id="b853" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">在这一步中，我们将在所需的区域下添加资产，即映射到云存储或BigQuery中存储的数据。您可以将存储在单独的Google Cloud项目中的数据作为资产映射到一个区域中。完成此操作后，数据发现作业将访问元数据，我们可以看到代表结构化和半结构化数据(表)以及非结构化数据(文件)的元数据的实体。</p><p id="6640" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">单击创建的原始区域&gt;单击添加资产&gt;在类型下-&gt;选择存储桶-&gt;为原始数据选择所需的桶-&gt;提供发现设置(继承/覆盖)-&gt;检查资产-&gt;提交</p><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/df53010a78e7fe08aaeac6f17e8f36a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9LHqSw664xbIzvpV"/></div></div></figure><p id="8334" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">要在区域中添加资产，我们有两个选项:</p><ul class=""><li id="b4d4" class="kx ky hi jq b jr ks jv kt jb kz jf la jj lb ki lc ld le lf bi translated">存储桶</li><li id="085a" class="kx ky hi jq b jr lg jv lh jb li jf lj jj lk ki lc ld le lf bi translated">大型查询数据集</li></ul><p id="6144" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">注意:当将存储桶设置为资产时，应该注意在桶中创建一个对象文件夹，因为对于每个文件夹，BigQuery数据集中都会有一个表。Dataplex自动创建带有区域名称的数据集(原始/管理的),该数据集是在湖中创建的。在存储桶中创建对象文件夹(前缀)的原因是因为资产遵循配置单元样式约定，文件夹代表具有相似模式的一个实体或一组实体。</p><p id="fe7c" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">Dataplex将这些资产作为两个单独的表来发现，每个前缀/文件夹可以托管许多Hive风格的分区文件，Dataplex将发现这些文件并创建一个表，只要这些文件具有相同的模式。</p><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/2f677f39370dcec79e4197758583280a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fLh4Fwg4PPoAdSBr"/></div></div></figure><h2 id="1f1d" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">使用自定义PySpark任务设置并运行转换:</strong></h2><p id="6486" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">现在，<strong class="jq hj"> </strong>我们有了所需区域中的资产，我们创建了一个自定义PySpark任务，并通过传递参数运行转换作业，从原始区域资产中读取数据并转换它，然后将其写入与管理的区域资产映射的云存储桶中。Dataplex支持使用cron或一次性运行来调度定制代码的执行。我们可以使用Spark (Java)、PySpark ( &lt; =3.2)或SparkSql来调度定制任务。这些脚本通过无服务器Spark处理和内置的无服务器调度程序执行。</p><p id="969c" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">在进入下一步之前，您一定想知道在Dataplex上运行定制PySpark任务的<strong class="jq hj">好处是什么:</strong>运行定制PySpark任务可以更加灵活地编写转换作业，并将它们存储在一个管理区域中，该区域将自动在BigQuery中创建表。除此之外，熟悉Spark背景的开发人员将能够轻松地用Java、Spark、PySpark或Spark SQL编写转换代码。</p><blockquote class="lm ln lo"><p id="d5b3" class="jo jp lp jq b jr ks jt ju jv kt jx jy lq ku ka kb lr kv kd ke ls kw kg kh ki hb bi translated"><strong class="jq hj">步骤1:准备PySpark脚本并将其放入GCS桶中。</strong></p></blockquote><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">data plex _ custom _ spark _ template . py</figcaption></figure><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/0392bd730b42ba04da20cecd8ef6ce43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-YreA4ZbwaBdCKPuf2IlIQ.png"/></div></div></figure><p id="0ff3" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">放置在GCS bucket中的上述脚本读取Dataplex Create Task UI中Add Argument部分指定的参数。该脚本采用源、目标路径以及源和目标格式，运行转换，并将其写入在管理区域中注册为资产的目标路径。</p><blockquote class="lm ln lo"><p id="6746" class="jo jp lp jq b jr ks jt ju jv kt jx jy lq ku ka kb lr kv kd ke ls kw kg kh ki hb bi translated"><strong class="jq hj">第二步:启用谷歌私人访问:</strong></p></blockquote><p id="c3f7" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">为您的网络和/或子网络启用私人Google访问。如果未指定，它将采用默认子网。将在默认子网中启用私人Google访问。</p><p id="66ff" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">从导航菜单&gt;网络&gt;选择VPC网络&gt;点击默认-&gt;选择将运行Dataplex任务的子网区域-&gt;点击编辑-&gt;启用私有Google访问。</p><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/570a83f20b30c2b05163779f75717a89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8bhSwt-ChFIFRtWC"/></div></div></figure><blockquote class="lm ln lo"><p id="9f03" class="jo jp lp jq b jr ks jt ju jv kt jx jy lq ku ka kb lr kv kd ke ls kw kg kh ki hb bi translated"><strong class="jq hj">第三步:创建一个定制的Spark任务:</strong></p></blockquote><p id="b553" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">按照以下步骤创建Spark任务:</p><p id="9583" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">在管理湖泊-&gt;选择过程-&gt;创建任务-&gt;点击创建自定义Spark任务。</p><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/4321118e44e368b016cca250d4fa26fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XxGyamkMZ8wlhoa7"/></div></div></figure><blockquote class="lm ln lo"><p id="4361" class="jo jp lp jq b jr ks jt ju jv kt jx jy lq ku ka kb lr kv kd ke ls kw kg kh ki hb bi translated"><strong class="jq hj">步骤3a:选择Dataplex Lake，输入任务配置的详细信息:</strong></p></blockquote><ul class=""><li id="ef18" class="kx ky hi jq b jr ks jv kt jb kz jf la jj lb ki lc ld le lf bi translated"><strong class="jq hj">类型:</strong> Spark/ PySpark。</li><li id="b657" class="kx ky hi jq b jr lg jv lh jb li jf lj jj lk ki lc ld le lf bi translated"><strong class="jq hj">主类或Jar文件:</strong>对于PySpark:完全合格的云存储URI，对于Spark是完全合格的类名。</li><li id="a5e9" class="kx ky hi jq b jr lg jv lh jb li jf lj jj lk ki lc ld le lf bi translated"><strong class="jq hj">文件uri:</strong>每个执行器的工作目录中要放置的文件的GCS URIs，例如:log4j-spark-driver-template . properties文件可以放在每个执行器中进行日志记录。</li><li id="cc48" class="kx ky hi jq b jr lg jv lh jb li jf lj jj lk ki lc ld le lf bi translated"><strong class="jq hj">归档文件:</strong>归档文件提取在Spark工作目录下。</li><li id="fd03" class="kx ky hi jq b jr lg jv lh jb li jf lj jj lk ki lc ld le lf bi translated"><strong class="jq hj">添加参数:</strong>输入要在GCS Python脚本中传递的参数。示例:输入源、目标路径和使用的源、目标格式。</li></ul><blockquote class="lm ln lo"><p id="97ce" class="jo jp lp jq b jr ks jt ju jv kt jx jy lq ku ka kb lr kv kd ke ls kw kg kh ki hb bi translated"><strong class="jq hj">步骤3b:设置日程:我们可以在两个选项中选择:</strong></p></blockquote><ul class=""><li id="e596" class="kx ky hi jq b jr ks jv kt jb kz jf la jj lb ki lc ld le lf bi translated">运行一次:选择立即或Cron选项。</li><li id="9fe1" class="kx ky hi jq b jr lg jv lh jb li jf lj jj lk ki lc ld le lf bi translated">每日:选择任何选项，如每日、每周、每月或自定义。</li></ul><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/f930b6150304be5ac07d368f442495f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oTwwcBq62h1JBzol"/></div></div></figure><blockquote class="lm ln lo"><p id="9715" class="jo jp lp jq b jr ks jt ju jv kt jx jy lq ku ka kb lr kv kd ke ls kw kg kh ki hb bi translated"><strong class="jq hj">步骤3c: </strong> <strong class="jq hj">选择自定义资源- &gt;网络配置- &gt;检查默认VPC应被选中，因为我们为默认VPC启用了私人Google访问。</strong></p></blockquote><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/c370cf0320728f99fbe9fa3622e97603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7MhJe2DTMfRu3353"/></div></div></figure><blockquote class="lm ln lo"><p id="f964" class="jo jp lp jq b jr ks jt ju jv kt jx jy lq ku ka kb lr kv kd ke ls kw kg kh ki hb bi translated"><strong class="jq hj">步骤3d: </strong> <strong class="jq hj">验证您运行的自定义Spark作业:</strong></p></blockquote><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/b73176347a8d17b3ffd31e40eea047d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WxfaZHqd_Em59U0-"/></div></div></figure><h2 id="f9b6" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结论</h2><p id="8e55" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">在本文中，我们看到了如何在Dataplex中配置和运行PySpark任务来进行数据转换和处理。我们还讨论了如何传递多个参数并在PySpark脚本中使用它来执行转换，以及在Dataplex中使用数据管理时需要注意的各种问题。</p><h2 id="c4bc" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">参考</h2><ul class=""><li id="f38d" class="kx ky hi jq b jr js jv jw jb mg jf mh jj mi ki lc ld le lf bi translated"><a class="ae ll" href="https://cloud.google.com/dataplex/docs/schedule-custom-spark-tasks" rel="noopener ugc nofollow" target="_blank">https://cloud . Google . com/data plex/docs/schedule-custom-spark-tasks</a></li><li id="1e76" class="kx ky hi jq b jr lg jv lh jb li jf lj jj lk ki lc ld le lf bi translated"><a class="ae ll" href="https://cloud.google.com/dataplex/docs/introduction" rel="noopener ugc nofollow" target="_blank">https://cloud.google.com/dataplex/docs/introduction</a></li></ul><p id="5f63" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">希望你喜欢这篇文章，并发现它有用。感谢<a class="mj mk ge" href="https://medium.com/u/84d86011e914?source=post_page-----1cb73753fa03--------------------------------" rel="noopener" target="_blank"> KARTIK MALIK </a>的投入。你可以通过<a class="ae ll" href="https://www.linkedin.com/in/shashank-t-743126104/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系到我。</p></div></div>    
</body>
</html>