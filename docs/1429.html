<html>
<head>
<title>GCS Authentication Using Apache Hadoop Credential Provider in Dataproc</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Dataproc中使用Apache Hadoop凭证提供程序进行GCS认证</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/gcs-authentication-using-apache-hadoop-credential-provider-in-dataproc-c5b9927c976b?source=collection_archive---------0-----------------------#2020-05-18">https://medium.com/google-cloud/gcs-authentication-using-apache-hadoop-credential-provider-in-dataproc-c5b9927c976b?source=collection_archive---------0-----------------------#2020-05-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3ca4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章提供了使用Apache Hadoop凭证提供者和Dataproc中的服务帐户凭证进行GCS认证的概述和示例。对于Hadoop用户的应用程序，这种技术可以无缝检索凭证文件中的秘密，这些凭证文件需要访问云存储中的数据集。</p><h1 id="0964" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">概观</h1><p id="4b56" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated"><a class="ae kg" href="https://github.com/GoogleCloudDataproc/hadoop-connectors" rel="noopener ugc nofollow" target="_blank"> GCS连接器</a>为Dataproc Hadoop应用程序(即Spark、MapReduce、Hive)来访问存储在外部GCS桶中的数据集。默认情况下，对GCS的认证和访问不是由提交作业的Hadoop用户完成的，而是通过运行Dataproc集群的VM实例的<a class="ae kg" href="https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_cloud_dataproc" rel="noopener ugc nofollow" target="_blank">主机服务帐户完成的。</a></p><p id="9ed2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kg" href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CredentialProviderAPI.html" rel="noopener ugc nofollow" target="_blank"> Apache Hadoop凭证提供者</a>使该机制能够存储和检索加密文件中的秘密(即JCEKS文件)放在本地或Hadoop文件系统上。此外，GCS连接器支持Hadoop应用程序在访问GCS时使用安全存储的凭证进行检索和验证。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/3cb1bc279c0f275a593ed74d5153340d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HARtPL1U8X2smSZNj64UoQ.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">[图表1 —高层流程]</figcaption></figure><p id="b16f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图提供了两个流程，通过1)集群的默认主机服务帐户和2)使用存储在Hadoop凭据提供程序中的服务帐户机密来区分Spark应用程序请求的GCS数据访问。在本例中，默认服务帐户<code class="du kx ky kz la b">sa-dataproc-instance</code>没有被授予访问<code class="du kx ky kz la b">data-bucket-a</code>的IAM角色，但是<code class="du kx ky kz la b">sa-data-access-a</code>可以访问执行作业所需的数据。提交作业和访问GCS中存储的数据集时，序列1的流程是默认流程。在序列2中，spark应用程序定义了<em class="lb">Hadoop . security . credential . provider . path</em>，当应用程序请求从云存储桶中读取数据时，GCS连接器使用凭据提供程序检索服务帐户凭据，并使用这些凭据进行身份验证以访问<code class="du kx ky kz la b">data-bucket-a</code>。</p><p id="e56a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇博客的后面，我们将详细介绍创建<code class="du kx ky kz la b">sa-data-access</code>服务帐户的步骤，将凭证存储在加密的JCEKS文件中，并使用它来访问Google云存储。</p><h1 id="d555" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">集群安全设计</h1><p id="a528" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">实施安全措施来保护提供GCS数据访问的凭据非常重要。在本节中，我们将回顾在HDFS锁定这些凭据的高级原则。</p><p id="022a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Java密钥库文件存储使用Hadoop凭据实用程序创建的加密服务帐户凭据。该实用程序可以直接创建加密的秘密，并使用默认权限600将它们存储在HDFS中。应该在文件和父目录上正确设置HDFS ACL，以便只允许应该访问此凭据的Hadoop用户。</p><p id="49ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果一个Dataproc集群仅由一个租户使用，标准的GCP周边安全措施可以在GCP项目级和Dataproc集群实施，以防止对集群和机密的非法访问。这包括只允许被授权的用户访问项目和Dataproc集群，这些用户应该能够访问凭证。此外，确保Dataproc作业API和Hadoop APIs，如HDFS和YARN端点，包括WebHDFS、HTTP Rest API等，同样只能由有权访问底层凭证的用户访问。</p><p id="ac44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果一个Dataproc集群有多个租户，那么操作员需要确保启用OSLogin来管理SSH访问和sudo限制，但是还需要通过Kerberos启用Hadoop安全模式来强制验证Hadoop用户。经过身份验证的Hadoop用户确保只有被允许对加密的JCEKS文件应用HDFS ACL的用户才可以访问。如果没有Kerberos身份验证，Hadoop用户可以很容易地模拟其他用户并绕过访问控制。</p><p id="f4ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用HDFS权限和所有权来保护JCEKS文件，可以确保只有经过授权的用户才能在访问GCS时使用这些凭证。其他授权方法，如HDFS ACL和Apache Ranger也可用于保护措施。</p><blockquote class="lc ld le"><p id="1671" class="if ig lb ih b ii ij ik il im in io ip lf ir is it lg iv iw ix lh iz ja jb jc hb bi translated">*可以通过在hdfs-site.xml中将dfs.permissions.enabled设置为true来启用HDFS目录和文件权限/所有权</p><p id="1b1c" class="if ig lb ih b ii ij ik il im in io ip lf ir is it lg iv iw ix lh iz ja jb jc hb bi translated">* HDFS ACL扩展了权限/所有权模型，能够为其他用户和组应用访问相同文件/目录的附加权限。可以通过在hdfs-site.xml中将dfs.namenode.acls.enabled设置为true来启用ACL。</p></blockquote><p id="f1b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，作为额外的最佳实践，可以定期轮换服务帐户密钥。要做到这一点，只需为同一个服务帐户创建一个新的密钥，使用更新后的凭证生成并部署一个新的JCEKS文件，并在所有Hadoop应用程序切换到使用最新的凭证文件后删除旧的服务帐户密钥。</p><h1 id="66c9" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">创建和使用凭据</h1><p id="555e" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">本节将介绍以下步骤:</p><ol class=""><li id="b383" class="li lj hi ih b ii ij im in iq lk iu ll iy lm jc ln lo lp lq bi translated">创建服务帐户<code class="du kx ky kz la b">sa-data-access-a</code> <br/> *授权通过IAM访问GCS <code class="du kx ky kz la b">data-bucket-a</code></li><li id="da5d" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">使用Hadoop Credential Utility<br/>* Secure JCEKS file在HDFS创建带有服务帐户凭证的JCE ks加密文件</li><li id="2da3" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">使用JCEKS凭据通过Hadoop fs命令、MapReduce、Spark或Hive应用程序访问GC。</li></ol><h1 id="a854" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">1.创建服务帐户并授权访问</h1><p id="6056" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">在第一步中，我们创建一个服务帐户，并为其应用一个IAM角色来访问GCS bucket <code class="du kx ky kz la b">data-bucket-a</code>。</p><h2 id="f700" class="lw je hi bd jf lx ly lz jj ma mb mc jn iq md me jr iu mf mg jv iy mh mi jz mj bi translated">创建服务帐户和JSON密钥</h2><pre class="ki kj kk kl fd mk la ml mm aw mn bi"><span id="ed92" class="lw je hi la b fi mo mp l mq mr">PROJECT_ID=jh-data-sandbox<br/>SERVICE_ACCOUNT=sa-data-access-a</span><span id="f044" class="lw je hi la b fi ms mp l mq mr">gcloud iam service-accounts create ${SERVICE_ACCOUNT} \<br/>  --description="sa-data-access-a description" \<br/>  --display-name=${SERVICE_ACCOUNT}</span><span id="7312" class="lw je hi la b fi ms mp l mq mr">gcloud iam service-accounts keys \<br/>  create ~/${SERVICE_ACCOUNT}-key.json \<br/>  --iam-account ${SERVICE_ACCOUNT}@${PROJECT_ID}.iam.gserviceaccount.com</span></pre><h2 id="5bde" class="lw je hi bd jf lx ly lz jj ma mb mc jn iq md me jr iu mf mg jv iy mh mi jz mj bi translated">通过应用IAM权限来授权访问</h2><p id="d63b" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">授权服务帐户<em class="lb"> storage.buckets.get </em>对Dataproc集群的GCS bucket的权限:</p><pre class="ki kj kk kl fd mk la ml mm aw mn bi"><span id="30a5" class="lw je hi la b fi mo mp l mq mr">BUCKET=`gcloud dataproc clusters describe dataproc-015 --format="value(config.configBucket)"`</span><span id="7a4e" class="lw je hi la b fi ms mp l mq mr">gsutil iam ch serviceAccount:${SERVICE_ACCOUNT}@${PROJECT_ID}.iam.gserviceaccount.com:roles/storage.legacyBucketReader gs://${BUCKET}</span></pre><p id="7777" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将服务帐户权限授权给数据访问所需的其他GCS存储桶:</p><pre class="ki kj kk kl fd mk la ml mm aw mn bi"><span id="0a23" class="lw je hi la b fi mo mp l mq mr">gsutil iam ch serviceAccount:${SERVICE_ACCOUNT}@${PROJECT_ID}.iam.gserviceaccount.com:roles/storage.legacyBucketWriter gs://data-bucket-a</span></pre><h1 id="00a0" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">2.创建Hadoop凭据JCEKS文件</h1><pre class="ki kj kk kl fd mk la ml mm aw mn bi"><span id="6484" class="lw je hi la b fi mo mp l mq mr">hadoop credential create fs.gs.auth.service.account.email  \<br/>  -provider jceks://hdfs/app/client-app-a/sa-data-access-a.jceks \<br/>  -value "sa-data-access-a@jh-data-sandbox.iam.gserviceaccount.com"</span><span id="a160" class="lw je hi la b fi ms mp l mq mr">hadoop credential create fs.gs.auth.service.account.private.key.id \<br/>  -provider jceks://hdfs/app/client-app-a/sa-data-access-a.jceks \<br/>  -value "0a4e5cb521e2b7c75d082d7069f1cff75071f814"</span><span id="f50b" class="lw je hi la b fi ms mp l mq mr">hadoop credential create fs.gs.auth.service.account.private.key \<br/>  -provider jceks://hdfs/app/client-app-a/sa-data-access-a.jceks \<br/>  -value "-----BEGIN PRIVATE KEY-----\n redacted \n-----END PRIVATE KEY-----\n"</span><span id="b5b2" class="lw je hi la b fi ms mp l mq mr"># apply HDFS ownership to appropriate users and groups requiring access<br/>hadoop fs -chown -R ${USER}:${USER} /app/client-app-a<br/>hadoop fs -chmod 500 /app/client-app-a<br/>hadoop fs -chmod 400 /app/client-app-a/sa-data-access-a.jceks</span><span id="147b" class="lw je hi la b fi ms mp l mq mr"># apply HDFS ACLs (extended acls for advanced setup) for read permission for users and groups requiring access</span></pre><h1 id="b959" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">3.通过GCS连接器使用来自凭据提供者的机密</h1><h2 id="6804" class="lw je hi bd jf lx ly lz jj ma mb mc jn iq md me jr iu mf mg jv iy mh mi jz mj bi translated">Hadoop客户端</h2><pre class="ki kj kk kl fd mk la ml mm aw mn bi"><span id="77c5" class="lw je hi la b fi mo mp l mq mr">hadoop fs -Dhadoop.security.credential.provider.path=jceks://hdfs/app/client-app-a/sa-data-access-a.jceks -ls gs://data-bucket-a/</span></pre><h2 id="14ec" class="lw je hi bd jf lx ly lz jj ma mb mc jn iq md me jr iu mf mg jv iy mh mi jz mj bi translated">MapReduce作业</h2><pre class="ki kj kk kl fd mk la ml mm aw mn bi"><span id="268f" class="lw je hi la b fi mo mp l mq mr">hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen -Dhadoop.security.credential.provider.path=jceks://hdfs/app/client-app-a/sa-data-access-a.jceks 100000 gs://data-bucket-a/write-data-test/</span></pre><h2 id="0d48" class="lw je hi bd jf lx ly lz jj ma mb mc jn iq md me jr iu mf mg jv iy mh mi jz mj bi translated">火花</h2><pre class="ki kj kk kl fd mk la ml mm aw mn bi"><span id="e189" class="lw je hi la b fi mo mp l mq mr">spark-submit --class org.apache.spark.examples.DFSReadWriteTest \<br/>  --master yarn \<br/>  --deploy-mode cluster \<br/>  --num-executors 3 \<br/>  --conf spark.hadoop.hadoop.security.credential.provider.path=jceks://hdfs/app/client-app-a/sa-data-access-a.jceks \<br/>  /usr/lib/spark/examples/jars/spark-examples.jar \<br/>  /var/log/google-dataproc-agent.0.log \<br/>  gs://data-bucket-a/test-dfs-read-write/</span></pre><h2 id="fa23" class="lw je hi bd jf lx ly lz jj ma mb mc jn iq md me jr iu mf mg jv iy mh mi jz mj bi translated">储备</h2><pre class="ki kj kk kl fd mk la ml mm aw mn bi"><span id="0d8f" class="lw je hi la b fi mo mp l mq mr"># hive - table x is an external table stored on gs://data-bucket-a/x<br/>hive --hiveconf hadoop.security.credential.provider.path=jceks://hdfs/app/client-app-a/sa-data-access-a.jceks -e 'select count(1) from x;'</span><span id="2069" class="lw je hi la b fi ms mp l mq mr"># beeline <br/>beeline -u "jdbc:hive2://dataproc-015-m:10000/default;principal=hive/dataproc-015-m@DATAPROC-015.ACME.COM"  --hiveconf hadoop.security.credential.provider.path=jceks://hdfs/app/client-app-a/sa-data-access-a.jceks -e 'select count(1) from x;'</span></pre><h1 id="5fee" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">摘要</h1><p id="283f" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">总之，我们提供了一个示例来演示在执行Hadoop应用程序时，如何使用Apache Hadoop凭证提供程序无缝地访问存储在Java KeyStore文件中的加密服务帐户凭证。通过使用权限保护HDFS的JCEKS文件，并通过Kerberos执行身份验证，只有被允许使用这些秘密的应用程序和用户才能使用它们来访问云存储中的数据集。这种技术提供了一种机制和额外的选项，可以使用不同的凭证来验证和访问存储在GCS中的数据。</p><h1 id="7e08" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">资源</h1><ul class=""><li id="5fe8" class="li lj hi ih b ii kb im kc iq mt iu mu iy mv jc mw lo lp lq bi translated"><a class="ae kg" href="https://github.com/GoogleCloudDataproc/hadoop-connectors" rel="noopener ugc nofollow" target="_blank"> GCS连接器git库</a></li><li id="0152" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc mw lo lp lq bi translated"><a class="ae kg" href="https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/CONFIGURATION.md#authentication" rel="noopener ugc nofollow" target="_blank"> GCS连接器认证配置详情</a></li><li id="9580" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc mw lo lp lq bi translated"><a class="ae kg" href="https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts" rel="noopener ugc nofollow" target="_blank">data proc文档中的服务帐户</a></li><li id="dc68" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc mw lo lp lq bi translated"><a class="ae kg" href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CredentialProviderAPI.html" rel="noopener ugc nofollow" target="_blank"> Apache Hadoop凭证提供者指南</a></li></ul></div></div>    
</body>
</html>