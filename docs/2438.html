<html>
<head>
<title>Hive to BigQuery: Move Data efficiently using GCP Dataproc Serverless</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Hive to BigQuery:使用GCP Dataproc无服务器高效地移动数据</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/hive-to-bigquery-move-data-efficiently-using-gcp-dataproc-serverless-ee30d35aaf03?source=collection_archive---------1-----------------------#2022-10-19">https://medium.com/google-cloud/hive-to-bigquery-move-data-efficiently-using-gcp-dataproc-serverless-ee30d35aaf03?source=collection_archive---------1-----------------------#2022-10-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d6ef68dc37d2d5053c14d4f44aad6514.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ck_nDqG1Nzox583KDifHPw.jpeg"/></div></div></figure></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><p id="e1c6" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">任何云迁移活动的主要要求之一是将数据从本地/云服务器迁移到现代数据仓库，如BigQuery。将数据从Hadoop迁移到BigQuery是业内相当常见的用例。它通常包括多步过程，包括使用DistCP或其他方法将数据迁移到GCS，然后构建从GCS到BigQuery的管道。</p><p id="ae78" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><a class="ae jv" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> Dataproc模板</a>使用<a class="ae jv" href="https://cloud.google.com/vertex-ai/docs/tutorials/jupyter-notebooks" rel="noopener ugc nofollow" target="_blank"> VertexAI笔记本</a>和<a class="ae jv" href="https://cloud.google.com/dataproc-serverless/docs" rel="noopener ugc nofollow" target="_blank"> Dataproc无服务器</a>提供一站式解决方案，将数据从任何Hadoop环境中的Hive表直接迁移到GCP BigQuery。</p><p id="c2aa" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">设置VertexAI笔记本:</strong></p><ol class=""><li id="9390" class="jw jx hi iz b ja jb je jf ji jy jm jz jq ka ju kb kc kd ke bi translated">从<a class="ae jv" href="https://console.developers.google.com/" rel="noopener ugc nofollow" target="_blank"> API控制台</a>启用GCP项目中的以下服务:</li></ol><ul class=""><li id="b08d" class="jw jx hi iz b ja jb je jf ji jy jm jz jq ka ju kf kc kd ke bi translated">计算引擎API</li><li id="17c8" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kf kc kd ke bi translated">Dataproc API</li><li id="ab3d" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kf kc kd ke bi translated">顶点人工智能应用编程接口</li><li id="63cb" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kf kc kd ke bi translated">顶点笔记本API</li></ul><p id="ab12" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">2.在<a class="ae jv" href="https://pantheon.corp.google.com/vertex-ai/workbench/list/instances" rel="noopener ugc nofollow" target="_blank">顶点AI工作台中创建一个<a class="ae jv" href="https://cloud.google.com/vertex-ai/docs/workbench/user-managed/introduction" rel="noopener ugc nofollow" target="_blank">用户管理的</a>笔记本。</a></p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kl"><img src="../Images/3ade8fd670d9eba1c2aadaa8cd1bee72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jtegDP1x9AGAGOeC.png"/></div></div></figure><p id="c86d" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">3.使用GIT选项卡克隆Dataproc模板GitHub repo，如下图所示</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kq"><img src="../Images/6a99b1aff57fe78c90a973deee0a41ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fxf16h5v4Ialq-4Ejv-LBQ.png"/></div></div></figure><p id="2de6" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">或者从启动窗口打开一个终端，使用git clone进行克隆。</p><pre class="km kn ko kp fd kr ks kt ku aw kv bi"><span id="207d" class="kw kx hi ks b fi ky kz l la lb">git clone <a class="ae jv" href="https://github.com/GoogleCloudPlatform/dataproc-templates.git" rel="noopener ugc nofollow" target="_blank">https://github.com/GoogleCloudPlatform/dataproc-templates.git</a></span></pre><p id="570d" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">4.从文件夹选项卡中，打开路径中的<a class="ae jv" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/notebooks/hive2bq/HiveToBigquery_notebook.ipynb" rel="noopener ugc nofollow" target="_blank">hivetobiqquery _ notebook . ipynb</a>notebook:data proc-templates/notebooks/hive 2 bq</p><p id="a1b2" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">5.为了方便用户，下面的步骤被顺序标记并在Python笔记本中实现。</p><p id="db3f" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">第一步:安装所需的软件包<br/> </strong>迁移所需的一些软件包需要单独安装，因为它们在笔记本中不可用，例如PySpark、JDK等。</p><p id="94d3" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">第二步:设置Google Cloud属性<br/> </strong>运行笔记本前需要设置以下配置:</p><ul class=""><li id="677b" class="jw jx hi iz b ja jb je jf ji jy jm jz jq ka ju kf kc kd ke bi translated"><code class="du lc ld le ks b">REGION</code> : GCP地区卸载BQ中的Hive表。</li><li id="8586" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kf kc kd ke bi translated"><code class="du lc ld le ks b">GCS_STAGING_LOCATION</code>:存放人工制品的GCS桶。</li><li id="0faa" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kf kc kd ke bi translated"><code class="du lc ld le ks b">SUBNET</code> : VPC子网</li><li id="ca7b" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kf kc kd ke bi translated"><code class="du lc ld le ks b">INPUT_HIVE_DATABASE</code>:输入表的配置单元数据库</li><li id="f22b" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kf kc kd ke bi translated"><code class="du lc ld le ks b">INPUT_HIVE_TABLES</code>:逗号分隔的要移动的配置单元表名，要从数据库中移动所有的表放" * "</li><li id="20f2" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kf kc kd ke bi translated"><code class="du lc ld le ks b">OUTPUT_BIGQUERY_DATASET</code>:输出表的BigQuery数据集</li><li id="283a" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kf kc kd ke bi translated"><code class="du lc ld le ks b">TEMP_BUCKET</code>:存放中间文件的临时GCS桶</li><li id="3494" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kf kc kd ke bi translated"><code class="du lc ld le ks b">HIVE_METASTORE</code>:蜂房metastore URI</li><li id="e872" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kf kc kd ke bi translated"><code class="du lc ld le ks b">MAX_PARALLELISM</code>:要运行的并行Dataproc作业的数量(默认值=10)</li><li id="ab25" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kf kc kd ke bi translated"><code class="du lc ld le ks b">HIVE_OUTPUT_MODE</code>:可以是覆盖/追加/忽略/错误是否存在之一</li></ul><p id="a4ee" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">第三步:</strong>导入所需的库，如pandas和SparkSession。</p><p id="c152" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">步骤4: </strong>将当前工作目录设置为python文件夹，因为所有需要的工件都在那里。</p><p id="a2af" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">第五步:构建python egg包<br/></strong>hivetobiqquery笔记本在后端利用已有的<a class="ae jv" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/python/dataproc_templates/hive/hive_to_bigquery.py" rel="noopener ugc nofollow" target="_blank">Hive to biqquery模板</a>，需要egg文件形式的python包。</p><p id="b4ab" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">步骤6:将工件复制到GCS bucket <br/> </strong>将上面的包文件连同现有的main.py一起复制到用户提供的GCS bucket。</p><p id="056c" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">步骤7:获取配置单元表<br/> </strong>如果用户在步骤2中没有提供要迁移的配置单元表列表，该单元将连接到所提供的配置单元节约URI，并使用SparkSession获取数据库中所有表的列表。</p><p id="495d" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">步骤8:创建作业集<br/> </strong>基于步骤2中提供的MAX_PARALLELISM值，多个作业将并行运行，使用Dataproc无服务器批处理作业将配置单元表迁移到BigQuery。运行此单元格以将表格列表分组，然后这些表格将并行运行。</p><p id="6ae7" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">步骤9: </strong>设置要传递给VertexAI管道的Dataproc模板属性，如JAR路径和egg包路径。</p><p id="4a5a" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">步骤10:构建顶点AI管道<br/> </strong>在<a class="ae jv" href="https://cloud.google.com/vertex-ai/docs/pipelines/dataproc-component" rel="noopener ugc nofollow" target="_blank">用于顶点AI管道的Dataproc无服务器组件</a>的帮助下，可以触发一个Dataproc无服务器批处理作业，该作业在后端将运行HIVE到BQ模板，并将表集(参考步骤8)并行加载到BigQuery。</p><p id="accd" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">步骤11:触发作业并更新审计表<br/> </strong>最后一个单元负责触发上一步建立的顶点AI流水线，同时也更新审计表中设置的各个表的状态。</p><p id="5a20" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">6.使用菜单栏中给定的选项逐一或一次运行所有单元格。在第11步结束时，将为每个表集生成一个链接，该链接可用于监视作业。或者，也可以从<a class="ae jv" href="https://console.cloud.google.com/dataproc/batches" rel="noopener ugc nofollow" target="_blank"> Dataproc批处理UI </a>中查看详细日志。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/9c225946466e34357a8772c442837e7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vaGsQxS-u70XMQoe.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">VertexAI笔记本DAG可视化</figcaption></figure><h1 id="228b" class="lk kx hi bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">审计表</h1><p id="7aa7" class="pw-post-body-paragraph ix iy hi iz b ja mh jc jd je mi jg jh ji mj jk jl jm mk jo jp jq ml js jt ju hb bi translated">该模板以CSV格式将每次加载的审计数据存储在提供的GCS存储桶中。</p><p id="b20f" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">为了查看数据，创建一个指向GCS存储桶的外部表，如下所示。</p><pre class="km kn ko kp fd kr ks kt ku aw kv bi"><span id="8c07" class="kw kx hi ks b fi ky kz l la lb">CREATE EXTERNAL TABLE `&lt;project-id&gt;.&lt;dataset-name&gt;.hive_bq_audit`<br/>(<br/>  Source_DB_Name STRING,<br/>  Source_Table_Set STRING,<br/>  Target_DB_Name STRING,<br/>  Target_Table_Set STRING,<br/>  Job_Start_Time STRING,<br/>  Job_End_Time STRING,<br/>  Job_Status STRING<br/>)<br/>OPTIONS(<br/>  format="CSV",<br/>  uris=["gs://&lt;bucket-name&gt;/audit/*"]<br/>);</span></pre><p id="2015" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">限制:</strong></p><ol class=""><li id="0cc4" class="jw jx hi iz b ja jb je jf ji jy jm jz jq ka ju kb kc kd ke bi translated"><strong class="iz hj">分区表:<br/> </strong>当前版本不自动在BigQuery中创建分区。为了在BigQuery中复制配置单元分区，必须手动创建带有分区和集群列的DDL。一旦在BigQuery中创建了表结构，上述过程将负责将数据插入到分区中。<a class="ae jv" href="https://cloud.google.com/bigquery/docs/interactive-sql-translator" rel="noopener ugc nofollow" target="_blank"> BigQuery翻译API </a>可用于将Hive DDL转换为BigQuery格式。<br/>注意:在这里通读BigQuery分区限制<a class="ae jv" href="https://cloud.google.com/bigquery/docs/partitioned-tables#limitations" rel="noopener ugc nofollow" target="_blank">。</a></li><li id="26a4" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kb kc kd ke bi translated"><strong class="iz hj">增量加载:<br/> </strong>当前版本仅支持覆盖和追加模式。根据要求在上述步骤2中更改该值。</li><li id="3ea3" class="jw jx hi iz b ja kg je kh ji ki jm kj jq kk ju kb kc kd ke bi translated"><strong class="iz hj"> Kerberos认证:<br/> </strong>如果需要，用户必须自己实现Kerberos认证。</li></ol><p id="5f1a" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">如有任何疑问/建议，请联系:<br/><a class="ae jv" href="mailto:dataproc-templates-support-external@googlegroups.com" rel="noopener ugc nofollow" target="_blank">dataproc-templates-support-external@googlegroups.com</a></p></div></div>    
</body>
</html>