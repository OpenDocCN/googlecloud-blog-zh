<html>
<head>
<title>Processing large data tables from Hive to GCS using PySpark and Dataproc Serverless</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PySpark和Dataproc无服务器处理从Hive到GCS的大型数据表</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/processing-large-data-tables-from-hive-to-gcs-using-pyspark-and-dataproc-serverless-35d3d16daaf?source=collection_archive---------0-----------------------#2022-07-26">https://medium.com/google-cloud/processing-large-data-tables-from-hive-to-gcs-using-pyspark-and-dataproc-serverless-35d3d16daaf?source=collection_archive---------0-----------------------#2022-07-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/5513dfec72b499e08b9abe38c57f39ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/0*T1iiiR6Bug1F7AJ_"/></div></figure><p id="f04a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Dataproc模板允许我们使用Java和Python在Dataproc Serverless上运行常见用例，而不需要我们自己开发。这些模板实现了常见的Spark工作负载，让我们可以轻松地定制和运行它们。</p><p id="081b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果您不熟悉Dataproc Serverless，或者您正在寻找使用Dataproc Serverless将数据从GCS迁移到BigQuery的PySpark模板，那么可以使用这个<a class="ae jk" rel="noopener" href="/@ppaglilla/getting-started-with-dataproc-serverless-pyspark-templates-e32278a6a06e"> blogpost </a>。</p><p id="2c52" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">另外，你可以参考我的另一篇<a class="ae jk" rel="noopener" href="/google-cloud/moving-data-from-bigquery-to-gcs-using-gcp-dataproc-serverless-and-pyspark-f6481b86bcd1"> blogpost </a>来将数据从BigQuery移动到GCS。</p><p id="f77a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这篇博文中，我们将讨论如何使用Dataproc Serverless处理从Hive到GCS的大量数据。</p><h1 id="5ac6" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">先决条件</h1><p id="1eff" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">为了运行这些模板，我们需要:</p><ul class=""><li id="47b2" class="ko kp hi io b ip iq it iu ix kq jb kr jf ks jj kt ku kv kw bi translated">Google Cloud SDK已安装并通过验证</li><li id="7b1e" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated">Python 3.7以上版本已安装</li><li id="611b" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated">启用了专用Google访问的VPC子网。默认子网是合适的，只要启用了私有Google访问。您可以在此查看所有Dataproc无服务器网络需求<a class="ae jk" href="https://cloud.google.com/dataproc-serverless/docs/concepts/network" rel="noopener ugc nofollow" target="_blank">。</a></li></ul><h1 id="205c" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">主要优势</h1><ul class=""><li id="a13f" class="ko kp hi io b ip kj it kk ix lc jb ld jf le jj kt ku kv kw bi translated">使用Dataproc Serverless运行Spark batch工作负载，而无需提供和管理您自己的集群。</li><li id="c111" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated"><a class="ae jk" href="https://github.com/GoogleCloudPlatform/dataproc-templates/tree/main/python/dataproc_templates/hive" rel="noopener ugc nofollow" target="_blank"> HiveToGCS </a>模板是开源的，完全可定制，可用于简单的工作。</li><li id="0719" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated">您可以将数据以拼花、AVRO、CSV和JSON格式从Hive接收到GCS。</li></ul><h1 id="58a9" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">配置参数</h1><p id="8fdf" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">该模板包括以下参数来配置执行:</p><ul class=""><li id="dcfd" class="ko kp hi io b ip iq it iu ix kq jb kr jf ks jj kt ku kv kw bi translated"><code class="du lf lg lh li b">spark.hadoop.hive.metastore.uris</code>:URI Hive metastore的spark属性</li><li id="cf95" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated"><code class="du lf lg lh li b">hive.gcs.input.database</code>:输入表的配置单元数据库</li><li id="18bf" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated"><code class="du lf lg lh li b">hive.gcs.input.table</code> : Hive输入表名</li><li id="4f1b" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated"><code class="du lf lg lh li b">hive.gcs.output.location</code>:输出文件的GCS位置(格式:gs://BUCKET/…)</li><li id="976e" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated"><code class="du lf lg lh li b">hive.gcs.output.format</code>:输出文件格式。avro、parquet、csv、json之一。默认为拼花地板</li><li id="7e59" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated"><code class="du lf lg lh li b">hive.gcs.output.mode</code>:输出写入模式。append、overwrite、ignore、errorifexists之一。默认为追加。你可以在这里了解每种保存模式的行为<a class="ae jk" href="https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes" rel="noopener ugc nofollow" target="_blank">。</a></li></ul><h1 id="e7b9" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">使用</h1><ol class=""><li id="0d6e" class="ko kp hi io b ip kj it kk ix lc jb ld jf le jj lj ku kv kw bi translated">如果你要使用“默认的”由GCP生成的VPC网络，请确保你已经启用了私有谷歌访问子网。您仍然需要启用如下的私人访问。</li></ol><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/827b0ae76720352d70a481a1115a09c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/0*JLz5yIKPBO3rRbdu.png"/></div></figure><p id="9854" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.创建一个GCS存储桶，用作Dataproc的暂存位置。Dataproc将使用这个桶来存储运行我们的无服务器集群所需的依赖项。</p><pre class="ll lm ln lo fd lp li lq lr aw ls bi"><span id="85a5" class="lt jm hi li b fi lu lv l lw lx">export STAGING_BUCKET=”my-staging-bucket”<br/>gsutil mb gs://$STAGING_BUCKET</span></pre><p id="f6a2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">3.克隆Dataproc模板库并导航到Python模板的目录</p><pre class="ll lm ln lo fd lp li lq lr aw ls bi"><span id="cfef" class="lt jm hi li b fi lu lv l lw lx">git clone <a class="ae jk" href="https://github.com/GoogleCloudPlatform/dataproc-templates.git" rel="noopener ugc nofollow" target="_blank">https://github.com/GoogleCloudPlatform/dataproc-templates.git</a><br/>cd dataproc-templates/python</span></pre><p id="0cf2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">4.配置Dataproc无服务器作业</p><p id="de69" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了将作业提交给Dataproc Serverless，我们将使用提供的<code class="du lf lg lh li b">bin/start.sh</code>脚本。该脚本要求我们使用环境变量来配置Dataproc无服务器集群。</p><p id="1e34" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">强制性配置包括:</p><ul class=""><li id="854b" class="ko kp hi io b ip iq it iu ix kq jb kr jf ks jj kt ku kv kw bi translated"><code class="du lf lg lh li b">GCP_PROJECT</code>:无服务器运行Dataproc的GCP项目。</li><li id="96e4" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated"><code class="du lf lg lh li b">REGION</code>:运行Dataproc Serverless的区域。</li><li id="c9ca" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated"><code class="du lf lg lh li b">SUBNET</code>:Hive metastore所在的子网</li><li id="7c06" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated"><code class="du lf lg lh li b">GCS_STAGING_LOCATION</code>:一个GCS位置，Dataproc将在此存储登台资产。应该在我们之前创建的桶内。</li></ul><pre class="ll lm ln lo fd lp li lq lr aw ls bi"><span id="2058" class="lt jm hi li b fi lu lv l lw lx"># Project ID to run the Dataproc Serverless Job<br/>export GCP_PROJECT=&lt;project_id&gt;</span><span id="3440" class="lt jm hi li b fi ly lv l lw lx"># GCP region where the job should be submitted<br/>export REGION=&lt;region&gt;</span><span id="528d" class="lt jm hi li b fi ly lv l lw lx"># Subnet where Hive metastore exists so job can be launched in same subnet<br/>export SUBNET=&lt;region&gt;</span><span id="e71b" class="lt jm hi li b fi ly lv l lw lx"># The staging location for Dataproc<br/>export GCS_STAGING_LOCATION=gs://$STAGING_BUCKET/staging</span></pre><p id="735a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">5.执行配置单元到GCS Dataproc模板</p><p id="e28d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">配置作业后，我们就可以触发它了。我们将运行bin/start.sh脚本，指定要运行的模板和执行的参数值。</p><pre class="ll lm ln lo fd lp li lq lr aw ls bi"><span id="d22d" class="lt jm hi li b fi lu lv l lw lx">./bin/start.sh \<br/>— properties=spark.hadoop.hive.metastore.uris=thrift://&lt;hostname-or-ip&gt;:9083 \<br/>— — template=HIVETOGCS \<br/>— hive.gcs.input.database=”&lt;database&gt;” \<br/>— hive.gcs.input.table=”&lt;table&gt;” \<br/>— hive.gcs.output.location=”&lt;gs://bucket/path&gt;” \<br/>— hive.gcs.output.format=”&lt;csv|parquet|avro|json&gt;” \<br/>— hive.gcs.output.mode=”&lt;append|overwrite|ignore|errorifexists&gt;”</span></pre><p id="0528" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">注意:提交作业将要求您启用Dataproc API，如果还没有启用的话。</p><p id="1315" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">6.监控Spark批处理作业</p><p id="fd5b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">提交作业后，我们将能够在<a class="ae jk" href="https://console.cloud.google.com/dataproc/batches" rel="noopener ugc nofollow" target="_blank"> Dataproc批处理UI </a>中看到。从那里，我们可以查看作业的指标和日志。</p><h1 id="a303" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">预定处决</h1><p id="e77a" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">除了通过start.sh脚本提交作业，您还可以选择设置作业的计划执行。当您希望随着一天中新数据的到来而定期从Hive移动到GCS时，此设置非常有用。</p><p id="fb44" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">需要考虑的一件重要事情是修改HiveToGCS模板以允许指定分区，这样您的调度作业就可以以增量加载的方式从最近的分区中读取数据。目前，模板可以支持输出模式为覆盖的全表定时导出</p><p id="98f9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在下面的例子中，我们将使用Cloud Scheduler来调度dataproc模板的执行。云调度程序是一个GCP服务的cron作业调度程序。</p><ol class=""><li id="a851" class="ko kp hi io b ip iq it iu ix kq jb kr jf ks jj lj ku kv kw bi translated">克隆Dataproc模板库并导航到Python模板的目录</li></ol><pre class="ll lm ln lo fd lp li lq lr aw ls bi"><span id="0d00" class="lt jm hi li b fi lu lv l lw lx">git clone <a class="ae jk" href="https://github.com/GoogleCloudPlatform/dataproc-templates.git" rel="noopener ugc nofollow" target="_blank">https://github.com/GoogleCloudPlatform/dataproc-templates.git</a><br/>cd dataproc-templates/python</span></pre><p id="0200" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.运行下面的命令来创建egg文件，并使用gsutil命令将文件上传到GCS bucket，这将由云调度程序使用。还将main.py文件上传到同一个GCS存储桶</p><pre class="ll lm ln lo fd lp li lq lr aw ls bi"><span id="e9c0" class="lt jm hi li b fi lu lv l lw lx">python setup.py bdist_egg — output=dataproc_templates_distribution.egg</span><span id="cb35" class="lt jm hi li b fi ly lv l lw lx">gsutil mb gs://{DEPENDENCY_BUCKET}</span><span id="493c" class="lt jm hi li b fi ly lv l lw lx">gsutil cp dataproc_templates_distribution.egg gs://{DEPENDENCY_BUCKET}</span><span id="f6d6" class="lt jm hi li b fi ly lv l lw lx">gsutil cp main.py gs://{DEPENDENCY_BUCKET}</span></pre><p id="84d1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">3.让我们创建云调度程序作业:</p><ul class=""><li id="0ab8" class="ko kp hi io b ip iq it iu ix kq jb kr jf ks jj kt ku kv kw bi translated">打开<a class="ae jk" href="https://cloud.google.com/scheduler" rel="noopener ugc nofollow" target="_blank">云调度器</a>并点击创建作业</li></ul><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es lz"><img src="../Images/ad33283b8cd27838c8c08dac0f0bbefa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*E6hASsL6opmN-QaB"/></div></div></figure><ul class=""><li id="a7be" class="ko kp hi io b ip iq it iu ix kq jb kr jf ks jj kt ku kv kw bi translated">填写工作名称的详细信息。支持HTTPS克朗工作的地区。对于这个博客，我们将使用美国中心1。还要填写您希望该作业运行的<a class="ae jk" href="https://crontab.guru/#0_5_*_*" rel="noopener ugc nofollow" target="_blank"> Cron </a>频率和时区，然后单击继续</li></ul><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es me"><img src="../Images/4b920bb3544b1537c587ae964e9c2a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PP4X1jaLmyfbVwGT"/></div></div></figure><ul class=""><li id="4f9e" class="ko kp hi io b ip iq it iu ix kq jb kr jf ks jj kt ku kv kw bi translated">下面是提交dataproc无服务器作业的POST请求所需的URL</li></ul><pre class="ll lm ln lo fd lp li lq lr aw ls bi"><span id="63dc" class="lt jm hi li b fi lu lv l lw lx"><a class="ae jk" href="https://dataproc.googleapis.com/v1/projects/%7BPROJECT_ID%7D/locations/%7BREGION%7D/batches" rel="noopener ugc nofollow" target="_blank">https://dataproc.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/batches</a></span></pre><ul class=""><li id="b0d6" class="ko kp hi io b ip iq it iu ix kq jb kr jf ks jj kt ku kv kw bi translated">创建作业时需要提供的示例JSON主体。根据您的值编辑JSON主体</li></ul><pre class="ll lm ln lo fd lp li lq lr aw ls bi"><span id="ee4e" class="lt jm hi li b fi lu lv l lw lx">{</span><span id="db0e" class="lt jm hi li b fi ly lv l lw lx">“labels”: {</span><span id="cf61" class="lt jm hi li b fi ly lv l lw lx">“job_type”: “dataproc_template”</span><span id="469d" class="lt jm hi li b fi ly lv l lw lx">},</span><span id="71d4" class="lt jm hi li b fi ly lv l lw lx">“runtimeConfig”: {</span><span id="9e48" class="lt jm hi li b fi ly lv l lw lx">“properties”: {</span><span id="8941" class="lt jm hi li b fi ly lv l lw lx">“spark.hadoop.hive.metastore.uris”: “thrift://{hostname-or-ip}:9083”</span><span id="d271" class="lt jm hi li b fi ly lv l lw lx">}</span><span id="4453" class="lt jm hi li b fi ly lv l lw lx">},</span><span id="49d8" class="lt jm hi li b fi ly lv l lw lx">“environmentConfig”: {</span><span id="c547" class="lt jm hi li b fi ly lv l lw lx">“executionConfig”: {</span><span id="2d77" class="lt jm hi li b fi ly lv l lw lx">“subnetworkUri”: “projects/{PROJECT_ID}/regions/{REGION}/subnetworks/{SUBNET}”</span><span id="207a" class="lt jm hi li b fi ly lv l lw lx">}</span><span id="9fe9" class="lt jm hi li b fi ly lv l lw lx">},</span><span id="12fc" class="lt jm hi li b fi ly lv l lw lx">“pysparkBatch”: {</span><span id="1887" class="lt jm hi li b fi ly lv l lw lx">“mainPythonFileUri”: “gs://{DEPENDENCY_BUCKET}/main.py”,</span><span id="1c52" class="lt jm hi li b fi ly lv l lw lx">“args”: [</span><span id="eb78" class="lt jm hi li b fi ly lv l lw lx">“ — template=HIVETOGCS”,</span><span id="d83b" class="lt jm hi li b fi ly lv l lw lx">“ — hive.gcs.input.database={databaseName}”,</span><span id="f87c" class="lt jm hi li b fi ly lv l lw lx">“ — hive.gcs.input.table={tableName}”,</span><span id="6e0e" class="lt jm hi li b fi ly lv l lw lx">“ — hive.gcs.output.location={gs://bucket/path}”,</span><span id="d1a6" class="lt jm hi li b fi ly lv l lw lx">“ — hive.gcs.output.format={csv|parquet|avro|json}”,</span><span id="4182" class="lt jm hi li b fi ly lv l lw lx">“ — hive.gcs.output.mode={append|overwrite|ignore|errorifexists}”</span><span id="c75d" class="lt jm hi li b fi ly lv l lw lx">],</span><span id="b3a7" class="lt jm hi li b fi ly lv l lw lx">“pythonFileUris”: [</span><span id="9c4e" class="lt jm hi li b fi ly lv l lw lx">“gs://{DEPENDENCY_BUCKET}/dataproc_templates_distribution.egg”</span><span id="db28" class="lt jm hi li b fi ly lv l lw lx">],</span><span id="0bdc" class="lt jm hi li b fi ly lv l lw lx">“jarFileUris”: [</span><span id="4227" class="lt jm hi li b fi ly lv l lw lx">“file:///usr/lib/spark/external/spark-avro.jar”,</span><span id="0696" class="lt jm hi li b fi ly lv l lw lx">“gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar”</span><span id="b2f8" class="lt jm hi li b fi ly lv l lw lx">]</span><span id="6bd0" class="lt jm hi li b fi ly lv l lw lx">}</span><span id="fce3" class="lt jm hi li b fi ly lv l lw lx">}</span></pre><ul class=""><li id="4398" class="ko kp hi io b ip iq it iu ix kq jb kr jf ks jj kt ku kv kw bi translated">填写完详细信息后，页面将如下所示。最后，单击“创建”完成作业的创建。</li></ul><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es mf"><img src="../Images/37f2afaa23c5720809162ab60de3c83b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0UM7iLDQniNVf-Xh"/></div></div></figure><ul class=""><li id="0461" class="ko kp hi io b ip iq it iu ix kq jb kr jf ks jj kt ku kv kw bi translated">确保选择正确的认证头/服务帐户，并有权限提交Dataproc无服务器作业</li><li id="4ea8" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated">一旦创建了我们的作业，它将按照定义的频率运行。我们还可以手动触发云调度器作业(强制作业运行)进行测试，然后在<a class="ae jk" href="https://console.cloud.google.com/dataproc/batches" rel="noopener ugc nofollow" target="_blank"> Dataproc Batches UI </a>中监控执行的dataproc作业</li></ul><h1 id="f209" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">参考</h1><ul class=""><li id="9060" class="ko kp hi io b ip kj it kk ix lc jb ld jf le jj kt ku kv kw bi translated"><a class="ae jk" href="https://cloud.google.com/dataproc-serverless/docs/overview" rel="noopener ugc nofollow" target="_blank"> Dataproc无服务</a></li><li id="2888" class="ko kp hi io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated"><a class="ae jk" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> Dataproc模板库</a></li></ul></div></div>    
</body>
</html>