<html>
<head>
<title>Dataproc Serverless PySpark Template for Ingesting Compressed Text files To Bigquery</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Dataproc无服务器PySpark模板，用于将压缩文本文件接收到Bigquery</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/dataproc-serverless-pyspark-template-for-ingesting-compressed-text-files-to-bigquery-c6eab8fb6bc9?source=collection_archive---------1-----------------------#2022-07-07">https://medium.com/google-cloud/dataproc-serverless-pyspark-template-for-ingesting-compressed-text-files-to-bigquery-c6eab8fb6bc9?source=collection_archive---------1-----------------------#2022-07-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/074fe88022ecaa0711651676dd135e78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wJgOSunGGHKiH3wJocU66w.jpeg"/></div></div></figure><p id="0e4a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="https://cloud.google.com/dataproc-serverless/docs/overview" rel="noopener ugc nofollow" target="_blank"> Dataproc无服务器</a>允许用户运行Spark工作负载，而无需供应或管理集群。因此，数据工程师现在可以专注于构建他们的管道，而不是担心集群基础设施。</p><p id="7904" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> Dataproc模板</a>是进一步简化数据工程师在Dataproc无服务器上的工作的倡议。这些模板附带了一组现成的特性，这些特性实现了基本的用例，并且可以根据需要进一步定制。模板有Java和Python两种版本。在这篇文章中，我们将关注如何使用“文本到BigQuery PySpark模板”将GZIP格式的压缩数据接收到BigQuery中。</p><h1 id="692f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">Dataproc无服务器—“文本到大查询”模板</h1><p id="ad55" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">该模板用于从Google云存储中读取文本文件，并将它们写入BigQuery表。它还支持压缩文本文件在<strong class="is hj"> BZIP2，GZIP，LZ4，DEFLATE </strong>格式，作为输入。(如果文本文件未压缩，使用<strong class="is hj"> NONE </strong>作为压缩格式)。该模板使用<a class="ae jo" href="https://cloud.google.com/dataproc-serverless/docs/guides/bigquery-connector-spark-example" rel="noopener ugc nofollow" target="_blank"> Spark BigQuery连接器</a>写入BigQuery。</p><h2 id="9413" class="ks jq hi bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf bi translated">先决条件</h2><p id="2959" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">为了运行这些模板，我们需要:</p><ul class=""><li id="4421" class="lg lh hi is b it iu ix iy jb li jf lj jj lk jn ll lm ln lo bi translated">Google Cloud SDK已安装并通过验证。</li><li id="49d6" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">Python 3.7+已安装。</li><li id="9e56" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">启用了专用Google访问的VPC子网。默认子网是合适的，只要启用了私有Google访问。你可以在这里查看所有的Dataproc无服务器网络需求<a class="ae jo" href="https://cloud.google.com/dataproc-serverless/docs/concepts/network" rel="noopener ugc nofollow" target="_blank"/>。</li></ul><h2 id="f83e" class="ks jq hi bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf bi translated">配置参数</h2><p id="e222" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">该模板包括以下参数来配置执行:</p><ul class=""><li id="fe7e" class="lg lh hi is b it iu ix iy jb li jf lj jj lk jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">text.bigquery.input.location</code>:输入文件的GCS位置。支持通配符。示例:<code class="du lu lv lw lx b">gs://BUCKET/*.gz</code></li><li id="aa8a" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">text.bigquery.input.compression</code>:输入文件压缩格式。<code class="du lu lv lw lx b">bzip2</code>、<code class="du lu lv lw lx b">gzip</code>、<code class="du lu lv lw lx b">lz4</code>、<code class="du lu lv lw lx b">deflate or none</code>其中之一。</li><li id="0825" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">text.bigquery.input.delimiter</code>:输入文件文本分隔符。例如:<code class="du lu lv lw lx b">"/"</code>、<code class="du lu lv lw lx b">"|"</code>、<code class="du lu lv lw lx b">","</code>等。</li><li id="96d6" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">text.bigquery.output.dataset</code>:输出表的BigQuery数据集。</li><li id="fa4e" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">text.bigquery.output.table</code>:big query输出表。</li><li id="8a25" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">text.bigquery.output.mode</code>:火花输出保存模式。append、overwrite、ignore、errorifexists之一。默认为追加。你可以在这里了解每种保存模式的表现<a class="ae jo" href="https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes" rel="noopener ugc nofollow" target="_blank">。</a></li><li id="5602" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">text.bigquery.temp.bucket.name</code>:Spark big query连接器的临时桶。在将输出数据加载到BigQuery之前，连接器会将输出数据写入这个GCS bucket。这种方法叫做间接写，你可以在这里了解更多。</li></ul><h2 id="f6b7" class="ks jq hi bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf bi translated">运行模板</h2><ol class=""><li id="42e2" class="lg lh hi is b it kn ix ko jb ly jf lz jj ma jn mb lm ln lo bi translated"><strong class="is hj">创建一个GCS存储桶</strong></li></ol><p id="af8c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">需要一个GCS存储桶作为Dataproc的暂存位置。Dataproc将使用这个桶来存储运行我们的无服务器集群所需的依赖项。</p><pre class="mc md me mf fd mg lx mh mi aw mj bi"><span id="5933" class="ks jq hi lx b fi mk ml l mm mn">export STAGING_BUCKET=”my-staging-bucket”<br/>gsutil mb gs://$STAGING_BUCKET</span></pre><p id="dbea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.<strong class="is hj">克隆Dataproc模板库</strong></p><p id="c907" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">成功克隆后，导航到Python模板的目录</p><pre class="mc md me mf fd mg lx mh mi aw mj bi"><span id="9c94" class="ks jq hi lx b fi mk ml l mm mn">git clone<a class="ae jo" href="https://github.com/GoogleCloudPlatform/dataproc-templates.git" rel="noopener ugc nofollow" target="_blank"> https://github.com/GoogleCloudPlatform/dataproc-templates.git</a><br/>cd dataproc-templates/python</span></pre><p id="692b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.<strong class="is hj">配置Dataproc无服务器作业</strong></p><p id="9580" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了将作业提交给Dataproc Serverless，我们将使用提供的bin/start.sh脚本。该脚本要求我们使用环境变量来配置Dataproc无服务器集群。</p><p id="6845" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">强制配置包括:</p><ul class=""><li id="6a30" class="lg lh hi is b it iu ix iy jb li jf lj jj lk jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">GCP_PROJECT</code>:在其上运行Dataproc Serverless的GCP项目。</li><li id="1387" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">REGION</code>:运行Dataproc无服务器的区域。</li><li id="04cf" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">GCS_STAGING_LOCATION</code>:一个GCS位置，Dataproc将在此存储登台资产。应该在我们之前创建的桶内。</li></ul><pre class="mc md me mf fd mg lx mh mi aw mj bi"><span id="f93c" class="ks jq hi lx b fi mk ml l mm mn"><em class="mo"># Project ID to run the Dataproc Serverless Job</em><br/>export GCP_PROJECT=&lt;project_id&gt;</span><span id="26de" class="ks jq hi lx b fi mp ml l mm mn"><em class="mo"># GCP region where the job should be submitted</em><br/>export REGION=&lt;region&gt;</span><span id="11eb" class="ks jq hi lx b fi mp ml l mm mn"><em class="mo"># The staging location for Dataproc</em><br/>export GCS_STAGING_LOCATION=gs://$STAGING_BUCKET/staging</span></pre><p id="15d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们的例子中，Text To BigQuery模板需要Spark BigQuery连接器在类路径中可用。连接器是公开托管的，所以我们将使用JARS环境变量来添加它。您还可以选择将JAR文件存储在您自己的存储桶中。</p><pre class="mc md me mf fd mg lx mh mi aw mj bi"><span id="b861" class="ks jq hi lx b fi mk ml l mm mn"><em class="mo"># Path to the Spark BigQuery Connector JAR file</em><br/>export JARS=”gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar”</span></pre><p id="e82c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.<strong class="is hj">执行文本到BigQuery Dataproc模板</strong></p><p id="7889" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">配置作业后，我们就可以触发它了。我们将运行bin/start.sh脚本，指定要运行的模板和执行的参数值。</p><pre class="mc md me mf fd mg lx mh mi aw mj bi"><span id="e8ce" class="ks jq hi lx b fi mk ml l mm mn">./bin/start.sh \<br/>-- --template=TEXTTOBIGQUERY \<br/> --text.bigquery.input.compression=&lt;gzip|bzip4|lz4|deflate|none&gt; \<br/> —-text.bigquery.input.delimiter=&lt;delimiter&gt; \<br/> --text.bigquery.input.location=&lt;gs://bucket/path&gt; \<br/> --text.bigquery.output.dataset=&lt;dataset&gt; \<br/> --text.bigquery.output.table=&lt;table&gt; \<br/> --text.bigquery.output.mode=&lt;append|overwrite|ignore|errorifexists&gt; \<br/> --text.bigquery.temp.bucket.name=”&lt;temp-bq-bucket-name&gt;”</span></pre><p id="9fea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意</strong>:提交作业将要求您启用Dataproc API，如果尚未启用的话。</p><p id="9de1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5.<strong class="is hj">监控火花批量作业</strong></p><p id="8c15" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦提交了作业，我们就可以在<a class="ae jo" href="https://console.cloud.google.com/dataproc/batches" rel="noopener ugc nofollow" target="_blank"> Dataproc Batches UI </a>中监控我们的作业。搜索批处理id以获取详细的指标和日志。</p><h2 id="68d4" class="ks jq hi bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf bi translated">高级作业配置选项</h2><p id="08b3" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">在配置我们的Dataproc无服务器作业时，我们还可以指定以下高级配置选项:</p><ul class=""><li id="f246" class="lg lh hi is b it iu ix iy jb li jf lj jj lk jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">SUBNET</code>:运行Dataproc无服务器的VPC子网，如果不使用默认子网的话(格式:<code class="du lu lv lw lx b">projects/&lt;project_id&gt;/regions/&lt;region&gt;/subnetworks/&lt;subnetwork&gt;</code>)。</li><li id="d467" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">JARS</code>:应该添加到Spark类路径中的逗号分隔的JAR文件。我们在本指南中使用它来为我们的工作提供Spark BigQuery连接器。</li><li id="80b1" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">FILES</code>:放置在Spark工作目录中的逗号分隔文件。</li><li id="95ab" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">PY_FILES</code>:要添加到PySpark的逗号分隔的Python脚本或包。支持<code class="du lu lv lw lx b">.py</code>、<code class="du lu lv lw lx b">.zip</code>和<code class="du lu lv lw lx b">.egg</code>格式。</li><li id="aae7" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">HISTORY_SERVER_CLUSER</code>:作为Spark历史服务器的现有Dataproc集群(格式:<code class="du lu lv lw lx b">projects/&lt;project_id&gt;/regions/&lt;region&gt;/clusters/&lt;cluster_name&gt;</code>)。</li><li id="904a" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><code class="du lu lv lw lx b">METASTORE_SERVICE</code>:用作外部Metastore的Dataproc Metastore服务的名称(格式:<code class="du lu lv lw lx b">projects/&lt;project_id&gt;/locations/&lt;region&gt;/services/&lt;service_name&gt;</code>)。</li></ul><h2 id="d3a8" class="ks jq hi bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf bi translated">火花配置选项</h2><p id="81d4" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">当提交Spark批处理工作负载时，我们还可以设置<a class="ae jo" href="https://cloud.google.com/dataproc-serverless/docs/concepts/properties" rel="noopener ugc nofollow" target="_blank"> Spark属性</a>供Dataproc Serverless为特定的批处理使用。这允许您确定分配工作负载的计算、内存和磁盘资源。</p><p id="2167" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在下面的例子中，我们指定<strong class="is hj">最小</strong>和<strong class="is hj">最大执行者</strong>分别作为<strong class="is hj"> 100 </strong>和<strong class="is hj"> 400 </strong>。</p><pre class="mc md me mf fd mg lx mh mi aw mj bi"><span id="9ce9" class="ks jq hi lx b fi mk ml l mm mn">./bin/start.sh \<br/>--properties=spark.dynamicAllocation.minExecutors=100, \<br/>spark.dynamicAllocation.maxExecutors=400 \<br/>-- --template=TEXTTOBIGQUERY \<br/> --text.bigquery.input.compression=&lt;gzip|bzip4|lz4|deflate|none&gt; \<br/> —-text.bigquery.input.delimiter=&lt;delimiter&gt; \<br/> --text.bigquery.input.location=&lt;gs://bucket/path&gt; \<br/> --text.bigquery.output.dataset=&lt;dataset&gt; \<br/> --text.bigquery.output.table=&lt;table&gt; \<br/> --text.bigquery.output.mode=&lt;append|overwrite|ignore|errorifexists&gt; \<br/> --text.bigquery.temp.bucket.name=”&lt;temp-bq-bucket-name&gt;”</span></pre><p id="4920" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的资源分配下，BigQuery中的<strong class="is hj"> ~6TB </strong>负载用了<strong class="is hj"> ~43分钟</strong>。在<strong class="is hj">最少10个</strong>和<strong class="is hj">最多100个执行器</strong>的情况下，同样的负载用了<strong class="is hj"> 1小时20分钟</strong>。</p><h2 id="832f" class="ks jq hi bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf bi translated">摘要</h2><p id="f601" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">“<a class="ae jo" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/python/dataproc_templates/gcs/README.md#text-to-bigquery" rel="noopener ugc nofollow" target="_blank"> Text To Bigquery Template </a>”提供了一个从压缩/未压缩文本文件(带有任何分隔符)直接向Bigquery摄取数据的起点。此外，它还具有以下优点:</p><ul class=""><li id="e765" class="lg lh hi is b it iu ix iy jb li jf lj jj lk jn ll lm ln lo bi translated">可以直接将压缩文本文件从GCS接收到BigQuery。不需要先解压缩数据。</li><li id="68aa" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">可根据任何加工或转换要求进行定制。</li><li id="226f" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">允许在自定义分隔符上拆分数据。</li></ul><p id="95c8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>如果您正在从<strong class="is hj"> avro、parquet、csv </strong>或<strong class="is hj"> json </strong>文件中寻找数据摄取，请使用“<a class="ae jo" href="https://github.com/GoogleCloudPlatform/dataproc-templates/blob/main/python/dataproc_templates/gcs/README.md#gcs-to-bigquery" rel="noopener ugc nofollow" target="_blank"> GCS To Bigquery Template </a>”。</p><h2 id="2447" class="ks jq hi bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf bi translated">参考资料:</h2><ul class=""><li id="ec09" class="lg lh hi is b it kn ix ko jb ly jf lz jj ma jn ll lm ln lo bi translated"><a class="ae jo" href="https://cloud.google.com/dataproc-serverless/docs/overview" rel="noopener ugc nofollow" target="_blank"> Dataproc无服务器文档</a></li><li id="4abc" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><a class="ae jo" href="https://github.com/GoogleCloudPlatform/dataproc-templates" rel="noopener ugc nofollow" target="_blank"> Dataproc模板库</a></li><li id="c6ee" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><a class="ae jo" rel="noopener" href="/@ppaglilla/getting-started-with-dataproc-serverless-pyspark-templates-e32278a6a06e">data proc无服务器PySpark模板入门</a></li></ul></div></div>    
</body>
</html>