<html>
<head>
<title>Using Airflow Experimental Rest API on Google Cloud Platform: Cloud Composer and IAP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Google云平台上使用气流实验Rest API:Cloud Composer和IAP</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/using-airflow-experimental-rest-api-on-google-cloud-platform-cloud-composer-and-iap-9bd0260f095a?source=collection_archive---------1-----------------------#2018-08-09">https://medium.com/google-cloud/using-airflow-experimental-rest-api-on-google-cloud-platform-cloud-composer-and-iap-9bd0260f095a?source=collection_archive---------1-----------------------#2018-08-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="a35e" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">无论何时你这么说</h1><p id="7ed6" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，您已经开始使用Apache Airflow，并且您已经迷上了它，或者您正在研究Airflow可以如何帮助您，但是您的需求并不完全符合<a class="ae kb" rel="noopener" href="/videoamp/what-we-learned-migrating-off-cron-to-airflow-b391841a0da4">我们希望迁移我们的cron调度</a>或<a class="ae kb" href="https://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf" rel="noopener ugc nofollow" target="_blank">只要有文件进入桶中</a>就行。唉，你需要你的狗在你说的任何时候跑<em class="kc"/>。嗯，我有消息告诉你！Airflow有一个<a class="ae kb" href="https://airflow.apache.org/api.html" rel="noopener ugc nofollow" target="_blank">实验性的REST API </a>，您可以使用它来触发Dag。因此，当您的上游系统完成一些事情时，您可以调用一个简单的python脚本<a class="ae kb" href="https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/iap/make_iap_request.py" rel="noopener ugc nofollow" target="_blank">来处理与</a><a class="ae kb" href="https://cloud.google.com/iap/" rel="noopener ugc nofollow" target="_blank"> Google身份感知代理</a>的认证，并向您的Airflow端点发出一个HTTP请求。本文将带您建立一个<a class="ae kb" href="https://cloud.google.com/composer/" rel="noopener ugc nofollow" target="_blank"> Cloud Composer </a>环境来实现这一点！</p><h1 id="4d06" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">你将建造什么</h1><p id="dcfc" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，获取最新版本的<a class="ae kb" href="https://cloud.google.com/sdk/" rel="noopener ugc nofollow" target="_blank"> Google Cloud SDK </a>，让我们使用Google Cloud Composer来自动化ETL数据管道的转换和加载步骤！管道将创建一个Dataproc集群，对提取的数据执行转换(通过PySpark作业)，然后将结果上传到BigQuery。然后，您将通过使用Google Identity Aware Proxy (IAP)进行身份验证并发布到DAG的Airflow端点来触发这个管道。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/268c7d0688031297225d2400fb55260c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2JjHcrJZtqKXu41P"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">使用HTTP POST到端点来触发Airlfow DAG的架构图，该DAG自动启动/关闭Dataproc集群以运行Spark作业来增强数据并将增强的数据写入BigQuery。</figcaption></figure><h1 id="e2dd" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">正在设置</h1><h2 id="615c" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">项目设置</h2><p id="a189" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">从您的Google云平台控制台页面打开云外壳:</p><p id="b6ba" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">查看来自<a class="ae kb" href="https://github.com/GoogleCloudPlatform/professional-services/blob/master/examples/cloud-composer-examples/composer_http_post_example/README.md" rel="noopener ugc nofollow" target="_blank">谷歌云平台专业服务Github </a>的代码。</p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="a7ed" class="kt ig hi ln b fi lr ls l lt lu"># Contains source code for pyspark job and DAG.<br/>git clone https://github.com/GoogleCloudPlatform/professional-services.git ~/professional-services</span></pre><p id="dfff" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">为方便起见，设置一个与项目ID相等的变量:</p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="d6c8" class="kt ig hi ln b fi lr ls l lt lu">export PROJECT=<strong class="ln hj">&lt;REPLACE-THIS-WITH-YOUR-PROJECT-ID&gt;<br/></strong>gcloud config set project $PROJECT</span></pre><h2 id="bbf8" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">启用Google云平台API</h2><p id="57f1" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">使用此<a class="ae kb" href="https://console.cloud.google.com/flows/enableapi?apiid=dataflow,compute_component,logging,storage_component,storage_api,bigquery,pubsub,dataproc" rel="noopener ugc nofollow" target="_blank">助手链接</a>启用BigQuery、计算引擎、Dataproc、Composer和Google云存储API(如果尚未启用)。啜饮你的咖啡，这将需要几分钟。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lv"><img src="../Images/9980a40affc84fbe3bfadc6bc469079a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/0*wH8BpVNQkmz4JYwd"/></div></figure><h2 id="52d5" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">创建云存储桶</h2><p id="58b6" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">使用make bucket命令在项目中的us-central1中创建新的区域存储桶。</p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="5428" class="kt ig hi ln b fi lr ls l lt lu">gsutil mb -c regional -l us-central1 gs://$PROJECT<br/>gsutil mb -c regional -l us-central1 gs://$PROJECT-dataproc-staging</span></pre><h2 id="a19a" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">创建BigQuery数据集</h2><p id="5362" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在BigQuery中创建数据集。这是BigQuery中所有表的加载位置。</p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="ac63" class="kt ig hi ln b fi lr ls l lt lu">bq mk ComposerDemo</span></pre><h2 id="15ba" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">导出数据</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lw"><img src="../Images/bb073ede612926499b794335ebadf4ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/0*fRssWp-KZ_TS4E-u"/></div></figure><p id="8380" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">在本教程中，您将使用BigQuery公共表转储作为上游数据源。您将使用<a class="ae kb" href="https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips?pli=1" rel="noopener ugc nofollow" target="_blank">纽约市黄色出租车数据</a>。跟随<a class="ae kb" href="https://cloud.google.com/bigquery/public-data/nyc-tlc-trips" rel="noopener ugc nofollow" target="_blank">这个链接</a>了解更多关于数据的信息，然后使用下面的gcloud命令将表格导出为换行符分隔的JSON格式。您将为数据添加时间戳以避免冲突。注意，本实验后面的dag_trigger.py依赖于这个<strong class="jf hj"> $EXPORT_TIMESTAMP </strong> bash环境变量。</p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="0ced" class="kt ig hi ln b fi lr ls l lt lu">export EXPORT_TS=`date “+%Y-%m-%dT%H%M%S”`&amp;&amp; bq extract \<br/>--destination_format=NEWLINE_DELIMITED_JSON \<br/>nyc-tlc:yellow.trips \<br/>gs://$PROJECT/cloud-composer-lab/new-$EXPORT_TS/nyc-tlc-yellow-*.json</span></pre><p id="06a0" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">接下来，从这个公共云存储桶中复制为您的增强数据准备的模式文件:</p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="aaf3" class="kt ig hi ln b fi lr ls l lt lu">gsutil cp gs://python-dataflow-example/schemas/nyc-tlc-yellow.json gs://$PROJECT/schemas/nyc-tlc-yellow.json</span></pre><h1 id="89e7" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">创建一个编写器环境</h1><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es lx"><img src="../Images/76100a0bc22b041df6e95f4b14644e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Rvqq4oeFx6Z7JTIw"/></div></div></figure><p id="06d1" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated"><a class="ae kb" href="https://cloud.google.com/composer/docs/how-to/managing/creating" rel="noopener ugc nofollow" target="_blank">为此DAG创建一个Cloud Composer环境</a>。这将增加托管DAG和安装必要软件所需的计算资源。</p><p id="e30e" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">在云Shell中，使用gcloud命令创建环境。请注意，env-variables采用了一个对该环境中的所有Dag都可用的变量<a class="ae kb" href="https://airflow.apache.org/concepts.html#variables" rel="noopener ugc nofollow" target="_blank">列表。再次啜饮你的咖啡，这也需要几分钟。</a></p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="a6b7" class="kt ig hi ln b fi lr ls l lt lu">gcloud composer environments create demo-ephemeral-dataproc1 \<br/>--location us-central1 \<br/>--zone us-central1-b \<br/>--machine-type n1-standard-2 \<br/>--disk-size 20 </span><span id="3551" class="kt ig hi ln b fi ly ls l lt lu"># Set Airflow Variables in the Composer Environment we just created.<br/>gcloud composer environments run \<br/>demo-ephemeral-dataproc1 \<br/>--location=us-central1 variables -- \<br/>--set gcp_project $PROJECT</span><span id="bc9e" class="kt ig hi ln b fi ly ls l lt lu">gcloud composer environments run demo-ephemeral-dataproc1 \<br/>--location=us-central1 variables -- \<br/>--set gce_zone us-central1-b</span><span id="d197" class="kt ig hi ln b fi ly ls l lt lu">gcloud composer environments run demo-ephemeral-dataproc1 \<br/>--location=us-central1 variables -- \<br/>--set gcs_bucket $PROJECT</span><span id="9345" class="kt ig hi ln b fi ly ls l lt lu">gcloud composer environments run demo-ephemeral-dataproc1 \<br/>--location=us-central1 variables -- \<br/>--set bq_output_table $PROJECT:ComposerDemo.nyc_tlc_yellow_trips</span><span id="ba0f" class="kt ig hi ln b fi ly ls l lt lu">gcloud composer environments run demo-ephemeral-dataproc1 \<br/>--location=us-central1 variables -- \<br/>--set dataproc_bucket $PROJECT-dataproc-staging</span></pre><h2 id="4bc4" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">请注意作曲家DAG文件夹</h2><p id="30b2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">单击新composer环境的DAG文件夹图标。记下存储桶名称。(类似于:us-central 1-demo-短命—* * * * * * * * * *-bucket)</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lz"><img src="../Images/236a7bc2cb6e5348d819f413b815b875.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/0*PGFuPauwqVVS8V-r"/></div></figure><h1 id="6ba3" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">准备IAP认证</h1><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ma"><img src="../Images/b15445e3fef99db62ea3bd37e9157731.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/0*cCTdgzae5BTCfLVR"/></div></figure><p id="afb9" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">触发DAG的端点将位于身份识别代理之后，需要使用服务帐户进行调用。</p><h2 id="2550" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">获取用于发出IAP请求的最新Python代码</h2><p id="072e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">首先，您需要获得用于发出IAP请求的最新python脚本，并安装它的需求。</p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="7f80" class="kt ig hi ln b fi lr ls l lt lu"># Install necessary requirements for making iap requests with<br/># dag_trigger.py<br/>(curl https://raw.githubusercontent.com/GoogleCloudPlatform/python-docs-samples/master/iap/requirements.txt; echo 'tzlocal&gt;=1.5.1') &gt;&gt; ~/professional-services/examples/cloud-composer-examples/composer_http_post_example/iap_requirements.txt</span><span id="b45e" class="kt ig hi ln b fi ly ls l lt lu"># Get latest version of make_iap_request from python-docs-samples.<br/>curl <a class="ae kb" href="https://raw.githubusercontent.com/GoogleCloudPlatform/python-docs-samples/master/iap/make_iap_request.py" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/GoogleCloudPlatform/python-docs-samples/master/iap/make_iap_request.py</a> &gt;&gt; ~/professional-services/examples/cloud-composer-examples/composer_http_post_example/make_iap_request.py</span></pre><h2 id="4fc8" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">为POST触发器创建服务帐户</h2><p id="5e54" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">您需要创建一个服务帐户，以便通过发送到端点来触发DAG。</p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="718b" class="kt ig hi ln b fi lr ls l lt lu">gcloud iam service-accounts create dag-trigger</span><span id="4877" class="kt ig hi ln b fi ly ls l lt lu"># Give service account permissions to create tokens for <br/># iap requests.<br/>gcloud projects add-iam-policy-binding $PROJECT \<br/>--member \<br/>serviceAccount:dag-trigger@$PROJECT.iam.gserviceaccount.com \<br/>--role roles/iam.serviceAccountTokenCreator</span><span id="8401" class="kt ig hi ln b fi ly ls l lt lu">gcloud projects add-iam-policy-binding $PROJECT \<br/>--member \<br/>serviceAccount:dag-trigger@$PROJECT.iam.gserviceaccount.com \<br/>--role roles/iam.serviceAccountActor</span><span id="0c13" class="kt ig hi ln b fi ly ls l lt lu"># Service account also needs to be authorized to use Composer.<br/>gcloud projects add-iam-policy-binding $PROJECT \<br/>--member \<br/>serviceAccount:dag-trigger@$PROJECT.iam.gserviceaccount.com \<br/>--role roles/composer.user</span><span id="9b0e" class="kt ig hi ln b fi ly ls l lt lu"># We need a service account key to trigger the dag.<br/>gcloud iam service-accounts keys create ~/$PROJECT-dag-trigger-key.json \<br/>--iam-account=dag-trigger@$PROJECT.iam.gserviceaccount.com</span><span id="80ac" class="kt ig hi ln b fi ly ls l lt lu">export GOOGLE_APPLICATION_CREDENTIALS=~/$PROJECT-dag-trigger-key.json</span></pre><h1 id="0dc7" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">触发DAG</h1><h2 id="320f" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">Python设置</h2><p id="2170" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">首先，您将为所需的python库做一些设置。</p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="d3a2" class="kt ig hi ln b fi lr ls l lt lu">cd professional-services/examples/cloud-composer-example</span><span id="c057" class="kt ig hi ln b fi ly ls l lt lu"># Here you set up the python environment.<br/># Pip is a tool, similar to maven in the java world<br/>pip install — upgrade virtualenv<br/>pip install -U pip<br/>virtualenv composer-env<br/>source composer-env/bin/activate</span><span id="b56b" class="kt ig hi ln b fi ly ls l lt lu"># By default one of Airflow's dependencies installs a GPL dependency<br/>#(unidecode). To avoid this dependency set<br/># SLUGIFY_USES_TEXT_UNIDECODE=yes in your environment when you<br/># install or upgrade Airflow.<br/>export SLUGIFY_USES_TEXT_UNIDECODE=yes</span><span id="b451" class="kt ig hi ln b fi ly ls l lt lu"># Install requirements for this and other examples in <br/># cloud-composer-examples<br/>pip install -r requirements.txt</span><span id="fa1d" class="kt ig hi ln b fi ly ls l lt lu"># Required for dag_trigger.py<br/>pip install -r iap_requirements.txt</span><span id="a675" class="kt ig hi ln b fi ly ls l lt lu"># (Optional for testing spark code locally).<br/># pip install pyspark&gt;=2.3.1</span></pre><h2 id="0b2c" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">正在获取Airflow web服务器URL</h2><p id="1bf9" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">接下来，您需要找到这个DAG的Airflow API的url和Airflow的客户端id。</p><p id="a41d" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">在控制台中，使用hamburger stack导航到Cloud Composer。您应该看到这个:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/c61be4c0e25f869c7476d3b330317098.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GYsBURVvjy46wgba"/></div></div></figure><p id="cf96" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">单击您的环境的名称，并将web服务器的url复制到一个注释中，您稍后会用到它。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/607f8c20dac236ccf225cb99f2be13a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5PzQvBslJDotqS4V"/></div></div></figure><h1 id="130b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">获取气流客户端ID</h1><p id="8d90" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在匿名窗口访问气流网址<a class="ae kb" href="https://YOUR_UNIQUE_ID.appspot.com" rel="noopener ugc nofollow" target="_blank">https://<strong class="jf hj">YOUR _ UNIQUE _ ID</strong>. appspot . com</a>(你在上一步中注明的)，不要登录。在IAP Auth的第一个登录页面中，地址栏的url中有客户端id:</p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="fc94" class="kt ig hi ln b fi lr ls l lt lu">curl https://raw.githubusercontent.com/GoogleCloudPlatform/python-docs-samples/master/composer/rest/get_client_id.py &gt;&gt; ~/professional-services/examples/cloud-composer-examples/composer_http_post_example/get_client_id.py</span><span id="ad10" class="kt ig hi ln b fi ly ls l lt lu">CLIENT_ID=`python get_client_id.py $PROJECT us-central1 demo-ephemeral-dataproc`</span></pre><h2 id="7e19" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">构造端点URL并触发DAG</h2><p id="826d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">触发DAG的端点具有以下结构:</p><p id="b6c4" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">https:// <strong class="jf hj"> &lt; airflow web服务器URL&gt;</strong>/API/experimental/DAGs/<strong class="jf hj">&lt;Dag-id&gt;</strong>/Dag _ runs</p><h2 id="f1d7" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated"><strong class="ak">关于DAG代码的一些注释</strong></h2><p id="67c7" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这个例子中的spark作业实际上只是一个占位符。然而，DAG是在<code class="du mb mc md ln b">ephemeral_dataproc_spark_dag.py</code>中定义的，也是这篇文章的主要兴趣所在。关于此DAG，需要了解的两件重要事情是:</p><ol class=""><li id="1fa5" class="me mf hi jf b jg lh jk li jo mg js mh jw mi ka mj mk ml mm bi translated"><code class="du mb mc md ln b">schedule_interval</code>属性被设置为<code class="du mb mc md ln b">None</code>,这样这个DAG只在有到端点的post时运行。</li><li id="af05" class="me mf hi jf b jg mn jk mo jo mp js mq jw mr ka mj mk ml mm bi translated">我们的DAG通过DAG每次运行时创建的<code class="du mb mc md ln b">dag_run</code>对象的<code class="du mb mc md ln b">conf</code>属性读取HTTP POST有效负载的内容。注意，帖子里还应该有唯一的<code class="du mb mc md ln b">run_id</code>。</li></ol><h2 id="8034" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">在GCS中暂存代码</h2><p id="c837" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">审查完代码后，您需要将我们的DAG上传到Cloud Composer为我们创建的DAG文件夹中，并将我们的spark作业源代码上传到Google云存储中。再次啜饮您的咖啡，Cloud Composer需要5分钟来处理您的DAG并将更改级联到气流环境中。</p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="58ec" class="kt ig hi ln b fi lr ls l lt lu">gsutil cp ~/professional-services/examples/cloud-composer-examples/composer_http_post_example/ephemeral_dataproc_spark_dag.py gs://<strong class="ln hj">&lt;dag-folder&gt;</strong>/dags</span><span id="5cb6" class="kt ig hi ln b fi ly ls l lt lu">gsutil cp ~/professional-services/examples/cloud-composer-examples/composer_http_post_example/spark_avg_speed.py gs://$PROJECT/spark-jobs/</span></pre><h2 id="3c1c" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">使用方便的Python脚本触发端点</h2><p id="396d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><code class="du mb mc md ln b">dag_trigger.py</code>中的代码将构造一个HTTP POST请求，其中包含您希望增强的新文件的Google云存储路径。我们的DAG可以使用<code class="du mb mc md ln b">dag_run</code>对象的<code class="du mb mc md ln b">conf</code>属性读取post消息的内容。</p><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="04e7" class="kt ig hi ln b fi lr ls l lt lu">AIRFLOW_URI=`gcloud composer environments describe demo-ephemeral-dataproc --location us-central1 --format="get(config.airflowUri)"`</span><span id="c5a9" class="kt ig hi ln b fi ly ls l lt lu">python dag_trigger.py \<br/>--url=https://${AIRFLOW_URI}/api/experimental/dags/average-speed/dag_runs \<br/>--iapClientId=${CLIENT_ID} \<br/>--raw_path=gs://$PROJECT/cloud-composer-lab/new-$EXPORT_TS</span></pre><h1 id="4063" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">观察气流做它的事</h1><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="ms mt l"/></div></figure><h2 id="b293" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">导航到气流用户界面</h2><p id="345f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">你将登陆DAGs页面。在平均速度行中，单击“图表视图”图标。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mu"><img src="../Images/1b9675f4bddea46a2a5eb9abc81e0831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/0*odH9KtM85FGqfC37"/></div></figure><p id="d4eb" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">在这里，您可以查看DAG的结构以及工作流程中每个操作员的状态。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mv"><img src="../Images/4ffb9863792532f4e4f1d8b5c09a22cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*i91PL5jVhZWI9Wna"/></div></div></figure><h2 id="9035" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">导航至浏览&gt;任务实例</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mw"><img src="../Images/fa3520b777dd6aaa6c7f81105ab76ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WmaMt7PJY4MxFrFE"/></div></div></figure><p id="debd" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">在这里，您可以看到在此环境中运行的任务实例的历史记录。通过单击一个任务实例，您可以深入研究相关的元数据、日志、呈现的模板和在执行该任务实例期间记录的XComs。这在调试DAG时非常有用。</p><h1 id="681f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">从控制台监控您的GCP资源</h1><p id="0bfc" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当DAG执行时，坐下来看着这些资源被创建和销毁是一件非常有趣的事情。</p><p id="93e4" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">首先，导航到<a class="ae kb" href="https://console.cloud.google.com/dataproc/clusters" rel="noopener ugc nofollow" target="_blank"> Dataproc集群</a>并观察集群的创建。(90秒)</p><p id="c56b" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">接下来，跳转到<a class="ae kb" href="https://console.cloud.google.com/dataproc/jobs" rel="noopener ugc nofollow" target="_blank"> Dataproc作业</a>，查看DAG提交PySpark作业以向数据添加average_speed字段并转换为CSV。通过点击您的工作，您可以监控日志。(40分钟)</p><p id="7565" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">导航到<a class="ae kb" href="https://console.cloud.google.com/storage/browser" rel="noopener ugc nofollow" target="_blank"> Google云存储</a>选择您的项目存储桶和，查看累积在带有时间戳的GCS文件夹中的增强数据。</p><p id="3304" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">接下来，您可以看到DAG清理了GCS中的增强文件(10秒钟)</p><p id="08f2" class="pw-post-body-paragraph jd je hi jf b jg lh ji jj jk li jm jn jo lj jq jr js lk ju jv jw ll jy jz ka hb bi translated">然后导航到BigQuery控制台，看到增强的数据被写入BigQuery表，并导航回<a class="ae kb" href="https://console.cloud.google.com/dataproc/clusters" rel="noopener ugc nofollow" target="_blank"> Dataproc Clusters </a>，看到集群被拆除。(2分钟)</p><h1 id="76dd" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">打扫</h1><p id="6341" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当DAG为您拆除Dataproc集群时，其他资源将消耗您的免费信用或更糟地从您的信用卡中收费。为了避免这种情况，请确保删除您在本教程中创建的云存储桶、Composer环境和BigQuery数据集。</p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="mx mt l"/></div></figure><pre class="ke kf kg kh fd lm ln lo lp aw lq bi"><span id="7318" class="kt ig hi ln b fi lr ls l lt lu">yes -Y | gcloud composer environments delete <strong class="ln hj">&lt;your-composer-environment-name&gt;</strong></span><span id="d269" class="kt ig hi ln b fi ly ls l lt lu">gsutil rm -r gs://$PROJECT</span><span id="5626" class="kt ig hi ln b fi ly ls l lt lu">bq rm -r -f -d $<!-- -->PROJECT<!-- -->:ComposerDemo</span></pre><h1 id="f143" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结束语</h1><p id="2c16" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当您开始使用气流时，需要考虑很多问题，Composer为您简化了很多。只需对这个非常简单的DAG进行一些修改，您就可以将现有的Spark作业迁移到Dataproc，并且只在作业运行时支付计算费用，并且只在向airflow端点提交HTTP POST请求时运行它。</p><h2 id="e26d" class="kt ig hi bd ih ku kv kw il kx ky kz ip jo la lb it js lc ld ix jw le lf jb lg bi translated">安全考虑</h2><p id="dcc4" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果不提到Composer在一个公共URL上设置了Airflow UI和REST API端点，那将是我的疏忽。虽然Google身份识别代理是一种强大的身份验证方法，但这可能不符合您公司的安全协议。</p></div></div>    
</body>
</html>