<html>
<head>
<title>Running visual quality inspection at the edge with Google Cloud and Apache NiFi &amp; MiNiFi</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Google Cloud和Apache NiFi &amp; MiNiFi在边缘运行视觉质量检查</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/running-visual-quality-inspection-at-the-edge-with-google-cloud-and-apache-nifi-minifi-45282ce7af2d?source=collection_archive---------0-----------------------#2019-10-29">https://medium.com/google-cloud/running-visual-quality-inspection-at-the-edge-with-google-cloud-and-apache-nifi-minifi-45282ce7af2d?source=collection_archive---------0-----------------------#2019-10-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b4e56acdfed4306038418e944deacb14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lgwn17WxAt84vQ3ggA7pgA.png"/></div></div></figure><p id="c481" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi jo translated">【2019年10月23日，我在柏林Apache Con上做了一个关于在边缘运行视觉质量检查的演讲。这个故事描述了这次谈话，并帮助您使用谷歌云和Apache NiFi &amp; MiNiFi在边缘连续运行更新的TensorFlow模型。</p><h1 id="ec48" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">语境</h1><p id="934b" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">想象一下:你在一家生产饼干的工厂里…每天有成千上万的饼干通过你的生产线，你想让你的顾客开心。问题是…一块碎饼干会让顾客不高兴！因此，您需要一种方法，在饼干进入最终包装之前检测出破损的饼干。</p><p id="c82c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是我选择来支持这个故事的例子，但它也可以是与视觉质量检测相关的任何东西，跨越多个行业，如零售、制造、能源等。你能想到的任何东西都需要一个经过图像数据集训练的ML模型。</p><h1 id="3b4e" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">什么是Apache NiFi &amp; MiNiFi？</h1><p id="1865" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated"><a class="ae la" href="https://nifi.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache NiFi </a>是一个易于使用、功能强大且可靠的数据处理和分发系统。Apache NiFi支持强大且可伸缩的数据路由、转换和系统中介逻辑的有向图。<a class="ae la" href="https://nifi.apache.org/minifi/index.html" rel="noopener ugc nofollow" target="_blank">MiNiFi</a>——Apache NiFi的子项目——是一种补充性的数据收集方法，补充了NiFi在数据流管理方面的核心原则，侧重于在数据创建的源头收集数据。</p><p id="3faa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">简而言之，Apache NiFi和MiNiFi提供了工具和功能的强大组合，用于收集和移动数据、处理数据、清理数据并将其与其他系统集成。一旦需要引入数据，您就会希望使用Apache NiFi。</p><h1 id="9a93" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">什么是谷歌云愿景？</h1><p id="5486" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated"><a class="ae la" href="https://cloud.google.com/vision/" rel="noopener ugc nofollow" target="_blank"> Google Cloud Vision </a>允许您使用AutoML Vision从云中或边缘的图像中获得洞察力，或者使用预先训练的Vision API模型来检测情绪、理解文本等。</p><p id="0e15" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有了Google Cloud Vision，您有两个选择:</p><ul class=""><li id="62b9" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated"><strong class="is hj"> AutoML Vision </strong>:自动训练你自己定制的机器学习模型。使用<a class="ae la" href="https://cloud.google.com/automl/" rel="noopener ugc nofollow" target="_blank"> AutoML Vision </a>易于使用的图形界面，只需上传图像并训练自定义图像模型；针对准确性、延迟和大小优化您的模型；并将它们导出到云中的应用程序或边缘的一系列设备中。</li><li id="b369" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated"><strong class="is hj"> Vision API </strong>:谷歌云的Vision API通过REST和RPC APIs提供了强大的预训练机器学习模型。这些API包括:图像分类和标记成数百万个预定义的类别，检测物体和人脸，阅读印刷和手写文本，将有价值的元数据构建到图像目录中，调节内容等。</li></ul><p id="03a2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个故事中，我们将使用<strong class="is hj"> AutoML Vision </strong>使用我们自己的标签和图像来构建我们自己的定制机器学习模型。</p><h1 id="8cc6" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">目的:连续模型再训练</h1><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/773c7a7ddddfd2e3fa4612b0c22a0ef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hz5TqHzqjTt-Dg6Rj1YWUA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">自定义模型的自动重新训练</figcaption></figure><p id="f43f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">目标是拥有一个端到端的系统来检测缺陷，同时用新收集的图像进行连续的模型再训练。主要优势包括:</p><ul class=""><li id="9e9a" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated">在云中自动训练定制的ML模型</li><li id="6895" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">高效获取图像、标记图像、部署模型和运行推理</li><li id="eccb" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">使用来自生产线的新数据不断刷新模型</li><li id="a57d" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">最少的人类互动</li><li id="57a6" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">ML模型的准确性随着时间的推移而提高</li></ul><h1 id="47a2" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">在边缘使用树莓皮</h1><p id="3786" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">为了从生产线上收集数据，我将使用一个Raspberry Pi来运行Apache MiNiFi。我添加了一个摄像头来拍照，还添加了led来显示当前正在处理的内容。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/adce078d43c765046e5ae739a18df52c.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*ckf6mePeTyMmyaS60zz4VA.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">Raspberry Pi 3用于收集数据</figcaption></figure><p id="1762" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了使用Raspberry Pi从边缘收集数据，我将使用<a class="ae la" href="https://cloud.google.com/iot-core/" rel="noopener ugc nofollow" target="_blank">谷歌物联网核心</a>，这是一种完全托管的服务，可以轻松安全地连接、管理和接收来自全球分散设备的数据。</p><p id="f7f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一步是创建一个设备注册表，它是具有共享属性的设备的容器。创建注册表后，您可以使用设备标识注册您的设备。为此，您需要生成一个公钥/私钥对，设备将使用它来验证Google IoT核心(您也可以使用证书颁发机构)。下面是我用来为Raspberry Pi 生成密钥对的命令。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/06167ce3a91c6bd150e149f09d83aa4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ue5wsQYmVib5NKCa"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">我的设备已在我的设备注册表中注册</figcaption></figure><p id="98f1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，我将使用MQTT协议在我的设备和Google Cloud之间发送数据，它会自动将数据发布到发布/订阅主题中:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/812e83c47ea5a885e1ca45efa5ac65b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E-sdKIaH-GdDmjFWunVabQ.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">设备和谷歌云物联网核心之间的接口</figcaption></figure><p id="67f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了通过MQTT与Google Cloud IoT Core通信，我将使用Github 上的Pull请求中正在进行代码审查的处理器<a class="ae la" href="https://github.com/apache/nifi/pull/3392" rel="noopener ugc nofollow" target="_blank">。我还将在一个repo中使用这个处理器</a><a class="ae la" href="https://github.com/pvillard31/aceu19" rel="noopener ugc nofollow" target="_blank">，我创建这个repo是为了分享我演讲的代码片段</a>。</p><h1 id="7802" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">一个问题，两种架构</h1><p id="5931" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">当我们从设备中收集到图片后，我们就可以训练我们的定制模型了。那么我们必须在两个选项中做出选择:</p><ul class=""><li id="bdd2" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated">在Google Cloud中部署模型，在云中进行推理。这意味着对于每张图片，带有Apache MiNiFi的Raspberry Pi将对被服务的模型进行HTTPS调用，以获取该图片的标签。虽然这是有效的，并且易于实现和扩展，但这意味着通过互联网进行调用和更长的推断时间。在这个故事中我称之为模式"<strong class="is hj"><em class="mb"/></strong>"时配置设备。在<strong class="is hj"> <em class="mb">云</em> </strong>模式下，这里是架构的样子:</li></ul><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/02a3382a787f63a2765fa2e0b2137f44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WW5LoPG8FxhziCn0CHdl9A.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">“云”模式下的架构</figcaption></figure><ul class=""><li id="c221" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated">在Raspberry Pi上从Google Cloud下载模型，并使用Apache MiNiFi在边缘进行推理。Google Cloud提供了多种选项来下载您的定制模型，这些模型具有不同的准确性和延迟参数。在边缘上运行模型将导致更快的推断时间。在这个故事中，当配置设备时，我将这种模式称为“<strong class="is hj"><em class="mb"/></strong>”。在<strong class="is hj"> <em class="mb">边缘</em> </strong>模式下，架构看起来是这样的:</li></ul><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/1c5f98c2e7f8906ff5f95953b68c8985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YzCfApAvEMNDd4sv9a9TTA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">“边缘”模式下的建筑</figcaption></figure><p id="04f8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们还将看到，在<strong class="is hj"> <em class="mb"> edge </em> </strong>模式下，使用谷歌的硬件和Coral Edge TPU，有可能获得更好的推断时间。</p><h1 id="9b64" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">收集图片</h1><p id="44e7" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">以下是MiNiFi(在Raspberry Pi上)中运行的工作流的外观:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/3a8499ca138507f4b5f25e8237b8765c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fuOvDAySYy0KgyjwziUJYA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">在Raspberry Pi上的MiNiFi中运行工作流以收集图像</figcaption></figure><p id="f8db" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">工作流程非常简单:“独立”处理器执行Python脚本，使用Pi相机以给定的频率拍摄照片。然后图片被取到MiNiFi中，通过MQTT处理器发送到Google Cloud物联网核心。当这个过程发生时，我还会打开/关闭琥珀色LED，以便在Pi上直观地显示正在发生的事情。</p><p id="3570" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们所说的，数据将被自动发送到一个发布/订阅主题，并可供消费。在Google云平台中，<a class="ae la" rel="noopener" href="/@pierre.villard/nifi-with-oidc-using-terraform-on-the-google-cloud-platform-8686ac247ee9">我运行一个独立的安全Apache NiFi实例</a>(参见我之前的帖子，使用Terraform进行快速部署),下面的工作流正在其上运行:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/716cc5f6c3b4916b6dd819640e2c4a45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RhVXdHbjibeRZNMpaDaroA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">在GCP NiFi中运行的摄取图像的工作流</figcaption></figure><p id="7826" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些图片来自我的主题订阅，然后存储在Google云存储中，我使用Google Cloud Vision API将这些图片添加到我的数据集中(这是我用来训练我的定制模型的图像集合)。</p><p id="57a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我不会讲太多细节，因为<a class="ae la" href="https://cloud.google.com/vision/automl/docs/create-datasets" rel="noopener ugc nofollow" target="_blank">文档是不言自明的</a>但是我要做的调用是这样的——它给出了我创建的CSV文件的GCS路径，列出了我在过去X分钟摄取的所有图像的GCS路径:</p><pre class="lq lr ls lt fd mg mh mi mj aw mk bi"><span id="fce2" class="ml jy hi mh b fi mm mn l mo mp">curl \<br/>  -X POST \<br/>  -H "Authorization: Bearer $(gcloud auth application-default print-access-token)" \<br/>  -H "Content-Type: application/json" \<br/> <a class="ae la" href="https://automl.googleapis.com/v1beta1/projects/${projectID}/locations/us-central1/datasets/${datasetID}:importData" rel="noopener ugc nofollow" target="_blank">https://automl.googleapis.com/v1beta1/projects/${projectID}/locations/us-central1/datasets/${datasetID}:importData</a> \<br/>  -d '{<br/>        "inputConfig": {<br/>          "gcsSource": {<br/>            "inputUris": "gs://${gcs.bucket}/dataset.csv"<br/>          }<br/>        }<br/>      }'</span></pre><p id="8da4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注1 </strong>:数据集ID类似于ICN4695798657952251904。<br/> <strong class="is hj">注2 </strong>:在NiFi中，我将许多变量(项目ID、GCS bucket、数据集ID等)外部化，我在工作流中重用这些变量，使其更易于使用和配置。</p><h1 id="312c" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">管理您的设备配置</h1><p id="9dc3" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">一旦您的设备在Google Cloud IoT Core的设备注册表中注册，您就可以使用UI向您的设备发送配置更新或命令。它看起来是这样的:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/b128ed90f0e89df55fbbf96dfc996e4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w2c98GNTjB9soGaS"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">通过MQTT更新我的设备配置</figcaption></figure><p id="0a94" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在Raspberry Pi上，在MiNiFi中，MQTT处理器接收配置更新和命令，我处理接收到的数据以更新我的设备的配置:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/f7d5ecf71ee4421200f0d4e953a3c7f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Qa5wvn1bZZ45YAAx"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">接收配置更新和命令</figcaption></figure><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/c71ca0d238be0734e04daadb3259ef45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RhqNXjl_l2V02phRFEvMrA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">正在处理配置更新</figcaption></figure><p id="d08a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我收到配置更新时，我会处理JSON有效负载以提取模式，如果我应该使用或不使用边缘TPU，我会将此信息存储到本地缓存中，该缓存也会在设备重启时保存在磁盘上。</p><h1 id="ddb3" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">培训和服务您的模型</h1><p id="9392" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">我们现在有一个设备，我们能够从任何地方进行配置，它会自动从我们的生产线发送图片，这些图片会自动存储并添加到我们的谷歌云视觉数据集，以训练我们的定制模型。</p><p id="5daa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">谷歌云视觉的下一步是创建标签，用于对我们的图像进行分类。在本例中，我只创建了两个标签:</p><ul class=""><li id="8ff6" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated"><strong class="is hj"> OK </strong>:饼干看起来不错，可以进包装了</li><li id="56b5" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">NOK :饼干碎了，应该拿掉</li></ul><p id="1d48" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在UI中，我可以手动标记图像，每个标签至少需要10个图像来开始训练我的定制模型(图像越多，结果越好，这就是为什么我们希望不断接收新图像并随着时间的推移重新训练我们的模型):</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/b721d5109e394a610453e8defb613307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IzPnIwbrbjGT_NoA"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">我的数据集，包含我的cookies和标签的图片</figcaption></figure><p id="9089" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在GCP运行的NiFi实例中，我创建了一个工作流，用于每天触发模型训练，以便考虑新捕获的图像，并随着时间的推移提高模型的准确性:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/d11097e33deb4a82863f44b687fa808b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hGN9HgjC4dWLegJYXsSymw.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">NiFi每天都会触发模型训练</figcaption></figure><p id="b426" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了开始定制模型训练并等到训练完成，我使用了文档中描述的REST APIs。但是，在这个阶段，您必须指定您是否想要训练一个将在云中(<strong class="is hj"> <em class="mb">云模式</em> </strong>)或在边缘(<strong class="is hj"> <em class="mb">边缘模式</em> </strong>)运行的模型:</p><ul class=""><li id="4d0b" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated"><a class="ae la" href="https://cloud.google.com/vision/automl/docs/train" rel="noopener ugc nofollow" target="_blank">培训云托管模式</a></li><li id="b100" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated"><a class="ae la" href="https://cloud.google.com/vision/automl/docs/train-edge" rel="noopener ugc nofollow" target="_blank">训练边缘可导出模型</a></li></ul><p id="fe17" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当训练要在边缘导出的模型时，您可以指定一个参数，允许您选择是否更喜欢延迟而不是准确性:</p><ul class=""><li id="fed5" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated"><code class="du ms mt mu mh b">mobile-low-latency-1</code>对于低延迟，</li><li id="9807" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated"><code class="du ms mt mu mh b">mobile-versatile-1</code>用于一般用途，或</li><li id="4af6" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated"><code class="du ms mt mu mh b">mobile-high-accuracy-1</code>为了更高的预测质量。</li></ul><p id="895d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当云托管模型被训练时，Google Cloud Vision用户界面将如下所示:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/0567a57330d6b5ec83cc522cf05c6a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tZzpnmyAGrB-UxJQ"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">目前正在培训云托管模型</figcaption></figure><p id="eb5d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦模型训练完成，NiFi将收到一个JSON有效载荷，例如:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/76e96fef322c86cd1d22d5ef16c66929.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jgBpjdmBPi17I_t0"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">模型训练完成时的JSON有效负载</figcaption></figure><p id="bfba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">模型训练完成后，我们可以访问大量关于模型训练及其与提供的数据集相比的准确性的信息:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/438c51b193a9c50ee388e49797482814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Xg6-TkAvG4Bvbnn4"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">关于精度、召回等的定制模型信息</figcaption></figure><h1 id="4add" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">云托管模型</h1><p id="c7c9" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">一旦模型被训练，我们就可以调用REST APIs来部署模型。以下是NiFi工作流中负责此工作的部分:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/cef13a71bd5cacb8b2f05ac0c7ac2f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6HogaXR4mI0VUWQhGEd7pw.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">NiFi部署云旅馆定制模型</figcaption></figure><p id="1d35" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦部署完成，模型将通过Google Cloud中的REST API自动公开(您不必担心如何和在哪里，Google会处理好)。您需要的唯一信息是型号ID，这是我们通过ExecuteStreamCommand处理器使用Google IoT核心命令发送给Raspberry Pi的信息:</p><pre class="lq lr ls lt fd mg mh mi mj aw mk bi"><span id="d87f" class="ml jy hi mh b fi mm mn l mo mp">gcloud iot devices commands send \<br/>    --command-data=ICN147321363982450688<br/>    --region=REGION  \<br/>    --registry=REGISTRY_ID \<br/>    --device=DEVICE_ID</span></pre><p id="ff87" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在MiNiFi中，我们接收命令并将型号ID存储在缓存中:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/d710bcefbb46cf6f99ede5ae3e812ca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D6ricPiU-8-V3JG1pnOcKg.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">通过Google IoT核心MQTT命令接收型号ID</figcaption></figure><p id="fe5d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们可以通过对公开的API进行HTTPS调用来对捕获的每张图片进行推断:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/ae2c17c00e9d92e5ed49e316c8e8ad08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Y0uDQBaHwSw4wMXaEvOyg.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">使用公开的API在云中进行推理</figcaption></figure><p id="4832" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请注意RouteOnAttribute处理器，它将检查设备配置为哪种模式。在这种情况下，设备配置在<strong class="is hj"> <em class="mb">云</em> </strong>模式。</p><p id="2cdb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">图片有效载荷需要进行base64编码，并且<a class="ae la" href="https://cloud.google.com/vision/automl/docs/predict#automl_vision_predict-cli" rel="noopener ugc nofollow" target="_blank">要使用的API</a>是:</p><pre class="lq lr ls lt fd mg mh mi mj aw mk bi"><span id="a31e" class="ml jy hi mh b fi mm mn l mo mp">curl -X POST \<br/>  -H "Authorization: Bearer $(gcloud auth application-default print-access-token)" \<br/>  -H "Content-Type: application/json" \<br/>https://automl.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/us-central1/models/${MODEL_ID}:predict \<br/>  -d '{<br/>        "payload" : {<br/>          "image": {<br/>            "imageBytes" : "/9j/4AAQSkZJRgABAQAAAQ … "<br/>          }<br/>        }<br/>      }'</span></pre><h1 id="8230" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">Edge可导出模型</h1><p id="0809" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">在这种模式下，一旦TensorFlow模型训练完毕，我们只需要进行一次API调用，将训练好的模型导出到Google云存储中:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/fbd888d2b17215035fbe2001118c2f3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0hY2aXIG5BI9Rj0TwdpEww.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">API调用将自定义edge模型导出到Google云存储中</figcaption></figure><p id="863e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要使用的<a class="ae la" href="https://cloud.google.com/vision/automl/docs/export-edge" rel="noopener ugc nofollow" target="_blank"> API是:</a></p><pre class="lq lr ls lt fd mg mh mi mj aw mk bi"><span id="34fd" class="ml jy hi mh b fi mm mn l mo mp">curl \<br/>  -H "Authorization: Bearer $(gcloud auth application-default print-access-token)" \<br/>  -H "Content-Type: application/json" \<br/>  https://${<strong class="mh hj">ENDPOINT</strong>}/v1beta1/projects/${<strong class="mh hj">PROJECT_ID</strong>}/locations/us-central1/models/${<strong class="mh hj">MODEL_ID</strong>}:export \<br/>  -d '{<br/>        "output_config": {<br/>          "model_format": "<strong class="mh hj">tflite</strong>",<br/>          "gcs_destination": {<br/>              "output_uri_prefix": "${<strong class="mh hj">USER_GCS_PATH</strong>}"<br/>          }<br/>        }<br/>      }'</span></pre><p id="5799" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您想导出一个针对Edge TPU优化的模型(我们稍后会谈到这一点)，那么<a class="ae la" href="https://cloud.google.com/vision/automl/docs/export-edge#edge-tpu" rel="noopener ugc nofollow" target="_blank">您必须使用</a>:</p><pre class="lq lr ls lt fd mg mh mi mj aw mk bi"><span id="3a77" class="ml jy hi mh b fi mm mn l mo mp">curl \<br/>  -H "Authorization: Bearer $(gcloud auth application-default print-access-token)" \<br/>  -H "Content-Type: application/json" \<br/>  https://${<strong class="mh hj">ENDPOINT</strong>}/v1beta1/projects/${<strong class="mh hj">PROJECT_ID</strong>}/locations/us-central1/models/${<strong class="mh hj">MODEL_ID</strong>}:export \<br/>  -d '{<br/>        "output_config": {<br/>          "model_format": "<strong class="mh hj">edgetpu_tflite</strong>",<br/>          "gcs_destination": {<br/>              "output_uri_prefix": "${<strong class="mh hj">USER_GCS_PATH</strong>}"<br/>          }<br/>        }<br/>      }'</span></pre><p id="49ed" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过这个API调用，NiFi将收到一个JSON有效负载，其中包含表示您的定制模型的tflite文件的GCS路径。这里，我们再次使用Google Cloud IoT核心命令，通过MQTT将这些信息发送到我们的设备:</p><pre class="lq lr ls lt fd mg mh mi mj aw mk bi"><span id="878d" class="ml jy hi mh b fi mm mn l mo mp">gcloud iot devices commands send \<br/>    --command-data=gs://.../model.tflite<br/>    --region=REGION  \<br/>    --registry=REGISTRY_ID \<br/>    --device=DEVICE_ID</span></pre><p id="eced" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在MiNiFi上，我们接收这些信息并将模型直接下载到设备上，以便进行本地推断:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/cb5103eb09ca7977dc5a5525e5f6a930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g7WAudRpBvOorqllzniC-Q.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">通过MQTT接收模型的GCS路径，并将模型下载到本地</figcaption></figure><p id="31e8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们已经在Raspberry Pi上下载了TensorFlow lite模型，我们可以对每个捕获的图像执行推理:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/192818702940685522427a0833cb074e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rmFCjJdlWN-UubDz-XCDfQ.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">使用下载的TensorFlow lite模型在边缘上进行推断</figcaption></figure><p id="1f47" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了在本地执行模型，<a class="ae la" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/python/" rel="noopener ugc nofollow" target="_blank">我正在执行一个Python脚本，你可以在这里找到</a>。</p><h1 id="a1f1" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">使用珊瑚边TPU提升您的设备</h1><p id="6cee" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">2019年10月22日起，<a class="ae la" href="https://coral.ai/" rel="noopener ugc nofollow" target="_blank">珊瑚现GA </a>！Coral是一个硬件和软件平台，用于构建具有快速神经网络推理的智能设备。</p><p id="e6c7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些设备的核心是Edge TPU协处理器。这是一个由谷歌制造的小型ASIC，专门设计用于高速执行最先进的神经网络，功耗低。Edge TPU每秒能够执行4万亿次运算(万亿次运算)，每个TOPS使用0.5瓦(每瓦2 TOPS)。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/9c83f34ab4e6b8ef42a2aaa45670f0d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dt_4gQoNsRIeLdqdkqgbDQ.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">谷歌Edge TPU基准测试</figcaption></figure><p id="1f0a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我的用例中，我使用的是USB加速器，我将它直接插入到树莓Pi中。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/65439095bc70021a7a6f209350cf8e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KODAWePknRnQrBkyZDN6hg.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">Coral USB加速器</figcaption></figure><p id="73d8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用我在谷歌云视觉上训练的定制ML模型，我可以轻松地比较使用或不使用边缘TPU时的TensorFlow“调用”时间:</p><ul class=""><li id="0cc2" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated"><strong class="is hj">不使用边缘TPU优化模型时:</strong></li></ul><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/da52f4c443877af0dce64de1c004f7b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-AoOL-oyScgfyQhZ"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">不使用边缘TPU时的TF Lite调用时间-约127毫秒/图像</figcaption></figure><ul class=""><li id="a953" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated"><strong class="is hj">使用边缘TPU和优化型号时:</strong></li></ul><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/80909ffab5a1bacbfff291bd69976409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ABOTnT74Mwj-aoG-"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">使用边缘TPU时的TF Lite调用时间—大约9毫秒/图像</figcaption></figure><p id="4bd1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在不使用Edge TPU的情况下，我们大约需要127毫秒，而在为我的cookies用例生成的低延迟优化模型上使用Edge TPU时，我们只需要9毫秒。</p><h1 id="d189" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">初步结果</h1><p id="a6ec" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">到目前为止，我们已经通过MQTT发送了图片，并在MiNiFi工作流的两个不同分支中执行了分类推断(换句话说:图片摄取和图片推断是并行进行的)。下面是我们得到的结果(处理时间是我们拍照的时刻和我们得到分类的标签和置信度得分的时刻之间的持续时间；推理时间是使用TensorFlow Lite模型获取分类的“调用”的持续时间):</p><ul class=""><li id="bead" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated"><strong class="is hj"> <em class="mb">云模式</em> </strong>(通过HTTPS在云端推理):<br/> -处理时间:每张图像约6秒<br/> -推理时间:每张图像约2.5秒</li><li id="c53a" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated"><strong class="is hj"> <em class="mb">边缘模式</em> </strong>(无边缘TPU): <br/> -处理时间:每幅图像约750毫秒<br/> -推断时间:每幅图像约127毫秒</li><li id="86d1" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated"><strong class="is hj"> <em class="mb">边缘模式+ TPU </em> </strong>(带边缘TPU): <br/> -处理时间:每幅图像约500毫秒<br/> -推断时间:每幅图像约9毫秒</li></ul><h1 id="0a0f" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">监控和自动标记</h1><p id="7cdf" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">到目前为止，我们所做的很好，但是，一旦我们对树莓派做出了推断，我们就采取行动保留或不保留生产线上的饼干(在我的例子中，我打开或关闭红灯或绿灯)，然后我们忘记了推断结果。下一步是在通过MQTT发送图片之前执行推理<strong class="is hj">，这样我们就可以将推理结果和图片一起发送。图片推断和图片摄取是顺序完成的。这有两个主要的好处:</strong></p><ul class=""><li id="d8c6" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated">我们可以收集额外的数据来部署<strong class="is hj">监控仪表板</strong>，以便获得关于我们的定制模型在我们的设备上表现如何的准确信息</li><li id="ec67" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">我们可以引入图像的<strong class="is hj">自动标记</strong>:如果推断标记的置信分数超过，例如90%，我们可以在将图像摄取到Google Cloud Vision数据集时自动标记图像，以便在标记时需要最少的人工交互:只有代表新类型缺陷的图像可能需要人工标记。</li></ul><p id="0fa3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我不会详细介绍工作流中所需的更改，但在MiNiFi上看起来是这样的:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/d0d94c1f22c795fbb4d57a53913e2909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zopgZx_ie0YhdJi0"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">通过MQTT发送推理结果和图片时的MiNiFi工作流</figcaption></figure><p id="c648" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦我们收到NiFi中的所有信息，我们就可以将这些信息添加到Stackdriver监控仪表板中，并实时查看我们的定制模型的执行情况:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nd"><img src="../Images/64b04a750e339582f0ea36eb27bca4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRnoevWi9sonRc7K0stbtA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">云模式与边缘模式的处理时间和分数</figcaption></figure><p id="e5b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上图中，我们可以看到在更新设备配置后，从<strong class="is hj"> <em class="mb">云</em> </strong>模式到<strong class="is hj"> <em class="mb">边缘</em> </strong>模式，处理时间和置信度得分是如何变化的。我们清楚地看到，处理速度快得多，但我们在准确性方面有所损失(尽管仍在90%以上！).</p><p id="0402" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们还可以在器件配置为<strong class="is hj"> <em class="mb">边缘</em> </strong>模式时，比较使用或不使用边缘TPU的结果:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ne"><img src="../Images/d498836a1b7f9fa1181a329d921d8175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CbOarHGpMFuwWe5Qf_iOwA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">有和没有边缘TPU的边缘模式的处理时间和分数</figcaption></figure><p id="b7ea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的图片中，我们可以看到一个有趣的结果:当使用边缘TPU和边缘TPU优化模型时，我们获得了更短的处理时间，同时也获得了更高的推断置信度得分。没有理由不用！</p><h1 id="142e" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">结论</h1><p id="6883" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">首先，感谢您阅读这个非常长的故事…然后，这里是它的结论:感谢Apache NiFi、MiNiFi、Google云平台的产品和Coral产品，我们实现了一个高效的端到端解决方案，在边缘上运行定制的ML模型，同时不断用新数据刷新我们的模型，以随着时间的推移提高模型的准确性，而无需代码和最少的人工交互。我会让您发挥想象力，将所有这些转换成您自己的用例！</p><p id="4fb9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">演讲的幻灯片、录音以及代码片段、定制处理器、工作流程、脚本等将在未来几天内添加到这个<a class="ae la" href="https://github.com/pvillard31/aceu19" rel="noopener ugc nofollow" target="_blank"> Github资源库</a>中。像往常一样，请随意评论/提问。</p><p id="96b3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">想要更多吗？访问<a class="ae la" href="https://pierrevillard.com/" rel="noopener ugc nofollow" target="_blank">我的网站</a> …或<a class="ae la" href="http://twitter.com/pvillard31" rel="noopener ugc nofollow" target="_blank">在Twitter上关注我</a>！</p></div></div>    
</body>
</html>