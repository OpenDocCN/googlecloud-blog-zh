<html>
<head>
<title>Running PyTorch with TPUs on GCP AI Platform Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在GCP人工智能平台上用TPUs运行PyTorch培训</h1>
<blockquote>原文：<a href="https://medium.com/google-cloud/running-pytorch-with-tpus-on-gcp-ai-platform-training-4dacc0134b6?source=collection_archive---------1-----------------------#2021-01-06">https://medium.com/google-cloud/running-pytorch-with-tpus-on-gcp-ai-platform-training-4dacc0134b6?source=collection_archive---------1-----------------------#2021-01-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/08dfa08d65f5c2d99f26e9e284c32dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*sjnwic6GUhYEjWX66vu5Fw.png"/></div></figure><blockquote class="im in io"><p id="a96c" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">目前人工智能平台培训只为CPU和GPU提供了<a class="ae jo" href="https://cloud.google.com/ai-platform/training/docs/getting-started-pytorch" rel="noopener ugc nofollow" target="_blank">预构建的Pytorch环境</a>，因此当在人工智能平台培训上使用Pytorch和TPUs时，您需要构建一个自定义容器。你基本上需要构建一个支持PyTorch的XLA版本的容器，正如在<a class="ae jo" href="https://github.com/pytorch/xla" rel="noopener ugc nofollow" target="_blank"> XLA PyTorch文档</a>中所解释的。按照XLA文档中的步骤构建容器非常容易，但是您应该注意，在初始化XLA TPU库之前，必须将TPU的地址导出到名为$ TPU IP地址的流程环境中。当我们试图在AI平台训练中部署容器时，这将产生一个困难，因为TPU的地址事先并不知道。</p><p id="c96d" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个问题的解决方法很简单。我们可以简单地修改XLA PyTorch容器的入口点，使其指向一个启动器“片段”,它将首先从环境中检索TPU地址，然后在调用模型代码之前将其导出到$ TPU IP地址环境变量中。AI平台训练以Tensorflow可以理解的格式提供TPU的“坐标”，但不提供环境中TPU的实际IP地址和端口。这个问题的一个简单解决方案是调用Tensorflow代码片段，该代码片段将使用tpu聚类解析器从AI平台训练环境中提取实际的IP地址和端口:</p></blockquote><p id="ae21" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><em class="ir">TPU _聚类_解析器= TF . distribute . cluster _解析器。TPUClusterResolver(tpu名称，区域名称，项目)</em></p><blockquote class="im in io"><p id="7c59" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个片段将返回TPU的实际IP地址和端口。我们现在可以在$ TPU IP地址变量中导出这些信息，一切都正常了！这个解决方案很简单，但是它涉及到很多步骤，所以我们在下面提供了一个逐步的例子。</p></blockquote></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h1 id="19db" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">在TPUs上启动PyTorch jobs进行人工智能培训:循序渐进</h1><p id="cd18" class="pw-post-body-paragraph ip iq hi is b it kx iv iw ix ky iz ja jp kz jd je jq la jh ji jr lb jl jm jn hb bi translated">在这条学习道路上有三个基本步骤:</p><ol class=""><li id="2f11" class="lc ld hi is b it iu ix iy jp le jq lf jr lg jn lh li lj lk bi translated">通过GCP培训了解如何在CPU/GPU上使用PyTorch</li><li id="fb54" class="lc ld hi is b it ll ix lm jp ln jq lo jr lp jn lh li lj lk bi translated">了解如何在TPUs上运行PyTorch</li><li id="b150" class="lc ld hi is b it ll ix lm jp ln jq lo jr lp jn lh li lj lk bi translated">通过GCP培训了解如何在TPUs上使用PyTorch</li></ol></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h1 id="0eb4" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">了解如何在GCP培训中使用客户容器(CPU/GPU上的PyTorch)</h1><p id="9ddf" class="pw-post-body-paragraph ip iq hi is b it kx iv iw ix ky iz ja jp kz jd je jq la jh ji jr lb jl jm jn hb bi translated">首先，了解如何在CPU或GPU上运行PyTorch，并按照下面的教程在GCP培训中使用自定义容器:</p><ul class=""><li id="2376" class="lc ld hi is b it iu ix iy jp le jq lf jr lg jn lq li lj lk bi translated"><a class="ae jo" href="https://cloud.google.com/ai-platform/training/docs/custom-containers-training" rel="noopener ugc nofollow" target="_blank">https://cloud . Google . com/ai-platform/training/docs/custom-containers-training</a></li></ul><p id="4b5c" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">需要理解的一个重要方面是所使用的docker文件，它使用PyTorch构建映像，将示例模型复制到容器中，并定义GCP培训用来启动容器的docker入口点。Dockerfile文件复制如下:</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="523c" class="ma ka hi lw b fi mb mc l md me"># Dockerfile<br/>FROM python:2.7.15-jessie<br/>WORKDIR /root</span><span id="492a" class="ma ka hi lw b fi mf mc l md me"># Installs pytorch and torchvision.<br/>RUN pip install torch==1.0.0 torchvision==0.2.1</span><span id="7a01" class="ma ka hi lw b fi mf mc l md me"># Installs cloudml-hypertune for hyperparameter tuning.<br/># It’s not needed if you don’t want to do hyperparameter tuning.<br/>RUN pip install cloudml-hypertune</span><span id="cf38" class="ma ka hi lw b fi mf mc l md me"># Installs google cloud sdk, this is mostly for using gsutil to export model.<br/>RUN wget -nv \<br/>https://dl.google.com/dl/cloudsdk/release/google-cloud-sdk.tar.gz &amp;&amp; \<br/>mkdir /root/tools &amp;&amp; \<br/>tar xvzf google-cloud-sdk.tar.gz -C /root/tools &amp;&amp; \<br/>rm google-cloud-sdk.tar.gz &amp;&amp; \<br/>/root/tools/google-cloud-sdk/install.sh --usage-reporting=false \<br/>--path-update=false --bash-completion=false \<br/>--disable-installation-options &amp;&amp; \<br/>rm -rf /root/.config/* &amp;&amp; \<br/>ln -s /root/.config /config &amp;&amp; \</span><span id="1918" class="ma ka hi lw b fi mf mc l md me"># Remove the backup directory that gcloud creates<br/>rm -rf /root/tools/google-cloud-sdk/.install/.backup</span><span id="7acb" class="ma ka hi lw b fi mf mc l md me"># Path configuration<br/>ENV PATH $PATH:/root/tools/google-cloud-sdk/bin</span><span id="21bb" class="ma ka hi lw b fi mf mc l md me"># Make sure gsutil will use the default service account<br/>RUN echo '[GoogleCompute]\nservice_account = default' &gt; /etc/boto.cfg</span><span id="60c4" class="ma ka hi lw b fi mf mc l md me"># Copies the trainer code<br/>RUN mkdir /root/trainer<br/>COPY trainer/mnist.py /root/trainer/mnist.py</span><span id="5f52" class="ma ka hi lw b fi mf mc l md me"># Sets up the entry point to invoke the trainer.<br/>ENTRYPOINT ["python", "trainer/mnist.py"]</span></pre><h1 id="0a48" class="jz ka hi bd kb kc mg ke kf kg mh ki kj kk mi km kn ko mj kq kr ks mk ku kv kw bi translated">了解如何在TPUs上运行PyTorch</h1><p id="bb41" class="pw-post-body-paragraph ip iq hi is b it kx iv iw ix ky iz ja jp kz jd je jq la jh ji jr lb jl jm jn hb bi translated">要在TPUs上运行PyTorch，最好的方法是使用谷歌XLA团队提供的容器映像(XLA =加速线性代数)。这是PyTorch针对GPU的优化版本。该教程可从以下网站获得:</p><ul class=""><li id="708d" class="lc ld hi is b it iu ix iy jp le jq lf jr lg jn lq li lj lk bi translated"><a class="ae jo" href="https://github.com/pytorch/xla" rel="noopener ugc nofollow" target="_blank">https://github.com/pytorch/xla</a></li></ul><p id="5dd1" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">阅读本教程时，您已经在单个TPU上部署了mnist示例。本教程还解释了如何使用多个TPU。对于多个TPU，您可以选择使用一个虚拟机实例组，如教程中所述，也可以选择使用GCP培训，该培训将自动为您调配所有环境，如下一节所述。</p><h1 id="7319" class="jz ka hi bd kb kc mg ke kf kg mh ki kj kk mi km kn ko mj kq kr ks mk ku kv kw bi translated">了解如何在GCP培训的TPUs上运行PyTorch</h1><p id="b754" class="pw-post-body-paragraph ip iq hi is b it kx iv iw ix ky iz ja jp kz jd je jq la jh ji jr lb jl jm jn hb bi translated">现在，让我们一起学习这两个教程。我们基本上想用TPU部署前面的示例，但这次是在GCP培训中使用定制容器。我们可以简单地通过使用不同的docker文件来构建容器来实现这一点。在这个docker文件中，我们需要做一些事情:</p><ol class=""><li id="08e1" class="lc ld hi is b it iu ix iy jp le jq lf jr lg jn lh li lj lk bi translated">基础图像应该是我们在上一个教程中使用的XLA PyTorch图像</li><li id="7bcd" class="lc ld hi is b it ll ix lm jp ln jq lo jr lp jn lh li lj lk bi translated">我们复制一小段代码(启动器脚本),解析TPU工作者的地址，然后调用前面描述的模型。</li><li id="e138" class="lc ld hi is b it ll ix lm jp ln jq lo jr lp jn lh li lj lk bi translated">我们将容器的入口点改为指向启动器脚本</li></ol><p id="3d03" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">我们成功地使用了下面的dockerfile文件:</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="4dc3" class="ma ka hi lw b fi mb mc l md me"># Dockerfile<br/>#use the latest version of XLA enabled Pytorch<br/>FROM gcr.io/tpu-pytorch/xla:nightly<br/>WORKDIR /root</span><span id="7409" class="ma ka hi lw b fi mf mc l md me"># Installs Tensorflow to resolve the TPU name to IP Address<br/>RUN pip install tensorflow</span><span id="c8c3" class="ma ka hi lw b fi mf mc l md me"># Installs cloudml-hypertune for hyperparameter tuning.<br/># It’s not needed if you don’t want to do hyperparameter tuning.<br/>RUN pip install cloudml-hypertune</span><span id="6c03" class="ma ka hi lw b fi mf mc l md me"># Installs google cloud sdk, this is mostly for using gsutil to    <br/># export the model.<br/>RUN wget -nv \<br/>https://dl.google.com/dl/cloudsdk/release/google-cloud-sdk.tar.gz &amp;&amp; \<br/>mkdir /root/tools &amp;&amp; \<br/>tar xvzf google-cloud-sdk.tar.gz -C /root/tools &amp;&amp; \<br/>rm google-cloud-sdk.tar.gz &amp;&amp; \<br/>/root/tools/google-cloud-sdk/install.sh --usage-reporting=false \<br/>--path-update=false --bash-completion=false \<br/>--disable-installation-options &amp;&amp; \<br/>rm -rf /root/.config/* &amp;&amp; \<br/>ln -s /root/.config /config &amp;&amp; \</span><span id="7841" class="ma ka hi lw b fi mf mc l md me"># Remove the backup directory that gcloud creates<br/>rm -rf /root/tools/google-cloud-sdk/.install/.backup</span><span id="a71a" class="ma ka hi lw b fi mf mc l md me"># Path configuration<br/>ENV PATH $PATH:/root/tools/google-cloud-sdk/bin</span><span id="424b" class="ma ka hi lw b fi mf mc l md me"># Make sure gsutil will use the default service account<br/>RUN echo '[GoogleCompute]\nservice_account = default' &gt; /etc/boto.cfg</span><span id="f746" class="ma ka hi lw b fi mf mc l md me"># Copies the trainer code and launcher<br/>RUN mkdir /root/launcher<br/>COPY launcher/launcher.sh /root/launcher/launcher.sh<br/>COPY launcher/launcher.py /root/launcher/launcher.py</span><span id="8985" class="ma ka hi lw b fi mf mc l md me"># Sets up the entry point to invoke the trainer.<br/>ENTRYPOINT ["sh", "launcher/launcher.sh"]</span></pre><p id="d73e" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">现在让我们解释一下我们对入口点做了什么。现在，新映像将从launcher.sh命令开始。这样做的原因是，我们需要从GCP培训设置的环境变量中获取TPU名称。然后，我们将该名称转换为TPU IP地址和端口，以便像以前一样调用PyTorch。下面是实现这一点的launcher.sh文件:</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="ecc9" class="ma ka hi lw b fi mb mc l md me"># dump all the environment variables so that we can see them in the<br/># execution log<br/>export</span><span id="5f18" class="ma ka hi lw b fi mf mc l md me"># use an auxiliary python script to get the IP Address and Port of<br/># the TPU VM <br/>tpu_address=`python /root/launcher/launcher.py`<br/>echo "tpu_address is $tpu_address"</span><span id="1082" class="ma ka hi lw b fi mf mc l md me"># export the TPU address and port<br/>export XRT_TPU_CONFIG="tpu_worker;0;$tpu_address"</span><span id="9cf6" class="ma ka hi lw b fi mf mc l md me"># invoke the trainer code<br/>python /pytorch/xla/test/test_train_mnist.py</span></pre><p id="cfcc" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">launcher.sh脚本使用launcher.py脚本通过CAIP培训提供的环境变量解析TPU的名称:</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="eb81" class="ma ka hi lw b fi mb mc l md me">import json<br/>import os<br/>import tensorflow.compat.v1 as tf</span><span id="86ab" class="ma ka hi lw b fi mf mc l md me">tf_config_str = os.environ.get('TF_CONFIG')<br/>tf_config_dict  = json.loads(tf_config_str)</span><span id="ce94" class="ma ka hi lw b fi mf mc l md me">#print(json.dumps(tf_config_dict, indent=2))</span><span id="b79c" class="ma ka hi lw b fi mf mc l md me">tpu_config_str = os.environ.get('TPU_CONFIG')<br/>tpu_config_dict  = json.loads(tpu_config_str)</span><span id="4ee9" class="ma ka hi lw b fi mf mc l md me">#print(json.dumps(tpu_config_dict, indent=2))</span><span id="8908" class="ma ka hi lw b fi mf mc l md me">tpu_name = tpu_config_dict["tpu_node_name"]<br/>project_name = tpu_config_dict["project"]<br/>zone_name = tpu_config_dict["zone"]</span><span id="f902" class="ma ka hi lw b fi mf mc l md me">tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(<br/>tpu_name,<br/>zone=zone_name,<br/>project=project_name)</span><span id="7f9c" class="ma ka hi lw b fi mf mc l md me">#print(tpu_cluster_resolver.cluster_spec())<br/>worker_list=tpu_cluster_resolver.cluster_spec()</span><span id="7e29" class="ma ka hi lw b fi mf mc l md me">#print(vars(worker_list))</span><span id="a253" class="ma ka hi lw b fi mf mc l md me">print(worker_list._cluster_spec["worker"][0])</span></pre><p id="c11e" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">如您所见，有两个重要的环境变量:TF_CONFIG和TPU_CONFIG。这里我们只使用TPU配置。我们使用os.environ.get从环境中获取它，然后读取它是一个json对象，然后获取TPU名称、项目和区域。然后，这三个参数被传递给TPUClusterResolver类，以解析成一个worker定义，在本例中，它具有TPU IP地址和端口。以下是GCP培训工作开始时这些环境变量的情况:</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="0258" class="ma ka hi lw b fi mb mc l md me">export TF_CONFIG='{"cluster":{"master":["127.0.0.1:2222"]},"environment":"cloud","task":{"type":"master","index":0},"job":"{\n \"scale_tier\": \"CUSTOM\",\n \"master_type\": \"n1-highcpu-16\",\n \"worker_type\": \"cloud_tpu\",\n \"worker_count\": \"1\",\n \"region\": \"us-central1\",\n \"master_config\": {\n \"image_uri\": \"gcr.io/ml-3d-segmentation/mnist_pytorch_custom_container:mnist_pytorch_tpu\"\n },\n \"worker_config\": {\n \"accelerator_config\": {\n \"count\": \"8\",\n \"type\": \"TPU_V3\"\n },\n \"tpu_tf_version\": \"1.15\"\n }\n}"}</span><span id="4692" class="ma ka hi lw b fi mf mc l md me">export TPU_CONFIG='{"project": "j79edad251b5e3ba2-ml", "zone": "us-central1-b", "tpu_node_name": "cmle-training-14556593641811293930-tpu"}'</span></pre><h1 id="685f" class="jz ka hi bd kb kc mg ke kf kg mh ki kj kk mi km kn ko mj kq kr ks mk ku kv kw bi translated">构建和测试docker映像</h1><p id="5b99" class="pw-post-body-paragraph ip iq hi is b it kx iv iw ix ky iz ja jp kz jd je jq la jh ji jr lb jl jm jn hb bi translated">我们现在准备构建docker映像。创建一个工作目录(我们使用路径“/work”来说明)。然后将dockerfile文件复制到其中。使其成为活动目录。</p><p id="42de" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">&gt; cd /work</p><p id="8a34" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">为启动器文件创建一个目录</p><p id="7ebc" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">&gt; mkdir发射器</p><p id="93bd" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">将launcher.sh和launcher.py复制到/launcher目录</p><p id="c96a" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">现在可以运行docker build命令了。如果您尚未这样做，请执行上一个示例中的步骤，以使用GCP凭据启用docker:</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="a01d" class="ma ka hi lw b fi mb mc l md me">sudo usermod -a -G docker ${USER}<br/>gcloud auth configure-docker</span></pre><p id="da59" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">然后给图像一个名称和标签:</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="decf" class="ma ka hi lw b fi mb mc l md me">export PROJECT_ID=$(gcloud config list project --format "value(core.project)"<br/>export IMAGE_REPO_NAME=mnist_pytorch_custom_container<br/>export IMAGE_TAG=mnist_pytorch_cpu<br/>export IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME:$IMAGE_TAG</span></pre><p id="ded4" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">然后执行docker构建:<code class="du ml mm mn lw b">docker build -f Dockerfile -t $IMAGE_URI ./</code></p><h1 id="281b" class="jz ka hi bd kb kc mg ke kf kg mh ki kj kk mi km kn ko mj kq kr ks mk ku kv kw bi translated">用本地项目测试Docker映像</h1><p id="5f49" class="pw-post-body-paragraph ip iq hi is b it kx iv iw ix ky iz ja jp kz jd je jq la jh ji jr lb jl jm jn hb bi translated">您现在可以运行docker文件来测试它。但是要小心！我们已经更改了启动器脚本的入口点，该脚本希望环境像GCP培训那样设置。因此，要测试它，请在交互模式下启动它，并将入口点覆盖为“sh”模式:</p><p id="3827" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><code class="du ml mm mn lw b">docker run -it — shm-size 16G — entrypoint=sh $IMAGE_URI</code></p><p id="be43" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">您现在可以导出启动器所期望的变量(没有错误检查！).设置TF_CONFIG和TPU_CONFIG。在上一个教程中，您已经在项目中创建了一个TPU虚拟机。更改下面的TPU配置语句，以反映您项目中的项目、区域和TPU名称:</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="88f1" class="ma ka hi lw b fi mb mc l md me">TF_CONFIG='{"cluster":{"master":["127.0.0.1:2222"]},"environment":"cloud","task":{"type":"master","index":0},"job":"{\n \"scale_tier\": \"CUSTOM\",\n \"master_type\": \"n1-highcpu-16\",\n \"worker_type\": \"cloud_tpu\",\n \"worker_count\": \"1\",\n \"region\": \"us-central1\",\n \"master_config\": {\n \"image_uri\": \"gcr.io/ml-3d-segmentation/mnist_pytorch_custom_container:mnist_pytorch_tpu\"\n },\n \"worker_config\": {\n \"accelerator_config\": {\n \"count\": \"8\",\n \"type\": \"TPU_V3\"\n },\n \"tpu_tf_version\": \"1.15\"\n }\n}"}</span><span id="7238" class="ma ka hi lw b fi mf mc l md me">export TPU_CONFIG='{"project": "YOUR_PROJECT_NAME", "zone": "YOUR_ZONE", "tpu_node_name": "YOUR_TPU_NAME"}'</span></pre><p id="ce0a" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">现在，您可以使用cd转到/root/launcher目录，并执行launcher.sh。这应该会使用正确的TPU地址调用培训。</p><h1 id="2884" class="jz ka hi bd kb kc mg ke kf kg mh ki kj kk mi km kn ko mj kq kr ks mk ku kv kw bi translated">在GCP培训中检验码头工人形象</h1><p id="8174" class="pw-post-body-paragraph ip iq hi is b it kx iv iw ix ky iz ja jp kz jd je jq la jh ji jr lb jl jm jn hb bi translated">首先，将经过测试的docker映像推送到映像注册中心:</p><p id="eb26" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><code class="du ml mm mn lw b">docker push $IMAGE_URI</code></p><p id="953c" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">我们现在可以使用docker图像通过CAIP培训开始培训。我们将使用一个简单的配置，包括一个主节点和一个TPU节点。以下yaml文件具有正确的配置，请注意，您必须更改imageUri字段以指向您已经上传到云注册表的容器:</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="7366" class="ma ka hi lw b fi mb mc l md me">jobId: tpu_test_3<br/>labels:<br/>type: prod<br/>owner: novaes<br/>trainingInput:<br/>scaleTier: CUSTOM<br/>masterType: n1-highcpu-16<br/>masterConfig:</span><span id="617f" class="ma ka hi lw b fi mf mc l md me">imageUri: gcr.io/YOUR_IMAGE_PATH_HERE:mnist_pytorch_tpu</span><span id="7a6a" class="ma ka hi lw b fi mf mc l md me">workerType: cloud_tpu<br/>workerCount: 1<br/>workerConfig:<br/>acceleratorConfig:<br/>type: TPU_V3<br/>count: 8<br/>tpuTfVersion: "1.15"</span></pre><p id="e869" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">确保将ImageUri字段更改为指向您的docker图像。将yaml保存为py torch _ TPU _配置. yaml。现在可以使用以下命令提交作业:</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="69d7" class="ma ka hi lw b fi mb mc l md me">gcloud beta ai-platform jobs submit training your_job_name_here \ <br/>— config=pytorch_tpu_config.yaml — region us-central1</span></pre><p id="1a02" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">瞧啊。您现在可以使用GCP AI平台培训运行TPU启用的PyTorch作业。</p></div></div>    
</body>
</html>